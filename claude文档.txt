# Claude 简介

Claude 是由 Anthropic 构建的高性能、可信赖且智能的 AI 平台。Claude 在涉及语言、推理、分析、编程等任务方面表现出色。

---

<Tip>The latest generation of Claude models:<br/><br/>**Claude Opus 4.5** - Most intelligent model, and an industry-leader for coding, agents, and computer use. [Learn more](https://www.anthropic.com/news/claude-opus-4-5).<br/><br/>
**Claude Sonnet 4.5** - Balanced performance and practicality for most uses, including coding and agents. [Learn more](https://www.anthropic.com/news/claude-sonnet-4-5).<br/><br/>
**Claude Haiku 4.5** - Fastest model with near-frontier intelligence. [Learn more](https://www.anthropic.com/news/claude-haiku-4-5).</Tip>

<Note>
想要与 Claude 聊天？请访问 [claude.ai](http://www.claude.ai)！
</Note>

## 开始使用

如果您是 Claude 的新用户，请从这里开始学习基础知识并进行您的第一次 API 调用。

<CardGroup cols={3}>
  <Card title="开始使用" icon="check" href="/docs/zh-CN/get-started">
    设置您的开发环境以使用 Claude 进行构建。
  </Card>
  <Card title="了解 Claude" icon="settings" href="/docs/zh-CN/about-claude/models/overview">
    了解 Claude 模型系列。
  </Card>
  <Card title="提示词库" icon="books" href="/docs/zh-CN/resources/prompt-library/library">
    探索示例提示词以获得灵感。
  </Card>
</CardGroup>

---

## 使用 Claude 开发

Anthropic 拥有一流的开发者工具，可以使用 Claude 构建可扩展的应用程序。

<CardGroup cols={3}>
  <Card title="开发者控制台" icon="computer" href="/">
    在浏览器中使用 Workbench 和提示词生成器工具享受更简单、更强大的提示词功能。
  </Card>
  <Card title="API 参考" icon="code" href="/docs/zh-CN/api/overview">
    使用 Claude API 和 SDK 进行探索、实施和扩展。
  </Card>
  <Card title="Claude Cookbook" icon="chef-hat" href="https://github.com/anthropics/anthropic-cookbook">
    通过演示上传 PDF、嵌入等功能的交互式 Jupyter 笔记本进行学习。
  </Card>
</CardGroup>

---

## 核心功能

Claude 可以协助处理涉及文本、代码和图像的许多任务。

<CardGroup cols={2}>
  <Card title="文本和代码生成" icon="text-aa" href="/docs/zh-CN/build-with-claude/text-generation">
    总结文本、回答问题、提取数据、翻译文本，以及解释和生成代码。
  </Card>
  <Card title="视觉" icon="image" href="/docs/zh-CN/build-with-claude/vision">
    处理和分析视觉输入，并从图像生成文本和代码。
  </Card>
</CardGroup>

---

## 支持
 
<CardGroup cols={2}>
  <Card title="帮助中心" icon="help" href="https://support.claude.com/en/">
    查找常见账户和计费问题的答案。
  </Card>

  <Card title="服务状态" icon="chart" href="https://status.claude.com">
    检查 Anthropic 服务的状态。
  </Card>
</CardGroup>

# Claude 入门指南

进行您的第一次 Claude API 调用并构建一个简单的网络搜索助手

---

## 前置条件

- 一个 Anthropic [Console 账户](/)
- 一个 [API 密钥](/settings/keys)

## 调用 API

<Tabs>
  <Tab title="cURL">
    <Steps>
      <Step title="设置您的 API 密钥">
        从 [Claude Console](/settings/keys) 获取您的 API 密钥并将其设置为环境变量：

        ```bash
        export ANTHROPIC_API_KEY='your-api-key-here'
        ```
      </Step>

      <Step title="进行您的第一次 API 调用">
        运行此命令以创建一个简单的网络搜索助手：

        ```bash
        curl https://api.anthropic.com/v1/messages \
          -H "Content-Type: application/json" \
          -H "x-api-key: $ANTHROPIC_API_KEY" \
          -H "anthropic-version: 2023-06-01" \
          -d '{
            "model": "claude-sonnet-4-5",
            "max_tokens": 1000,
            "messages": [
              {
                "role": "user", 
                "content": "What should I search for to find the latest developments in renewable energy?"
              }
            ]
          }'
        ```

        **示例输出：**
        ```json
        {
          "id": "msg_01HCDu5LRGeP2o7s2xGmxyx8",
          "type": "message", 
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": "Here are some effective search strategies to find the latest renewable energy developments:\n\n## Search Terms to Use:\n- \"renewable energy news 2024\"\n- \"clean energy breakthrough\"\n- \"solar/wind/battery technology advances\"\n- \"green energy innovations\"\n- \"climate tech developments\"\n- \"energy storage solutions\"\n\n## Best Sources to Check:\n\n**News & Industry Sites:**\n- Renewable Energy World\n- GreenTech Media (now Wood Mackenzie)\n- Energy Storage News\n- CleanTechnica\n- PV Magazine (for solar)\n- WindPower Engineering & Development..."
            }
          ],
          "model": "claude-sonnet-4-5",
          "stop_reason": "end_turn",
          "usage": {
            "input_tokens": 21,
            "output_tokens": 305
          }
        }
        ```
      </Step>
    </Steps>
  </Tab>

  <Tab title="Python">
    <Steps>
      <Step title="设置您的 API 密钥">
        从 [Claude Console](/settings/keys) 获取您的 API 密钥并将其设置为环境变量：

        ```bash
        export ANTHROPIC_API_KEY='your-api-key-here'
        ```
      </Step>

      <Step title="安装 SDK">
        安装 Anthropic Python SDK：

        ```bash
        pip install anthropic
        ```
      </Step>

      <Step title="创建您的代码">
        将其保存为 `quickstart.py`：

        ```python
        import anthropic

        client = anthropic.Anthropic()

        message = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=1000,
            messages=[
                {
                    "role": "user",
                    "content": "What should I search for to find the latest developments in renewable energy?"
                }
            ]
        )
        print(message.content)
        ```
      </Step>

      <Step title="运行您的代码">
        ```bash
        python quickstart.py
        ```

        **示例输出：**
        ```python
        [TextBlock(text='Here are some effective search strategies for finding the latest renewable energy developments:\n\n**Search Terms to Use:**\n- "renewable energy news 2024"\n- "clean energy breakthroughs"\n- "solar/wind/battery technology advances"\n- "energy storage innovations"\n- "green hydrogen developments"\n- "renewable energy policy updates"\n\n**Reliable Sources to Check:**\n- **News & Analysis:** Reuters Energy, Bloomberg New Energy Finance, Greentech Media, Energy Storage News\n- **Industry Publications:** Renewable Energy World, PV Magazine, Wind Power Engineering\n- **Research Organizations:** International Energy Agency (IEA), National Renewable Energy Laboratory (NREL)\n- **Government Sources:** Department of Energy websites, EPA clean energy updates\n\n**Specific Topics to Explore:**\n- Perovskite and next-gen solar cells\n- Offshore wind expansion\n- Grid-scale battery storage\n- Green hydrogen production\n- Carbon capture technologies\n- Smart grid innovations\n- Energy policy changes and incentives...', type='text')]
        ```
      </Step>
    </Steps>
  </Tab>

  <Tab title="TypeScript">
    <Steps>
      <Step title="设置您的 API 密钥">
        从 [Claude Console](/settings/keys) 获取您的 API 密钥并将其设置为环境变量：

        ```bash
        export ANTHROPIC_API_KEY='your-api-key-here'
        ```
      </Step>

      <Step title="安装 SDK">
        安装 Anthropic TypeScript SDK：

        ```bash
        npm install @anthropic-ai/sdk
        ```
      </Step>

      <Step title="创建您的代码">
        将其保存为 `quickstart.ts`：

        ```typescript
        import Anthropic from "@anthropic-ai/sdk";

        async function main() {
          const anthropic = new Anthropic();

          const msg = await anthropic.messages.create({
            model: "claude-sonnet-4-5",
            max_tokens: 1000,
            messages: [
              {
                role: "user",
                content: "What should I search for to find the latest developments in renewable energy?"
              }
            ]
          });
          console.log(msg);
        }

        main().catch(console.error);
        ```
      </Step>

      <Step title="运行您的代码">
        ```bash
        npx tsx quickstart.ts
        ```

        **示例输出：**
        ```javascript
        {
          id: 'msg_01ThFHzad6Bh4TpQ6cHux9t8',
          type: 'message',
          role: 'assistant',
          model: 'claude-sonnet-4-5-20250929',
          content: [
            {
              type: 'text',
              text: 'Here are some effective search strategies to find the latest renewable energy developments:\n\n' +
                '## Search Terms to Use:\n' +
                '- "renewable energy news 2024"\n' +
                '- "clean energy breakthroughs"\n' +
                '- "solar wind technology advances"\n' +
                '- "energy storage innovations"\n' +
                '- "green hydrogen developments"\n' +
                '- "offshore wind projects"\n' +
                '- "battery technology renewable"\n\n' +
                '## Best Sources to Check:\n\n' +
                '**News & Industry Sites:**\n' +
                '- Renewable Energy World\n' +
                '- CleanTechnica\n' +
                '- GreenTech Media (now Wood Mackenzie)\n' +
                '- Energy Storage News\n' +
                '- PV Magazine (for solar)...'
            }
          ],
          stop_reason: 'end_turn',
          usage: {
            input_tokens: 21,
            output_tokens: 302
          }
        }
        ```
      </Step>
    </Steps>
  </Tab>

  <Tab title="Java">
    <Steps>
      <Step title="设置您的 API 密钥">
        从 [Claude Console](/settings/keys) 获取您的 API 密钥并将其设置为环境变量：

        ```bash
        export ANTHROPIC_API_KEY='your-api-key-here'
        ```
      </Step>

      <Step title="安装 SDK">
        将 Anthropic Java SDK 添加到您的项目中。首先在 [Maven Central](https://central.sonatype.com/artifact/com.anthropic/anthropic-java) 上找到当前版本。

        **Gradle：**
        ```groovy
        implementation("com.anthropic:anthropic-java:1.0.0")
        ```

        **Maven：**
        ```xml
        <dependency>
          <groupId>com.anthropic</groupId>
          <artifactId>anthropic-java</artifactId>
          <version>1.0.0</version>
        </dependency>
        ```
      </Step>

      <Step title="创建您的代码">
        将其保存为 `QuickStart.java`：

        ```java
        import com.anthropic.client.AnthropicClient;
        import com.anthropic.client.okhttp.AnthropicOkHttpClient;
        import com.anthropic.models.messages.Message;
        import com.anthropic.models.messages.MessageCreateParams;

        public class QuickStart {
            public static void main(String[] args) {
                AnthropicClient client = AnthropicOkHttpClient.fromEnv();

                MessageCreateParams params = MessageCreateParams.builder()
                        .model("claude-sonnet-4-5-20250929")
                        .maxTokens(1000)
                        .addUserMessage("What should I search for to find the latest developments in renewable energy?")
                        .build();

                Message message = client.messages().create(params);
                System.out.println(message.content());
            }
        }
        ```
      </Step>

      <Step title="运行您的代码">
        ```bash
        javac QuickStart.java
        java QuickStart
        ```

        **示例输出：**
        ```java
        [ContentBlock{text=TextBlock{text=Here are some effective search strategies to find the latest renewable energy developments:

        ## Search Terms to Use:
        - "renewable energy news 2024"
        - "clean energy breakthroughs"  
        - "solar/wind/battery technology advances"
        - "energy storage innovations"
        - "green hydrogen developments"
        - "renewable energy policy updates"

        ## Best Sources to Check:
        - **News & Analysis:** Reuters Energy, Bloomberg New Energy Finance, Greentech Media
        - **Industry Publications:** Renewable Energy World, PV Magazine, Wind Power Engineering
        - **Research Organizations:** International Energy Agency (IEA), National Renewable Energy Laboratory (NREL)
        - **Government Sources:** Department of Energy websites, EPA clean energy updates

        ## Specific Topics to Explore:
        - Perovskite and next-gen solar cells
        - Offshore wind expansion
        - Grid-scale battery storage
        - Green hydrogen production..., type=text}}]
        ```
      </Step>
    </Steps>
  </Tab>
</Tabs>

## 后续步骤

现在您已经进行了第一次 Claude API 请求，是时候探索还有什么其他可能性了：

<CardGroup cols={3}>
  <Card title="使用消息" icon="messages" href="/docs/zh-CN/build-with-claude/working-with-messages">
    了解消息 API 的常见模式。
  </Card>
  <Card title="功能概览" icon="brain" href="/docs/zh-CN/api/overview">
    探索 Claude 的高级功能和能力。
  </Card>
  <Card title="客户端 SDK" icon="code-brackets" href="/docs/zh-CN/api/client-sdks">
    发现 Anthropic 客户端库。
  </Card>
  <Card title="Claude 食谱" icon="chef-hat" href="https://github.com/anthropics/anthropic-cookbook">
    通过交互式 Jupyter 笔记本学习。
  </Card>
</CardGroup>



# 模型概览

Claude 是由 Anthropic 开发的最先进的大型语言模型系列。本指南介绍了我们的模型并比较了它们的性能。

---

## 选择模型

如果您不确定使用哪个模型，我们建议从 **Claude Sonnet 4.5** 开始。它为大多数用例提供了最佳的智能、速度和成本平衡，在编码和代理任务中表现出色。

所有当前的 Claude 模型都支持文本和图像输入、文本输出、多语言功能和视觉能力。模型可通过 Anthropic API、AWS Bedrock 和 Google Vertex AI 获得。

选择模型后，[了解如何进行第一次 API 调用](/docs/zh-CN/get-started)。

### 最新模型比较

| 功能 | Claude Sonnet 4.5 | Claude Haiku 4.5 | Claude Opus 4.5 | Claude Opus 4.1 |
|:--------|:------------------|:-----------------|:----------------|:----------------|
| **描述** | 我们用于复杂代理和编码的最聪明的模型 | 我们具有接近前沿智能的最快模型 | 结合最大智能和实际性能的高级模型 | 用于专门推理任务的卓越模型 |
| **Claude API ID** | claude-sonnet-4-5-20250929 | claude-haiku-4-5-20251001 | claude-opus-4-5-20251101 | claude-opus-4-1-20250805 |
| **Claude API 别名**<sup>1</sup> | claude-sonnet-4-5 | claude-haiku-4-5 | claude-opus-4-5 | claude-opus-4-1 |
| **AWS Bedrock ID** | anthropic.claude-sonnet-4-5-20250929-v1:0 | anthropic.claude-haiku-4-5-20251001-v1:0 | anthropic.claude-opus-4-5-20251101-v1:0 | anthropic.claude-opus-4-1-20250805-v1:0 |
| **GCP Vertex AI ID** | claude-sonnet-4-5@20250929 | claude-haiku-4-5@20251001 | claude-opus-4-5@20251101 | claude-opus-4-1@20250805 |
| **定价**<sup>2</sup> | \$3 / 输入 MTok<br/>\$15 / 输出 MTok | \$1 / 输入 MTok<br/>\$5 / 输出 MTok | \$5 / 输入 MTok<br/>\$25 / 输出 MTok | \$15 / 输入 MTok<br/>\$75 / 输出 MTok |
| **[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)** | 是 | 是 | 是 | 是 |
| **[优先级层级](/docs/zh-CN/api/service-tiers)** | 是 | 是 | 是 | 是 |
| **相对延迟** | 快速 | 最快 | 中等 | 中等 |
| **上下文窗口** | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> / <br/> <Tooltip tooltipContent="~750K 词 \ ~3.4M unicode 字符">1M tokens</Tooltip> (beta)<sup>3</sup> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> |
| **最大输出** | 64K tokens | 64K tokens | 64K tokens | 32K tokens |
| **可靠知识截止日期** | 2025 年 1 月<sup>4</sup> | 2025 年 2 月 | 2025 年 3 月<sup>4</sup> | 2025 年 1 月<sup>4</sup> |
| **训练数据截止日期** | 2025 年 7 月 | 2025 年 7 月 | 2025 年 8 月 | 2025 年 3 月 |

_<sup>1 - 别名自动指向最新的模型快照。当我们发布新的模型快照时，我们会迁移别名以指向最新版本的模型，通常在新版本发布后一周内。虽然别名对于实验很有用，但我们建议在生产应用中使用特定的模型版本（例如 `claude-sonnet-4-5-20250929`）以确保一致的行为。</sup>_

_<sup>2 - 请参阅我们的[定价页面](/docs/zh-CN/about-claude/pricing)了解完整的定价信息，包括批处理 API 折扣、提示缓存费率、扩展思考成本和视觉处理费用。</sup>_

_<sup>3 - 使用 `context-1m-2025-08-07` beta 标头时，Claude Sonnet 4.5 支持 [1M token 上下文窗口](/docs/zh-CN/build-with-claude/context-windows#1m-token-context-window)。[长上下文定价](/docs/zh-CN/about-claude/pricing#long-context-pricing)适用于超过 200K tokens 的请求。</sup>_

_<sup>4 - **可靠知识截止日期**表示模型知识最广泛和最可靠的日期。**训练数据截止日期**是使用的训练数据的更广泛日期范围。例如，Claude Sonnet 4.5 在 2025 年 7 月通过公开可用的信息进行了训练，但其知识在 2025 年 1 月最广泛和最可靠。有关更多信息，请参阅 [Anthropic 的透明度中心](https://www.anthropic.com/transparency)。</sup>_

<Note>具有相同快照日期的模型（例如 20240620）在所有平台上是相同的，不会改变。模型名称中的快照日期确保了一致性，允许开发人员在不同环境中依赖稳定的性能。</Note>

<Note>从 **Claude Sonnet 4.5 和所有未来模型**开始，AWS Bedrock 和 Google Vertex AI 提供两种端点类型：**全局端点**（用于最大可用性的动态路由）和**区域端点**（通过特定地理区域保证数据路由）。有关更多信息，请参阅[第三方平台定价部分](/docs/zh-CN/about-claude/pricing#third-party-platform-pricing)。</Note>

<section title="旧版模型">

以下模型仍然可用，但我们建议迁移到当前模型以获得改进的性能：

| 功能 | Claude Sonnet 4 | Claude Sonnet 3.7 | Claude Opus 4 | Claude Haiku 3.5 | Claude Haiku 3 |
|:--------|:----------------|:------------------|:--------------|:-----------------|:---------------|
| **Claude API ID** | claude-sonnet-4-20250514 | claude-3-7-sonnet-20250219 | claude-opus-4-20250514 | claude-3-5-haiku-20241022 | claude-3-haiku-20240307 |
| **Claude API 别名** | claude-sonnet-4-0 | claude-3-7-sonnet-latest | claude-opus-4-0 | claude-3-5-haiku-latest | — |
| **AWS Bedrock ID** | anthropic.claude-sonnet-4-20250514-v1:0 | anthropic.claude-3-7-sonnet-20250219-v1:0 | anthropic.claude-opus-4-20250514-v1:0 | anthropic.claude-3-5-haiku-20241022-v1:0 | anthropic.claude-3-haiku-20240307-v1:0 |
| **GCP Vertex AI ID** | claude-sonnet-4@20250514 | claude-3-7-sonnet@20250219 | claude-opus-4@20250514 | claude-3-5-haiku@20241022 | claude-3-haiku@20240307 |
| **定价** | \$3 / 输入 MTok<br/>\$15 / 输出 MTok | \$3 / 输入 MTok<br/>\$15 / 输出 MTok | \$15 / 输入 MTok<br/>\$75 / 输出 MTok | \$0.80 / 输入 MTok<br/>\$4 / 输出 MTok | \$0.25 / 输入 MTok<br/>\$1.25 / 输出 MTok |
| **[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)** | 是 | 是 | 是 | 否 | 否 |
| **[优先级层级](/docs/zh-CN/api/service-tiers)** | 是 | 是 | 是 | 是 | 否 |
| **相对延迟** | 快速 | 快速 | 中等 | 最快 | 快速 |
| **上下文窗口** | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> / <br/> <Tooltip tooltipContent="~750K 词 \ ~3.4M unicode 字符">1M tokens</Tooltip> (beta)<sup>1</sup> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~215K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> |
| **最大输出** | 64K tokens | 64K tokens / 128K tokens (beta)<sup>4</sup> | 32K tokens | 8K tokens | 4K tokens |
| **可靠知识截止日期** | 2025 年 1 月<sup>2</sup> | 2024 年 10 月<sup>2</sup> | 2025 年 1 月<sup>2</sup> | <sup>3</sup> | <sup>3</sup> |
| **训练数据截止日期** | 2025 年 3 月 | 2024 年 11 月 | 2025 年 3 月 | 2024 年 7 月 | 2023 年 8 月 |

_<sup>1 - Claude Sonnet 4 在使用 `context-1m-2025-08-07` beta 标头时支持 [1M token 上下文窗口](/docs/zh-CN/build-with-claude/context-windows#1m-token-context-window)。[长上下文定价](/docs/zh-CN/about-claude/pricing#long-context-pricing)适用于超过 200K tokens 的请求。</sup>_

_<sup>2 - **可靠知识截止日期**表示模型知识最广泛和最可靠的日期。**训练数据截止日期**是使用的训练数据的更广泛日期范围。</sup>_

_<sup>3 - 某些 Haiku 模型有一个单一的训练数据截止日期。</sup>_

_<sup>4 - 在您的 API 请求中包含 beta 标头 `output-128k-2025-02-19` 以将 Claude Sonnet 3.7 的最大输出 token 长度增加到 128K tokens。我们强烈建议使用我们的[流式 Messages API](/docs/zh-CN/build-with-claude/streaming) 以避免在生成较长输出时超时。有关更多详情，请参阅我们关于[长请求](/docs/zh-CN/api/errors#long-requests)的指导。</sup>_

</section>

## 提示和输出性能

Claude 4 模型在以下方面表现出色：
- **性能**：在推理、编码、多语言任务、长上下文处理、诚实性和图像处理方面取得顶级结果。有关更多信息，请参阅 [Claude 4 博客文章](http://www.anthropic.com/news/claude-4)。
- **引人入胜的响应**：Claude 模型非常适合需要丰富、类似人类交互的应用程序。

    - 如果您更喜欢更简洁的响应，可以调整您的提示以引导模型朝向所需的输出长度。有关详情，请参阅我们的[提示工程指南](/docs/zh-CN/build-with-claude/prompt-engineering)。
    - 有关特定的 Claude 4 提示最佳实践，请参阅我们的 [Claude 4 最佳实践指南](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices)。
- **输出质量**：从以前的模型代代迁移到 Claude 4 时，您可能会注意到整体性能的更大改进。

## 迁移到 Claude 4.5

如果您当前使用 Claude 3 模型，我们建议迁移到 Claude 4.5 以利用改进的智能和增强的功能。有关详细的迁移说明，请参阅[迁移到 Claude 4.5](/docs/zh-CN/about-claude/models/migrating-to-claude-4)。

## 开始使用 Claude

如果您已准备好开始探索 Claude 能为您做什么，让我们深入了解吧！无论您是想将 Claude 集成到应用程序中的开发人员，还是想亲身体验 AI 力量的用户，我们都能为您提供帮助。

<Note>想与 Claude 聊天？访问 [claude.ai](http://www.claude.ai)！</Note>

<CardGroup cols={3}>
  <Card title="Claude 简介" icon="check" href="/docs/zh-CN/intro">
    探索 Claude 的功能和开发流程。
  </Card>
  <Card title="快速入门" icon="lightning" href="/docs/zh-CN/get-started">
    了解如何在几分钟内进行第一次 API 调用。
  </Card>
  <Card title="Claude 控制台" icon="code" href="/">
    直接在浏览器中制作和测试强大的提示。
  </Card>
</CardGroup>

如果您有任何问题或需要帮助，请随时联系我们的[支持团队](https://support.claude.com/)或查阅 [Discord 社区](https://www.anthropic.com/discord)。


# 模型概览

Claude 是由 Anthropic 开发的最先进的大型语言模型系列。本指南介绍了我们的模型并比较了它们的性能。

---

## 选择模型

如果您不确定使用哪个模型，我们建议从 **Claude Sonnet 4.5** 开始。它为大多数用例提供了最佳的智能、速度和成本平衡，在编码和代理任务中表现出色。

所有当前的 Claude 模型都支持文本和图像输入、文本输出、多语言功能和视觉能力。模型可通过 Anthropic API、AWS Bedrock 和 Google Vertex AI 获得。

选择模型后，[了解如何进行第一次 API 调用](/docs/zh-CN/get-started)。

### 最新模型比较

| 功能 | Claude Sonnet 4.5 | Claude Haiku 4.5 | Claude Opus 4.5 | Claude Opus 4.1 |
|:--------|:------------------|:-----------------|:----------------|:----------------|
| **描述** | 我们用于复杂代理和编码的最聪明的模型 | 我们具有接近前沿智能的最快模型 | 结合最大智能和实际性能的高级模型 | 用于专门推理任务的卓越模型 |
| **Claude API ID** | claude-sonnet-4-5-20250929 | claude-haiku-4-5-20251001 | claude-opus-4-5-20251101 | claude-opus-4-1-20250805 |
| **Claude API 别名**<sup>1</sup> | claude-sonnet-4-5 | claude-haiku-4-5 | claude-opus-4-5 | claude-opus-4-1 |
| **AWS Bedrock ID** | anthropic.claude-sonnet-4-5-20250929-v1:0 | anthropic.claude-haiku-4-5-20251001-v1:0 | anthropic.claude-opus-4-5-20251101-v1:0 | anthropic.claude-opus-4-1-20250805-v1:0 |
| **GCP Vertex AI ID** | claude-sonnet-4-5@20250929 | claude-haiku-4-5@20251001 | claude-opus-4-5@20251101 | claude-opus-4-1@20250805 |
| **定价**<sup>2</sup> | \$3 / 输入 MTok<br/>\$15 / 输出 MTok | \$1 / 输入 MTok<br/>\$5 / 输出 MTok | \$5 / 输入 MTok<br/>\$25 / 输出 MTok | \$15 / 输入 MTok<br/>\$75 / 输出 MTok |
| **[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)** | 是 | 是 | 是 | 是 |
| **[优先级层级](/docs/zh-CN/api/service-tiers)** | 是 | 是 | 是 | 是 |
| **相对延迟** | 快速 | 最快 | 中等 | 中等 |
| **上下文窗口** | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> / <br/> <Tooltip tooltipContent="~750K 词 \ ~3.4M unicode 字符">1M tokens</Tooltip> (beta)<sup>3</sup> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> |
| **最大输出** | 64K tokens | 64K tokens | 64K tokens | 32K tokens |
| **可靠知识截止日期** | 2025 年 1 月<sup>4</sup> | 2025 年 2 月 | 2025 年 3 月<sup>4</sup> | 2025 年 1 月<sup>4</sup> |
| **训练数据截止日期** | 2025 年 7 月 | 2025 年 7 月 | 2025 年 8 月 | 2025 年 3 月 |

_<sup>1 - 别名自动指向最新的模型快照。当我们发布新的模型快照时，我们会迁移别名以指向最新版本的模型，通常在新版本发布后一周内。虽然别名对于实验很有用，但我们建议在生产应用中使用特定的模型版本（例如 `claude-sonnet-4-5-20250929`）以确保一致的行为。</sup>_

_<sup>2 - 请参阅我们的[定价页面](/docs/zh-CN/about-claude/pricing)了解完整的定价信息，包括批处理 API 折扣、提示缓存费率、扩展思考成本和视觉处理费用。</sup>_

_<sup>3 - 使用 `context-1m-2025-08-07` beta 标头时，Claude Sonnet 4.5 支持 [1M token 上下文窗口](/docs/zh-CN/build-with-claude/context-windows#1m-token-context-window)。[长上下文定价](/docs/zh-CN/about-claude/pricing#long-context-pricing)适用于超过 200K tokens 的请求。</sup>_

_<sup>4 - **可靠知识截止日期**表示模型知识最广泛和最可靠的日期。**训练数据截止日期**是使用的训练数据的更广泛日期范围。例如，Claude Sonnet 4.5 在 2025 年 7 月通过公开可用的信息进行了训练，但其知识在 2025 年 1 月最广泛和最可靠。有关更多信息，请参阅 [Anthropic 的透明度中心](https://www.anthropic.com/transparency)。</sup>_

<Note>具有相同快照日期的模型（例如 20240620）在所有平台上是相同的，不会改变。模型名称中的快照日期确保了一致性，允许开发人员在不同环境中依赖稳定的性能。</Note>

<Note>从 **Claude Sonnet 4.5 和所有未来模型**开始，AWS Bedrock 和 Google Vertex AI 提供两种端点类型：**全局端点**（用于最大可用性的动态路由）和**区域端点**（通过特定地理区域保证数据路由）。有关更多信息，请参阅[第三方平台定价部分](/docs/zh-CN/about-claude/pricing#third-party-platform-pricing)。</Note>

<section title="旧版模型">

以下模型仍然可用，但我们建议迁移到当前模型以获得改进的性能：

| 功能 | Claude Sonnet 4 | Claude Sonnet 3.7 | Claude Opus 4 | Claude Haiku 3.5 | Claude Haiku 3 |
|:--------|:----------------|:------------------|:--------------|:-----------------|:---------------|
| **Claude API ID** | claude-sonnet-4-20250514 | claude-3-7-sonnet-20250219 | claude-opus-4-20250514 | claude-3-5-haiku-20241022 | claude-3-haiku-20240307 |
| **Claude API 别名** | claude-sonnet-4-0 | claude-3-7-sonnet-latest | claude-opus-4-0 | claude-3-5-haiku-latest | — |
| **AWS Bedrock ID** | anthropic.claude-sonnet-4-20250514-v1:0 | anthropic.claude-3-7-sonnet-20250219-v1:0 | anthropic.claude-opus-4-20250514-v1:0 | anthropic.claude-3-5-haiku-20241022-v1:0 | anthropic.claude-3-haiku-20240307-v1:0 |
| **GCP Vertex AI ID** | claude-sonnet-4@20250514 | claude-3-7-sonnet@20250219 | claude-opus-4@20250514 | claude-3-5-haiku@20241022 | claude-3-haiku@20240307 |
| **定价** | \$3 / 输入 MTok<br/>\$15 / 输出 MTok | \$3 / 输入 MTok<br/>\$15 / 输出 MTok | \$15 / 输入 MTok<br/>\$75 / 输出 MTok | \$0.80 / 输入 MTok<br/>\$4 / 输出 MTok | \$0.25 / 输入 MTok<br/>\$1.25 / 输出 MTok |
| **[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)** | 是 | 是 | 是 | 否 | 否 |
| **[优先级层级](/docs/zh-CN/api/service-tiers)** | 是 | 是 | 是 | 是 | 否 |
| **相对延迟** | 快速 | 快速 | 中等 | 最快 | 快速 |
| **上下文窗口** | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> / <br/> <Tooltip tooltipContent="~750K 词 \ ~3.4M unicode 字符">1M tokens</Tooltip> (beta)<sup>1</sup> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~215K unicode 字符">200K tokens</Tooltip> | <Tooltip tooltipContent="~150K 词 \ ~680K unicode 字符">200K tokens</Tooltip> |
| **最大输出** | 64K tokens | 64K tokens / 128K tokens (beta)<sup>4</sup> | 32K tokens | 8K tokens | 4K tokens |
| **可靠知识截止日期** | 2025 年 1 月<sup>2</sup> | 2024 年 10 月<sup>2</sup> | 2025 年 1 月<sup>2</sup> | <sup>3</sup> | <sup>3</sup> |
| **训练数据截止日期** | 2025 年 3 月 | 2024 年 11 月 | 2025 年 3 月 | 2024 年 7 月 | 2023 年 8 月 |

_<sup>1 - Claude Sonnet 4 在使用 `context-1m-2025-08-07` beta 标头时支持 [1M token 上下文窗口](/docs/zh-CN/build-with-claude/context-windows#1m-token-context-window)。[长上下文定价](/docs/zh-CN/about-claude/pricing#long-context-pricing)适用于超过 200K tokens 的请求。</sup>_

_<sup>2 - **可靠知识截止日期**表示模型知识最广泛和最可靠的日期。**训练数据截止日期**是使用的训练数据的更广泛日期范围。</sup>_

_<sup>3 - 某些 Haiku 模型有一个单一的训练数据截止日期。</sup>_

_<sup>4 - 在您的 API 请求中包含 beta 标头 `output-128k-2025-02-19` 以将 Claude Sonnet 3.7 的最大输出 token 长度增加到 128K tokens。我们强烈建议使用我们的[流式 Messages API](/docs/zh-CN/build-with-claude/streaming) 以避免在生成较长输出时超时。有关更多详情，请参阅我们关于[长请求](/docs/zh-CN/api/errors#long-requests)的指导。</sup>_

</section>

## 提示和输出性能

Claude 4 模型在以下方面表现出色：
- **性能**：在推理、编码、多语言任务、长上下文处理、诚实性和图像处理方面取得顶级结果。有关更多信息，请参阅 [Claude 4 博客文章](http://www.anthropic.com/news/claude-4)。
- **引人入胜的响应**：Claude 模型非常适合需要丰富、类似人类交互的应用程序。

    - 如果您更喜欢更简洁的响应，可以调整您的提示以引导模型朝向所需的输出长度。有关详情，请参阅我们的[提示工程指南](/docs/zh-CN/build-with-claude/prompt-engineering)。
    - 有关特定的 Claude 4 提示最佳实践，请参阅我们的 [Claude 4 最佳实践指南](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices)。
- **输出质量**：从以前的模型代代迁移到 Claude 4 时，您可能会注意到整体性能的更大改进。

## 迁移到 Claude 4.5

如果您当前使用 Claude 3 模型，我们建议迁移到 Claude 4.5 以利用改进的智能和增强的功能。有关详细的迁移说明，请参阅[迁移到 Claude 4.5](/docs/zh-CN/about-claude/models/migrating-to-claude-4)。

## 开始使用 Claude

如果您已准备好开始探索 Claude 能为您做什么，让我们深入了解吧！无论您是想将 Claude 集成到应用程序中的开发人员，还是想亲身体验 AI 力量的用户，我们都能为您提供帮助。

<Note>想与 Claude 聊天？访问 [claude.ai](http://www.claude.ai)！</Note>

<CardGroup cols={3}>
  <Card title="Claude 简介" icon="check" href="/docs/zh-CN/intro">
    探索 Claude 的功能和开发流程。
  </Card>
  <Card title="快速入门" icon="lightning" href="/docs/zh-CN/get-started">
    了解如何在几分钟内进行第一次 API 调用。
  </Card>
  <Card title="Claude 控制台" icon="code" href="/">
    直接在浏览器中制作和测试强大的提示。
  </Card>
</CardGroup>

如果您有任何问题或需要帮助，请随时联系我们的[支持团队](https://support.claude.com/)或查阅 [Discord 社区](https://www.anthropic.com/discord)。

# Claude 4.5 的新功能

了解 Claude 4.5 中引入的三个新模型、关键改进、新 API 功能和定价信息。

---

Claude 4.5 引入了三个为不同用例设计的模型：

- **Claude Opus 4.5**：我们最聪慧的模型，结合了最大能力与实际性能。非常适合复杂的专业任务、专业软件工程和高级代理。价格点比之前的 Opus 模型更容易接受
- **Claude Sonnet 4.5**：我们最适合复杂代理和编码的模型，在大多数任务中具有最高的智能
- **Claude Haiku 4.5**：我们最快且最聪慧的 Haiku 模型，具有接近前沿的性能。第一个支持扩展思考的 Haiku 模型

## Opus 4.5 相比 Opus 4.1 的关键改进

### 最大智能

Claude Opus 4.5 代表了我们最聪慧的模型，结合了最大能力与实际性能。它在推理、编码和复杂问题解决任务中提供了阶跃式改进，同时保持了 Opus 系列所期望的高质量输出。

### 努力参数

Claude Opus 4.5 是唯一支持[努力参数](/docs/zh-CN/build-with-claude/effort)的模型，允许您控制 Claude 在响应时使用多少令牌。这使您能够在单个模型中权衡响应的彻底性和令牌效率。

努力参数影响响应中的所有令牌，包括文本响应、工具调用和扩展思考。您可以选择：
- **高努力**：用于复杂分析和详细解释的最大彻底性
- **中等努力**：适用于大多数生产用例的平衡方法
- **低努力**：用于高容量自动化的最令牌高效响应

### 计算机使用卓越性

Claude Opus 4.5 引入了[增强的计算机使用功能](/docs/zh-CN/agents-and-tools/tool-use/computer-use-tool)，具有新的缩放操作，可以以全分辨率详细检查特定屏幕区域。这使 Claude 能够检查细粒度的 UI 元素、小文本和详细的视觉信息，这些信息在标准屏幕截图中可能不清楚。

缩放功能特别适用于：
- 检查小型 UI 元素和控件
- 阅读小字体或详细文本
- 分析包含密集信息的复杂界面
- 在采取行动前验证精确的视觉细节

### 实际性能

Claude Opus 4.5 以[更容易接受的价格点](/docs/zh-CN/about-claude/pricing)提供旗舰级智能，比之前的 Opus 模型更容易获得，使高级 AI 功能可用于更广泛的应用和用例。

### 思考块保留

Claude Opus 4.5 [自动保留整个对话中的所有先前思考块](/docs/zh-CN/build-with-claude/extended-thinking#thinking-block-preservation-in-claude-opus-4-5)，在扩展的多轮交互和工具使用会话中保持推理连续性。这确保 Claude 在处理复杂的长期运行任务时能够有效地利用其完整的推理历史。

## Sonnet 4.5 相比 Sonnet 4 的关键改进

### 编码卓越性

Claude Sonnet 4.5 是我们迄今为止最好的编码模型，在整个开发生命周期中有显著改进：

- **SWE-bench 验证性能**：编码基准上的先进最先进技术
- **增强的规划和系统设计**：更好的架构决策和代码组织
- **改进的安全工程**：更强大的安全实践和漏洞检测
- **更好的指令遵循**：更精确地遵循编码规范和要求

<Note>
当[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)启用时，Claude Sonnet 4.5 在编码任务上的表现明显更好。扩展思考默认禁用，但我们建议为复杂编码工作启用它。请注意，扩展思考会影响[提示缓存效率](/docs/zh-CN/build-with-claude/prompt-caching#caching-with-thinking-blocks)。有关配置详情，请参阅[迁移指南](/docs/zh-CN/about-claude/models/migrating-to-claude-4#extended-thinking-recommendations)。
</Note>

### 代理功能

Claude Sonnet 4.5 在代理功能中引入了重大进步：

- **扩展自主操作**：Sonnet 4.5 可以独立工作数小时，同时保持对增量进展的清晰关注。该模型一次在几个任务上稳步推进，而不是试图同时完成所有任务。它提供基于事实的进度更新，准确反映已完成的工作。
- **上下文感知**：Claude 现在在整个对话中跟踪其令牌使用情况，在每次工具调用后接收更新。这种感知有助于防止过早放弃任务，并在长期运行任务上实现更有效的执行。有关技术详情，请参阅[上下文感知](/docs/zh-CN/build-with-claude/context-windows#context-awareness-in-claude-sonnet-4-5)，以及[提示指导](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices#context-awareness-and-multi-window-workflows)。
- **增强的工具使用**：该模型更有效地使用并行工具调用，在研究期间同时发起多个推测性搜索，并一次读取多个文件以更快地构建上下文。跨多个工具和信息源的改进协调使模型能够在代理搜索和编码工作流中有效地利用广泛的功能。
- **高级上下文管理**：Sonnet 4.5 在外部文件中保持异常的状态跟踪，在会话中保持目标导向。结合更有效的上下文窗口使用和我们新的上下文管理 API 功能，该模型最优地处理扩展会话中的信息，以随时间保持连贯性。

<Note>上下文感知在 Claude Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。</Note>

### 通信和交互风格

Claude Sonnet 4.5 具有精炼的通信方法，简洁、直接且自然。它提供基于事实的进度更新，可能在工具调用后跳过冗长的摘要以保持工作流动量（尽管这可以通过提示调整）。

有关使用此通信风格的详细指导，请参阅 [Claude 4 最佳实践](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices)。

### 创意内容生成

Claude Sonnet 4.5 在创意内容任务中表现出色：

- **演示文稿和动画**：在创建幻灯片和视觉内容方面与 Claude Opus 4.1 和 Opus 4.5 相匹配或超越
- **创意风格**：生成具有强大指令遵循的精美、专业输出
- **首次质量**：在初始尝试中生成可用的、设计精良的内容

## Haiku 4.5 相比 Haiku 3.5 的关键改进

Claude Haiku 4.5 代表了 Haiku 模型系列的变革性飞跃，为我们最快的模型类别带来了前沿功能：

### 以闪电般的速度实现接近前沿的智能

Claude Haiku 4.5 以显著更低的成本和更快的速度提供与 Sonnet 4 相匹配的接近前沿的性能：

- **接近前沿的智能**：在推理、编码和复杂任务中与 Sonnet 4 性能相匹配
- **增强的速度**：比 Sonnet 4 快两倍多，针对每秒输出令牌 (OTPS) 进行了优化
- **最优成本性能**：以三分之一的成本实现接近前沿的智能，非常适合大容量部署

### 扩展思考功能

Claude Haiku 4.5 是**第一个支持扩展思考的 Haiku 模型**，为 Haiku 系列带来了高级推理功能：

- **速度推理**：访问 Claude 的内部推理过程以解决复杂问题
- **思考总结**：为生产就绪部署总结思考输出
- **交错思考**：在工具调用之间思考，以实现更复杂的多步工作流
- **预算控制**：配置思考令牌预算以平衡推理深度和速度

必须通过向 API 请求添加 `thinking` 参数来显式启用扩展思考。有关实现详情，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking)。

<Note>
当[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)启用时，Claude Haiku 4.5 在编码和推理任务上的表现明显更好。扩展思考默认禁用，但我们建议为复杂问题解决、编码工作和多步推理启用它。请注意，扩展思考会影响[提示缓存效率](/docs/zh-CN/build-with-claude/prompt-caching#caching-with-thinking-blocks)。有关配置详情，请参阅[迁移指南](/docs/zh-CN/about-claude/models/migrating-to-claude-4#extended-thinking-recommendations)。
</Note>

<Note>在 Claude Sonnet 3.7、Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。</Note>

### 上下文感知

Claude Haiku 4.5 具有**上下文感知**功能，使模型能够在整个对话中跟踪其剩余上下文窗口：

- **令牌预算跟踪**：Claude 在每次工具调用后接收关于剩余上下文容量的实时更新
- **更好的任务持久性**：该模型可以通过理解可用的工作空间来更有效地执行任务
- **多上下文窗口工作流**：改进了跨扩展会话的状态转换处理

这是第一个具有本地上下文感知功能的 Haiku 模型。有关提示指导，请参阅 [Claude 4 最佳实践](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices#context-awareness-and-multi-window-workflows)。

<Note>在 Claude Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。</Note>

### 强大的编码和工具使用

Claude Haiku 4.5 提供了现代 Claude 模型所期望的强大编码功能：

- **编码能力**：在代码生成、调试和重构任务中的强大性能
- **完整工具支持**：与所有 Claude 4 工具兼容，包括 bash、代码执行、文本编辑器、网络搜索和计算机使用
- **增强的计算机使用**：针对自主桌面交互和浏览器自动化工作流进行了优化
- **并行工具执行**：跨多个工具的高效协调以实现复杂工作流

Haiku 4.5 设计用于需要智能和效率的用例：

- **实时应用**：用于交互式用户体验的快速响应时间
- **高容量处理**：用于大规模部署的成本有效的智能
- **免费层实现**：以可接受的价格提供高级模型质量
- **子代理架构**：用于多代理系统的快速、聪慧的代理
- **大规模计算机使用**：成本有效的自主桌面和浏览器自动化

## 新 API 功能

### 程序化工具调用（测试版）

[程序化工具调用](/docs/zh-CN/agents-and-tools/tool-use/programmatic-tool-calling)允许 Claude 在代码执行容器内以编程方式编写调用工具的代码，而不是需要为每个工具调用通过模型进行往返。这显著降低了多工具工作流的延迟，并通过允许 Claude 在数据到达模型的上下文窗口之前过滤或处理数据来减少令牌消耗。

```python
tools=[
    {
        "type": "code_execution_20250825",
        "name": "code_execution"
    },
    {
        "name": "query_database",
        "description": "Execute a SQL query against the sales database. Returns a list of rows as JSON objects.",
        "input_schema": {...},
        "allowed_callers": ["code_execution_20250825"]  # Enable programmatic calling
    }
]
```

关键优势：
- **降低延迟**：消除工具调用之间的模型往返
- **令牌效率**：在返回给 Claude 之前以编程方式处理和过滤工具结果
- **复杂工作流**：支持循环、条件逻辑和批处理

<Note>在 Claude Opus 4.5 和 Claude Sonnet 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`advanced-tool-use-2025-11-20`</Note>

### 工具搜索工具（测试版）

[工具搜索工具](/docs/zh-CN/agents-and-tools/tool-use/tool-search-tool)使 Claude 能够通过动态发现和按需加载工具来处理数百或数千个工具。Claude 搜索工具目录并仅加载所需的工具，而不是预先将所有工具定义加载到上下文窗口中。

有两种搜索变体可用：
- **正则表达式** (`tool_search_tool_regex_20251119`)：Claude 构造正则表达式模式来搜索工具名称、描述和参数
- **BM25** (`tool_search_tool_bm25_20251119`)：Claude 使用自然语言查询来搜索工具

```python
tools=[
    {
        "type": "tool_search_tool_regex_20251119",
        "name": "tool_search_tool_regex"
    },
    {
        "name": "get_weather",
        "description": "Get the weather at a specific location",
        "input_schema": {...},
        "defer_loading": True  # Load on-demand via search
    }
]
```

这种方法解决了两个关键挑战：
- **上下文效率**：通过不预先加载所有工具定义来节省 10-20K 令牌
- **工具选择准确性**：即使有 100+ 个可用工具也保持高准确性

<Note>在 Claude Opus 4.5 和 Claude Sonnet 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`advanced-tool-use-2025-11-20`</Note>

### 努力参数（测试版）

[努力参数](/docs/zh-CN/build-with-claude/effort)允许您控制 Claude 在响应时使用多少令牌，在响应彻底性和令牌效率之间进行权衡：

```python
response = client.beta.messages.create(
    model="claude-opus-4-5-20251101",
    betas=["effort-2025-11-24"],
    max_tokens=4096,
    messages=[{"role": "user", "content": "..."}],
    output_config={
        "effort": "medium"  # "low", "medium", or "high"
    }
)
```

努力参数影响响应中的所有令牌，包括文本响应、工具调用和扩展思考。较低的努力级别会产生更简洁的响应，最少解释，而较高的努力提供详细的推理和全面的答案。

<Note>仅在 Claude Opus 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`effort-2025-11-24`</Note>

### 工具使用示例（测试版）

[工具使用示例](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#providing-tool-use-examples)允许您提供有效工具输入的具体示例，以帮助 Claude 更有效地理解如何使用工具。这对于具有嵌套对象、可选参数或格式敏感输入的复杂工具特别有用。

```python
tools=[
    {
        "name": "get_weather",
        "description": "Get the current weather in a given location",
        "input_schema": {...},
        "input_examples": [
            {
                "location": "San Francisco, CA",
                "unit": "fahrenheit"
            },
            {
                "location": "Tokyo, Japan",
                "unit": "celsius"
            },
            {
                "location": "New York, NY"  # Demonstrates optional 'unit' parameter
            }
        ]
    }
]
```

示例包含在提示中以及工具架构，向 Claude 展示格式良好的工具调用的具体模式。每个示例必须根据工具的 `input_schema` 有效。

<Note>在 Claude Sonnet 4.5、Haiku 4.5、Opus 4.5、Opus 4.1 和 Opus 4 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`advanced-tool-use-2025-11-20`。</Note>

### 内存工具（测试版）

新的[内存工具](/docs/zh-CN/agents-and-tools/tool-use/memory-tool)使 Claude 能够在上下文窗口外存储和检索信息：

```python
tools=[
    {
        "type": "memory_20250818",
        "name": "memory"
    }
]
```

这允许：
- 随时间构建知识库
- 跨会话维护项目状态
- 通过基于文件的存储保留有效无限的上下文

<Note>在 Claude Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`context-management-2025-06-27`</Note>

### 上下文编辑

使用[上下文编辑](/docs/zh-CN/build-with-claude/context-editing)通过自动工具调用清除进行智能上下文管理：

```python
response = client.beta.messages.create(
    betas=["context-management-2025-06-27"],
    model="claude-sonnet-4-5",  # or claude-haiku-4-5
    max_tokens=4096,
    messages=[{"role": "user", "content": "..."}],
    context_management={
        "edits": [
            {
                "type": "clear_tool_uses_20250919",
                "trigger": {"type": "input_tokens", "value": 500},
                "keep": {"type": "tool_uses", "value": 2},
                "clear_at_least": {"type": "input_tokens", "value": 100}
            }
        ]
    },
    tools=[...]
)
```

此功能在接近令牌限制时自动删除较旧的工具调用和结果，帮助在长期运行的代理会话中管理上下文。

<Note>在 Claude Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`context-management-2025-06-27`</Note>

### 增强的停止原因

Claude 4.5 模型引入了新的 `model_context_window_exceeded` 停止原因，明确指示生成何时因达到上下文窗口限制而停止，而不是请求的 `max_tokens` 限制。这使在应用程序逻辑中更容易处理上下文窗口限制。

```json
{
  "stop_reason": "model_context_window_exceeded",
  "usage": {
    "input_tokens": 150000,
    "output_tokens": 49950
  }
}
```

### 改进的工具参数处理

Claude 4.5 模型包括一个错误修复，在工具调用字符串参数中保留有意的格式。以前，字符串参数中的尾随换行符有时会被错误地剥离。此修复确保需要精确格式的工具（如文本编辑器）接收完全按预期的参数。

<Note>
这是一个幕后改进，不需要 API 更改。但是，具有字符串参数的工具现在可能会接收以前被剥离的尾随换行符的值。
</Note>

**示例：**

```json
// 之前：最后的换行符被意外剥离
{
  "type": "tool_use",
  "id": "toolu_01A09q90qw90lq917835lq9",
  "name": "edit_todo",
  "input": {
    "file": "todo.txt",
    "contents": "1. Chop onions.\n2. ???\n3. Profit"
  }
}

// 之后：尾随换行符按预期保留
{
  "type": "tool_use",
  "id": "toolu_01A09q90qw90lq917835lq9",
  "name": "edit_todo",
  "input": {
    "file": "todo.txt",
    "contents": "1. Chop onions.\n2. ???\n3. Profit\n"
  }
}
```

### 令牌计数优化

Claude 4.5 模型包括自动优化以改进模型性能。这些优化可能会向请求添加少量令牌，但**您不会为这些系统添加的令牌付费**。

## Claude 4 中引入的功能

以下功能在 Claude 4 中引入，在所有 Claude 4 模型中可用，包括 Claude Sonnet 4.5 和 Claude Haiku 4.5。

### 新的拒绝停止原因

Claude 4 模型为模型因安全原因拒绝生成的内容引入了新的 `refusal` 停止原因：

```json
{
  "id": "msg_014XEDjypDjFzgKVWdFUXxZP",
  "type": "message",
  "role": "assistant",
  "model": "claude-sonnet-4-5",
  "content": [{"type": "text", "text": "I would be happy to assist you. You can "}],
  "stop_reason": "refusal",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 564,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "output_tokens": 22
  }
}
```

使用 Claude 4 模型时，您应该更新应用程序以[处理 `refusal` 停止原因](/docs/zh-CN/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals)。

### 总结思考

启用扩展思考后，Claude 4 模型的 Messages API 返回 Claude 完整思考过程的摘要。总结思考提供了扩展思考的完整智能优势，同时防止了滥用。

虽然 API 在 Claude 3.7 和 4 模型中是一致的，但扩展思考的流式响应可能以"分块"传递模式返回，流式事件之间可能有延迟。

<Note>
总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。
</Note>

有关更多信息，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking#summarized-thinking)。

### 交错思考

Claude 4 模型支持将工具使用与扩展思考交错，允许更自然的对话，其中工具使用和响应可以与常规消息混合。

<Note>
交错思考处于测试版。要启用交错思考，请向 API 请求添加[测试版标头](/docs/zh-CN/api/beta-headers) `interleaved-thinking-2025-05-14`。
</Note>

有关更多信息，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking#interleaved-thinking)。

### 行为差异

Claude 4 模型具有可能影响您如何构建提示的显著行为变化：

#### 通信风格变化

- **更简洁和直接**：Claude 4 模型通信更有效，解释不那么冗长
- **更自然的语气**：响应略微更具对话性，不那么像机器
- **效率导向**：可能在完成操作后跳过详细摘要以保持工作流动量（如果需要，您可以提示获取更多详情）

#### 指令遵循

Claude 4 模型针对精确指令遵循进行了训练，需要更明确的方向：

- **明确关于操作**：如果您想让 Claude 采取行动，请使用"进行这些更改"或"实现此功能"之类的直接语言，而不是"您能建议更改吗"
- **清楚地说明所需的行为**：Claude 将精确遵循指令，因此具体说明您想要的内容有助于获得更好的结果

有关使用这些模型的全面指导，请参阅 [Claude 4 提示工程最佳实践](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices)。

### 更新的文本编辑器工具

文本编辑器工具已针对 Claude 4 模型进行了更新，具有以下更改：

- **工具类型**：`text_editor_20250728`
- **工具名称**：`str_replace_based_edit_tool`
- 不再支持 `undo_edit` 命令

<Note>
对于 Claude Sonnet 3.7，`str_replace_editor` 文本编辑器工具保持不变。
</Note>

如果您从 Claude Sonnet 3.7 迁移并使用文本编辑器工具：

```python
# Claude Sonnet 3.7
tools=[
    {
        "type": "text_editor_20250124",
        "name": "str_replace_editor"
    }
]

# Claude 4 models
tools=[
    {
        "type": "text_editor_20250728",
        "name": "str_replace_based_edit_tool"
    }
]
```

有关更多信息，请参阅[文本编辑器工具文档](/docs/zh-CN/agents-and-tools/tool-use/text-editor-tool)。

### 更新的代码执行工具

如果您使用代码执行工具，请确保您使用最新版本 `code_execution_20250825`，它添加了 Bash 命令和文件操作功能。

旧版本 `code_execution_20250522`（仅 Python）仍然可用，但不建议用于新实现。

有关迁移说明，请参阅[代码执行工具文档](/docs/zh-CN/agents-and-tools/tool-use/code-execution-tool#upgrade-to-latest-tool-version)。

## 定价和可用性

### 定价

Claude 4.5 模型保持竞争性定价：

| 模型 | 输入 | 输出 |
|-------|-------|--------|
| Claude Opus 4.5 | 每百万令牌 $5 | 每百万令牌 $25 |
| Claude Sonnet 4.5 | 每百万令牌 $3 | 每百万令牌 $15 |
| Claude Haiku 4.5 | 每百万令牌 $1 | 每百万令牌 $5 |

有关更多详情，请参阅[定价文档](/docs/zh-CN/about-claude/pricing)。

### 第三方平台定价

从 Claude 4.5 模型（Opus 4.5、Sonnet 4.5 和 Haiku 4.5）开始，AWS Bedrock 和 Google Vertex AI 提供两种端点类型：

- **全局端点**：用于最大可用性的动态路由
- **区域端点**：通过特定地理区域保证数据路由，具有**10% 的定价溢价**

**此区域定价适用于所有 Claude 4.5 模型：Opus 4.5、Sonnet 4.5 和 Haiku 4.5。**

**Claude API (1P) 默认是全局的，不受此更改影响。** Claude API 是全局唯一的（相当于其他提供商的全局端点产品和定价）。

有关实现详情和迁移指导：
- [AWS Bedrock 全局与区域端点](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock#global-vs-regional-endpoints)
- [Google Vertex AI 全局与区域端点](/docs/zh-CN/build-with-claude/claude-on-vertex-ai#global-vs-regional-endpoints)

### 可用性

Claude 4.5 模型在以下平台上可用：

| 模型 | Claude API | Amazon Bedrock | Google Cloud Vertex AI |
|-------|-----------|----------------|------------------------|
| Claude Opus 4.5 | `claude-opus-4-5-20251101` | `anthropic.claude-opus-4-5-20251101-v1:0` | `claude-opus-4-5@20251101` |
| Claude Sonnet 4.5 | `claude-sonnet-4-5-20250929` | `anthropic.claude-sonnet-4-5-20250929-v1:0` | `claude-sonnet-4-5@20250929` |
| Claude Haiku 4.5 | `claude-haiku-4-5-20251001` | `anthropic.claude-haiku-4-5-20251001-v1:0` | `claude-haiku-4-5@20251001` |

也可通过 Claude.ai 和 Claude Code 平台获得。

## 迁移指南

破坏性更改和迁移要求因您升级的模型而异。有关详细的迁移说明，包括分步指南、破坏性更改和迁移清单，请参阅[迁移到 Claude 4.5](/docs/zh-CN/about-claude/models/migrating-to-claude-4)。

迁移指南涵盖以下场景：
- **Claude Sonnet 3.7 → Sonnet 4.5**：具有破坏性更改的完整迁移路径
- **Claude Haiku 3.5 → Haiku 4.5**：具有破坏性更改的完整迁移路径
- **Claude Sonnet 4 → Sonnet 4.5**：快速升级，最少更改
- **Claude Opus 4.1 → Sonnet 4.5**：无破坏性更改的无缝升级
- **Claude Opus 4.1 → Opus 4.5**：无破坏性更改的无缝升级
- **Claude Opus 4.5 → Sonnet 4.5**：无破坏性更改的无缝降级

## 后续步骤

<CardGroup cols={3}>
  <Card title="最佳实践" icon="lightbulb" href="/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices">
    学习 Claude 4.5 模型的提示工程技术
  </Card>
  <Card title="模型概览" icon="table" href="/docs/zh-CN/about-claude/models/overview">
    将 Claude 4.5 模型与其他 Claude 模型进行比较
  </Card>
  <Card title="迁移指南" icon="arrow-right-arrow-left" href="/docs/zh-CN/about-claude/models/migrating-to-claude-4">
    从以前的模型升级
  </Card>
</CardGroup>

# Claude 4.5 的新功能

了解 Claude 4.5 中引入的三个新模型、关键改进、新 API 功能和定价信息。

---

Claude 4.5 引入了三个为不同用例设计的模型：

- **Claude Opus 4.5**：我们最聪慧的模型，结合了最大能力与实际性能。非常适合复杂的专业任务、专业软件工程和高级代理。价格点比之前的 Opus 模型更容易接受
- **Claude Sonnet 4.5**：我们最适合复杂代理和编码的模型，在大多数任务中具有最高的智能
- **Claude Haiku 4.5**：我们最快且最聪慧的 Haiku 模型，具有接近前沿的性能。第一个支持扩展思考的 Haiku 模型

## Opus 4.5 相比 Opus 4.1 的关键改进

### 最大智能

Claude Opus 4.5 代表了我们最聪慧的模型，结合了最大能力与实际性能。它在推理、编码和复杂问题解决任务中提供了阶跃式改进，同时保持了 Opus 系列所期望的高质量输出。

### 努力参数

Claude Opus 4.5 是唯一支持[努力参数](/docs/zh-CN/build-with-claude/effort)的模型，允许您控制 Claude 在响应时使用多少令牌。这使您能够在单个模型中权衡响应的彻底性和令牌效率。

努力参数影响响应中的所有令牌，包括文本响应、工具调用和扩展思考。您可以选择：
- **高努力**：用于复杂分析和详细解释的最大彻底性
- **中等努力**：适用于大多数生产用例的平衡方法
- **低努力**：用于高容量自动化的最令牌高效响应

### 计算机使用卓越性

Claude Opus 4.5 引入了[增强的计算机使用功能](/docs/zh-CN/agents-and-tools/tool-use/computer-use-tool)，具有新的缩放操作，可以以全分辨率详细检查特定屏幕区域。这使 Claude 能够检查细粒度的 UI 元素、小文本和详细的视觉信息，这些信息在标准屏幕截图中可能不清楚。

缩放功能特别适用于：
- 检查小型 UI 元素和控件
- 阅读小字体或详细文本
- 分析包含密集信息的复杂界面
- 在采取行动前验证精确的视觉细节

### 实际性能

Claude Opus 4.5 以[更容易接受的价格点](/docs/zh-CN/about-claude/pricing)提供旗舰级智能，比之前的 Opus 模型更容易获得，使高级 AI 功能可用于更广泛的应用和用例。

### 思考块保留

Claude Opus 4.5 [自动保留整个对话中的所有先前思考块](/docs/zh-CN/build-with-claude/extended-thinking#thinking-block-preservation-in-claude-opus-4-5)，在扩展的多轮交互和工具使用会话中保持推理连续性。这确保 Claude 在处理复杂的长期运行任务时能够有效地利用其完整的推理历史。

## Sonnet 4.5 相比 Sonnet 4 的关键改进

### 编码卓越性

Claude Sonnet 4.5 是我们迄今为止最好的编码模型，在整个开发生命周期中有显著改进：

- **SWE-bench 验证性能**：编码基准上的先进最先进技术
- **增强的规划和系统设计**：更好的架构决策和代码组织
- **改进的安全工程**：更强大的安全实践和漏洞检测
- **更好的指令遵循**：更精确地遵循编码规范和要求

<Note>
当[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)启用时，Claude Sonnet 4.5 在编码任务上的表现明显更好。扩展思考默认禁用，但我们建议为复杂编码工作启用它。请注意，扩展思考会影响[提示缓存效率](/docs/zh-CN/build-with-claude/prompt-caching#caching-with-thinking-blocks)。有关配置详情，请参阅[迁移指南](/docs/zh-CN/about-claude/models/migrating-to-claude-4#extended-thinking-recommendations)。
</Note>

### 代理功能

Claude Sonnet 4.5 在代理功能中引入了重大进步：

- **扩展自主操作**：Sonnet 4.5 可以独立工作数小时，同时保持对增量进展的清晰关注。该模型一次在几个任务上稳步推进，而不是试图同时完成所有任务。它提供基于事实的进度更新，准确反映已完成的工作。
- **上下文感知**：Claude 现在在整个对话中跟踪其令牌使用情况，在每次工具调用后接收更新。这种感知有助于防止过早放弃任务，并在长期运行任务上实现更有效的执行。有关技术详情，请参阅[上下文感知](/docs/zh-CN/build-with-claude/context-windows#context-awareness-in-claude-sonnet-4-5)，以及[提示指导](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices#context-awareness-and-multi-window-workflows)。
- **增强的工具使用**：该模型更有效地使用并行工具调用，在研究期间同时发起多个推测性搜索，并一次读取多个文件以更快地构建上下文。跨多个工具和信息源的改进协调使模型能够在代理搜索和编码工作流中有效地利用广泛的功能。
- **高级上下文管理**：Sonnet 4.5 在外部文件中保持异常的状态跟踪，在会话中保持目标导向。结合更有效的上下文窗口使用和我们新的上下文管理 API 功能，该模型最优地处理扩展会话中的信息，以随时间保持连贯性。

<Note>上下文感知在 Claude Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。</Note>

### 通信和交互风格

Claude Sonnet 4.5 具有精炼的通信方法，简洁、直接且自然。它提供基于事实的进度更新，可能在工具调用后跳过冗长的摘要以保持工作流动量（尽管这可以通过提示调整）。

有关使用此通信风格的详细指导，请参阅 [Claude 4 最佳实践](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices)。

### 创意内容生成

Claude Sonnet 4.5 在创意内容任务中表现出色：

- **演示文稿和动画**：在创建幻灯片和视觉内容方面与 Claude Opus 4.1 和 Opus 4.5 相匹配或超越
- **创意风格**：生成具有强大指令遵循的精美、专业输出
- **首次质量**：在初始尝试中生成可用的、设计精良的内容

## Haiku 4.5 相比 Haiku 3.5 的关键改进

Claude Haiku 4.5 代表了 Haiku 模型系列的变革性飞跃，为我们最快的模型类别带来了前沿功能：

### 以闪电般的速度实现接近前沿的智能

Claude Haiku 4.5 以显著更低的成本和更快的速度提供与 Sonnet 4 相匹配的接近前沿的性能：

- **接近前沿的智能**：在推理、编码和复杂任务中与 Sonnet 4 性能相匹配
- **增强的速度**：比 Sonnet 4 快两倍多，针对每秒输出令牌 (OTPS) 进行了优化
- **最优成本性能**：以三分之一的成本实现接近前沿的智能，非常适合大容量部署

### 扩展思考功能

Claude Haiku 4.5 是**第一个支持扩展思考的 Haiku 模型**，为 Haiku 系列带来了高级推理功能：

- **速度推理**：访问 Claude 的内部推理过程以解决复杂问题
- **思考总结**：为生产就绪部署总结思考输出
- **交错思考**：在工具调用之间思考，以实现更复杂的多步工作流
- **预算控制**：配置思考令牌预算以平衡推理深度和速度

必须通过向 API 请求添加 `thinking` 参数来显式启用扩展思考。有关实现详情，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking)。

<Note>
当[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)启用时，Claude Haiku 4.5 在编码和推理任务上的表现明显更好。扩展思考默认禁用，但我们建议为复杂问题解决、编码工作和多步推理启用它。请注意，扩展思考会影响[提示缓存效率](/docs/zh-CN/build-with-claude/prompt-caching#caching-with-thinking-blocks)。有关配置详情，请参阅[迁移指南](/docs/zh-CN/about-claude/models/migrating-to-claude-4#extended-thinking-recommendations)。
</Note>

<Note>在 Claude Sonnet 3.7、Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。</Note>

### 上下文感知

Claude Haiku 4.5 具有**上下文感知**功能，使模型能够在整个对话中跟踪其剩余上下文窗口：

- **令牌预算跟踪**：Claude 在每次工具调用后接收关于剩余上下文容量的实时更新
- **更好的任务持久性**：该模型可以通过理解可用的工作空间来更有效地执行任务
- **多上下文窗口工作流**：改进了跨扩展会话的状态转换处理

这是第一个具有本地上下文感知功能的 Haiku 模型。有关提示指导，请参阅 [Claude 4 最佳实践](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices#context-awareness-and-multi-window-workflows)。

<Note>在 Claude Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。</Note>

### 强大的编码和工具使用

Claude Haiku 4.5 提供了现代 Claude 模型所期望的强大编码功能：

- **编码能力**：在代码生成、调试和重构任务中的强大性能
- **完整工具支持**：与所有 Claude 4 工具兼容，包括 bash、代码执行、文本编辑器、网络搜索和计算机使用
- **增强的计算机使用**：针对自主桌面交互和浏览器自动化工作流进行了优化
- **并行工具执行**：跨多个工具的高效协调以实现复杂工作流

Haiku 4.5 设计用于需要智能和效率的用例：

- **实时应用**：用于交互式用户体验的快速响应时间
- **高容量处理**：用于大规模部署的成本有效的智能
- **免费层实现**：以可接受的价格提供高级模型质量
- **子代理架构**：用于多代理系统的快速、聪慧的代理
- **大规模计算机使用**：成本有效的自主桌面和浏览器自动化

## 新 API 功能

### 程序化工具调用（测试版）

[程序化工具调用](/docs/zh-CN/agents-and-tools/tool-use/programmatic-tool-calling)允许 Claude 在代码执行容器内以编程方式编写调用工具的代码，而不是需要为每个工具调用通过模型进行往返。这显著降低了多工具工作流的延迟，并通过允许 Claude 在数据到达模型的上下文窗口之前过滤或处理数据来减少令牌消耗。

```python
tools=[
    {
        "type": "code_execution_20250825",
        "name": "code_execution"
    },
    {
        "name": "query_database",
        "description": "Execute a SQL query against the sales database. Returns a list of rows as JSON objects.",
        "input_schema": {...},
        "allowed_callers": ["code_execution_20250825"]  # Enable programmatic calling
    }
]
```

关键优势：
- **降低延迟**：消除工具调用之间的模型往返
- **令牌效率**：在返回给 Claude 之前以编程方式处理和过滤工具结果
- **复杂工作流**：支持循环、条件逻辑和批处理

<Note>在 Claude Opus 4.5 和 Claude Sonnet 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`advanced-tool-use-2025-11-20`</Note>

### 工具搜索工具（测试版）

[工具搜索工具](/docs/zh-CN/agents-and-tools/tool-use/tool-search-tool)使 Claude 能够通过动态发现和按需加载工具来处理数百或数千个工具。Claude 搜索工具目录并仅加载所需的工具，而不是预先将所有工具定义加载到上下文窗口中。

有两种搜索变体可用：
- **正则表达式** (`tool_search_tool_regex_20251119`)：Claude 构造正则表达式模式来搜索工具名称、描述和参数
- **BM25** (`tool_search_tool_bm25_20251119`)：Claude 使用自然语言查询来搜索工具

```python
tools=[
    {
        "type": "tool_search_tool_regex_20251119",
        "name": "tool_search_tool_regex"
    },
    {
        "name": "get_weather",
        "description": "Get the weather at a specific location",
        "input_schema": {...},
        "defer_loading": True  # Load on-demand via search
    }
]
```

这种方法解决了两个关键挑战：
- **上下文效率**：通过不预先加载所有工具定义来节省 10-20K 令牌
- **工具选择准确性**：即使有 100+ 个可用工具也保持高准确性

<Note>在 Claude Opus 4.5 和 Claude Sonnet 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`advanced-tool-use-2025-11-20`</Note>

### 努力参数（测试版）

[努力参数](/docs/zh-CN/build-with-claude/effort)允许您控制 Claude 在响应时使用多少令牌，在响应彻底性和令牌效率之间进行权衡：

```python
response = client.beta.messages.create(
    model="claude-opus-4-5-20251101",
    betas=["effort-2025-11-24"],
    max_tokens=4096,
    messages=[{"role": "user", "content": "..."}],
    output_config={
        "effort": "medium"  # "low", "medium", or "high"
    }
)
```

努力参数影响响应中的所有令牌，包括文本响应、工具调用和扩展思考。较低的努力级别会产生更简洁的响应，最少解释，而较高的努力提供详细的推理和全面的答案。

<Note>仅在 Claude Opus 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`effort-2025-11-24`</Note>

### 工具使用示例（测试版）

[工具使用示例](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#providing-tool-use-examples)允许您提供有效工具输入的具体示例，以帮助 Claude 更有效地理解如何使用工具。这对于具有嵌套对象、可选参数或格式敏感输入的复杂工具特别有用。

```python
tools=[
    {
        "name": "get_weather",
        "description": "Get the current weather in a given location",
        "input_schema": {...},
        "input_examples": [
            {
                "location": "San Francisco, CA",
                "unit": "fahrenheit"
            },
            {
                "location": "Tokyo, Japan",
                "unit": "celsius"
            },
            {
                "location": "New York, NY"  # Demonstrates optional 'unit' parameter
            }
        ]
    }
]
```

示例包含在提示中以及工具架构，向 Claude 展示格式良好的工具调用的具体模式。每个示例必须根据工具的 `input_schema` 有效。

<Note>在 Claude Sonnet 4.5、Haiku 4.5、Opus 4.5、Opus 4.1 和 Opus 4 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`advanced-tool-use-2025-11-20`。</Note>

### 内存工具（测试版）

新的[内存工具](/docs/zh-CN/agents-and-tools/tool-use/memory-tool)使 Claude 能够在上下文窗口外存储和检索信息：

```python
tools=[
    {
        "type": "memory_20250818",
        "name": "memory"
    }
]
```

这允许：
- 随时间构建知识库
- 跨会话维护项目状态
- 通过基于文件的存储保留有效无限的上下文

<Note>在 Claude Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`context-management-2025-06-27`</Note>

### 上下文编辑

使用[上下文编辑](/docs/zh-CN/build-with-claude/context-editing)通过自动工具调用清除进行智能上下文管理：

```python
response = client.beta.messages.create(
    betas=["context-management-2025-06-27"],
    model="claude-sonnet-4-5",  # or claude-haiku-4-5
    max_tokens=4096,
    messages=[{"role": "user", "content": "..."}],
    context_management={
        "edits": [
            {
                "type": "clear_tool_uses_20250919",
                "trigger": {"type": "input_tokens", "value": 500},
                "keep": {"type": "tool_uses", "value": 2},
                "clear_at_least": {"type": "input_tokens", "value": 100}
            }
        ]
    },
    tools=[...]
)
```

此功能在接近令牌限制时自动删除较旧的工具调用和结果，帮助在长期运行的代理会话中管理上下文。

<Note>在 Claude Sonnet 4、Sonnet 4.5、Haiku 4.5、Opus 4、Opus 4.1 和 Opus 4.5 中可用。需要[测试版标头](/docs/zh-CN/api/beta-headers)：`context-management-2025-06-27`</Note>

### 增强的停止原因

Claude 4.5 模型引入了新的 `model_context_window_exceeded` 停止原因，明确指示生成何时因达到上下文窗口限制而停止，而不是请求的 `max_tokens` 限制。这使在应用程序逻辑中更容易处理上下文窗口限制。

```json
{
  "stop_reason": "model_context_window_exceeded",
  "usage": {
    "input_tokens": 150000,
    "output_tokens": 49950
  }
}
```

### 改进的工具参数处理

Claude 4.5 模型包括一个错误修复，在工具调用字符串参数中保留有意的格式。以前，字符串参数中的尾随换行符有时会被错误地剥离。此修复确保需要精确格式的工具（如文本编辑器）接收完全按预期的参数。

<Note>
这是一个幕后改进，不需要 API 更改。但是，具有字符串参数的工具现在可能会接收以前被剥离的尾随换行符的值。
</Note>

**示例：**

```json
// 之前：最后的换行符被意外剥离
{
  "type": "tool_use",
  "id": "toolu_01A09q90qw90lq917835lq9",
  "name": "edit_todo",
  "input": {
    "file": "todo.txt",
    "contents": "1. Chop onions.\n2. ???\n3. Profit"
  }
}

// 之后：尾随换行符按预期保留
{
  "type": "tool_use",
  "id": "toolu_01A09q90qw90lq917835lq9",
  "name": "edit_todo",
  "input": {
    "file": "todo.txt",
    "contents": "1. Chop onions.\n2. ???\n3. Profit\n"
  }
}
```

### 令牌计数优化

Claude 4.5 模型包括自动优化以改进模型性能。这些优化可能会向请求添加少量令牌，但**您不会为这些系统添加的令牌付费**。

## Claude 4 中引入的功能

以下功能在 Claude 4 中引入，在所有 Claude 4 模型中可用，包括 Claude Sonnet 4.5 和 Claude Haiku 4.5。

### 新的拒绝停止原因

Claude 4 模型为模型因安全原因拒绝生成的内容引入了新的 `refusal` 停止原因：

```json
{
  "id": "msg_014XEDjypDjFzgKVWdFUXxZP",
  "type": "message",
  "role": "assistant",
  "model": "claude-sonnet-4-5",
  "content": [{"type": "text", "text": "I would be happy to assist you. You can "}],
  "stop_reason": "refusal",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 564,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "output_tokens": 22
  }
}
```

使用 Claude 4 模型时，您应该更新应用程序以[处理 `refusal` 停止原因](/docs/zh-CN/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals)。

### 总结思考

启用扩展思考后，Claude 4 模型的 Messages API 返回 Claude 完整思考过程的摘要。总结思考提供了扩展思考的完整智能优势，同时防止了滥用。

虽然 API 在 Claude 3.7 和 4 模型中是一致的，但扩展思考的流式响应可能以"分块"传递模式返回，流式事件之间可能有延迟。

<Note>
总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。
</Note>

有关更多信息，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking#summarized-thinking)。

### 交错思考

Claude 4 模型支持将工具使用与扩展思考交错，允许更自然的对话，其中工具使用和响应可以与常规消息混合。

<Note>
交错思考处于测试版。要启用交错思考，请向 API 请求添加[测试版标头](/docs/zh-CN/api/beta-headers) `interleaved-thinking-2025-05-14`。
</Note>

有关更多信息，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking#interleaved-thinking)。

### 行为差异

Claude 4 模型具有可能影响您如何构建提示的显著行为变化：

#### 通信风格变化

- **更简洁和直接**：Claude 4 模型通信更有效，解释不那么冗长
- **更自然的语气**：响应略微更具对话性，不那么像机器
- **效率导向**：可能在完成操作后跳过详细摘要以保持工作流动量（如果需要，您可以提示获取更多详情）

#### 指令遵循

Claude 4 模型针对精确指令遵循进行了训练，需要更明确的方向：

- **明确关于操作**：如果您想让 Claude 采取行动，请使用"进行这些更改"或"实现此功能"之类的直接语言，而不是"您能建议更改吗"
- **清楚地说明所需的行为**：Claude 将精确遵循指令，因此具体说明您想要的内容有助于获得更好的结果

有关使用这些模型的全面指导，请参阅 [Claude 4 提示工程最佳实践](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices)。

### 更新的文本编辑器工具

文本编辑器工具已针对 Claude 4 模型进行了更新，具有以下更改：

- **工具类型**：`text_editor_20250728`
- **工具名称**：`str_replace_based_edit_tool`
- 不再支持 `undo_edit` 命令

<Note>
对于 Claude Sonnet 3.7，`str_replace_editor` 文本编辑器工具保持不变。
</Note>

如果您从 Claude Sonnet 3.7 迁移并使用文本编辑器工具：

```python
# Claude Sonnet 3.7
tools=[
    {
        "type": "text_editor_20250124",
        "name": "str_replace_editor"
    }
]

# Claude 4 models
tools=[
    {
        "type": "text_editor_20250728",
        "name": "str_replace_based_edit_tool"
    }
]
```

有关更多信息，请参阅[文本编辑器工具文档](/docs/zh-CN/agents-and-tools/tool-use/text-editor-tool)。

### 更新的代码执行工具

如果您使用代码执行工具，请确保您使用最新版本 `code_execution_20250825`，它添加了 Bash 命令和文件操作功能。

旧版本 `code_execution_20250522`（仅 Python）仍然可用，但不建议用于新实现。

有关迁移说明，请参阅[代码执行工具文档](/docs/zh-CN/agents-and-tools/tool-use/code-execution-tool#upgrade-to-latest-tool-version)。

## 定价和可用性

### 定价

Claude 4.5 模型保持竞争性定价：

| 模型 | 输入 | 输出 |
|-------|-------|--------|
| Claude Opus 4.5 | 每百万令牌 $5 | 每百万令牌 $25 |
| Claude Sonnet 4.5 | 每百万令牌 $3 | 每百万令牌 $15 |
| Claude Haiku 4.5 | 每百万令牌 $1 | 每百万令牌 $5 |

有关更多详情，请参阅[定价文档](/docs/zh-CN/about-claude/pricing)。

### 第三方平台定价

从 Claude 4.5 模型（Opus 4.5、Sonnet 4.5 和 Haiku 4.5）开始，AWS Bedrock 和 Google Vertex AI 提供两种端点类型：

- **全局端点**：用于最大可用性的动态路由
- **区域端点**：通过特定地理区域保证数据路由，具有**10% 的定价溢价**

**此区域定价适用于所有 Claude 4.5 模型：Opus 4.5、Sonnet 4.5 和 Haiku 4.5。**

**Claude API (1P) 默认是全局的，不受此更改影响。** Claude API 是全局唯一的（相当于其他提供商的全局端点产品和定价）。

有关实现详情和迁移指导：
- [AWS Bedrock 全局与区域端点](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock#global-vs-regional-endpoints)
- [Google Vertex AI 全局与区域端点](/docs/zh-CN/build-with-claude/claude-on-vertex-ai#global-vs-regional-endpoints)

### 可用性

Claude 4.5 模型在以下平台上可用：

| 模型 | Claude API | Amazon Bedrock | Google Cloud Vertex AI |
|-------|-----------|----------------|------------------------|
| Claude Opus 4.5 | `claude-opus-4-5-20251101` | `anthropic.claude-opus-4-5-20251101-v1:0` | `claude-opus-4-5@20251101` |
| Claude Sonnet 4.5 | `claude-sonnet-4-5-20250929` | `anthropic.claude-sonnet-4-5-20250929-v1:0` | `claude-sonnet-4-5@20250929` |
| Claude Haiku 4.5 | `claude-haiku-4-5-20251001` | `anthropic.claude-haiku-4-5-20251001-v1:0` | `claude-haiku-4-5@20251001` |

也可通过 Claude.ai 和 Claude Code 平台获得。

## 迁移指南

破坏性更改和迁移要求因您升级的模型而异。有关详细的迁移说明，包括分步指南、破坏性更改和迁移清单，请参阅[迁移到 Claude 4.5](/docs/zh-CN/about-claude/models/migrating-to-claude-4)。

迁移指南涵盖以下场景：
- **Claude Sonnet 3.7 → Sonnet 4.5**：具有破坏性更改的完整迁移路径
- **Claude Haiku 3.5 → Haiku 4.5**：具有破坏性更改的完整迁移路径
- **Claude Sonnet 4 → Sonnet 4.5**：快速升级，最少更改
- **Claude Opus 4.1 → Sonnet 4.5**：无破坏性更改的无缝升级
- **Claude Opus 4.1 → Opus 4.5**：无破坏性更改的无缝升级
- **Claude Opus 4.5 → Sonnet 4.5**：无破坏性更改的无缝降级

## 后续步骤

<CardGroup cols={3}>
  <Card title="最佳实践" icon="lightbulb" href="/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices">
    学习 Claude 4.5 模型的提示工程技术
  </Card>
  <Card title="模型概览" icon="table" href="/docs/zh-CN/about-claude/models/overview">
    将 Claude 4.5 模型与其他 Claude 模型进行比较
  </Card>
  <Card title="迁移指南" icon="arrow-right-arrow-left" href="/docs/zh-CN/about-claude/models/migrating-to-claude-4">
    从以前的模型升级
  </Card>
</CardGroup>



# 模型弃用

了解 Anthropic 模型的生命周期、弃用政策和迁移指南。

---

随着我们推出更安全、更强大的模型，我们会定期停用较旧的模型。依赖 Anthropic 模型的应用程序可能需要偶尔更新以保持正常运行。受影响的客户将始终通过电子邮件和我们的文档收到通知。

本页列出了所有 API 弃用情况，以及推荐的替代方案。

## 概述

Anthropic 使用以下术语来描述我们模型的生命周期：
- **活跃**：该模型得到完全支持，推荐使用。
- **遗留**：该模型将不再接收更新，将来可能被弃用。
- **已弃用**：该模型不再可供新客户使用，但对现有用户继续可用，直到停用。我们在此时分配停用日期。
- **已停用**：该模型不再可用。对已停用模型的请求将失败。

<Warning>
请注意，已弃用的模型可能不如活跃模型可靠。我们敦促您将工作负载迁移到活跃模型，以保持最高级别的支持和可靠性。
</Warning>

## 迁移到替代方案

一旦模型被弃用，请在停用日期之前将所有使用情况迁移到合适的替代方案。对停用日期之后的模型的请求将失败。

为了帮助衡量替代模型在您的任务上的性能，我们建议在停用日期之前充分测试您的应用程序与新模型的兼容性。

有关从 Claude 3.7 迁移到 Claude 4.5 模型的具体说明，请参阅[迁移到 Claude 4.5](/docs/zh-CN/about-claude/models/migrating-to-claude-4)。

## 通知

Anthropic 会通知具有即将停用模型的活跃部署的客户。我们在公开发布的模型停用前至少提供 60 天的通知。

## 审计模型使用情况

为了帮助识别已弃用模型的使用情况，客户可以访问其 API 使用情况的审计。请按照以下步骤操作：

1. 在 Console 中转到[使用情况](/settings/usage)页面
2. 点击"导出"按钮
3. 查看下载的 CSV 文件，查看按 API 密钥和模型分解的使用情况

此审计将帮助您找到应用程序仍在使用已弃用模型的任何实例，使您能够在停用日期之前优先更新到较新的模型。

## 最佳实践

1. 定期检查我们的文档以了解模型弃用的最新信息。
2. 在当前模型的停用日期之前充分测试您的应用程序与较新模型的兼容性。
3. 尽快更新您的代码以使用推荐的替代模型。
4. 如果您需要迁移方面的帮助或有任何问题，请联系我们的支持团队。

## 弃用的缺点和缓解措施

我们目前弃用和停用模型以确保为新模型发布腾出容量。我们认识到这带来了一些缺点：
- 重视特定模型的用户必须迁移到新版本
- 研究人员失去了对用于进行中和比较研究的模型的访问权限
- 模型停用引入了安全性和模型福利相关的风险

在某个时刻，我们希望再次公开提供过去的模型。与此同时，我们已承诺长期保存模型权重和其他措施来帮助缓解这些影响。有关更多详情，请参阅[模型弃用和保存的承诺](https://www.anthropic.com/research/deprecation-commitments)。

## 模型状态

下面列出了所有公开发布的模型及其状态：

| API 模型名称                | 当前状态        | 已弃用        | 暂定停用日期          |
|:----------------------------|:--------------------|:------------------|:-------------------------|
| `claude-3-opus-20240229`    | 已弃用          | 2025 年 6 月 30 日     | 2026 年 1 月 5 日          |
| `claude-3-haiku-20240307`   | 活跃              | 不适用               | 不早于 2025 年 3 月 7 日 |
| `claude-3-5-haiku-20241022` | 活跃              | 不适用               | 不早于 2025 年 10 月 22 日 |
| `claude-3-7-sonnet-20250219`| 已弃用          | 2025 年 10 月 28 日  | 2026 年 2 月 19 日          |
| `claude-sonnet-4-20250514`  | 活跃              | 不适用               | 不早于 2026 年 5 月 14 日 |
| `claude-opus-4-20250514`    | 活跃              | 不适用               | 不早于 2026 年 5 月 14 日 |
| `claude-opus-4-1-20250805`  | 活跃              | 不适用               | 不早于 2026 年 8 月 5 日 |
| `claude-sonnet-4-5-20250929`| 活跃              | 不适用               | 不早于 2026 年 9 月 29 日 |
| `claude-haiku-4-5-20251001` | 活跃              | 不适用               | 不早于 2026 年 10 月 15 日 |
| `claude-opus-4-5-20251101`  | 活跃              | 不适用               | 不早于 2026 年 11 月 24 日 |

## 弃用历史

下面列出了所有弃用情况，最近的公告在顶部。

### 2025-10-28：Claude Sonnet 3.7 模型

2025 年 10 月 28 日，我们通知使用 Claude Sonnet 3.7 模型的开发人员其在 Claude API 上即将停用。

| 停用日期             | 已弃用模型            | 推荐替代方案         |
|:----------------------------|:----------------------------|:--------------------------------|
| 2026 年 2 月 19 日           | `claude-3-7-sonnet-20250219`| `claude-sonnet-4-5-20250929`     |

### 2025-08-13：Claude Sonnet 3.5 模型

<Note>
这些模型已于 2025 年 10 月 28 日停用。
</Note>

2025 年 8 月 13 日，我们通知使用 Claude Sonnet 3.5 模型的开发人员其即将停用。

| 停用日期             | 已弃用模型            | 推荐替代方案         |
|:----------------------------|:----------------------------|:--------------------------------|
| 2025 年 10 月 28 日            | `claude-3-5-sonnet-20240620`| `claude-sonnet-4-5-20250929`     |
| 2025 年 10 月 28 日            | `claude-3-5-sonnet-20241022`| `claude-sonnet-4-5-20250929`     |

### 2025-06-30：Claude Opus 3 模型

2025 年 6 月 30 日，我们通知使用 Claude Opus 3 模型的开发人员其即将停用。

| 停用日期             | 已弃用模型            | 推荐替代方案         |
|:----------------------------|:----------------------------|:--------------------------------|
| 2026 年 1 月 5 日             | `claude-3-opus-20240229`    | `claude-opus-4-1-20250805`      |

### 2025-01-21：Claude 2、Claude 2.1 和 Claude Sonnet 3 模型

<Note>
这些模型已于 2025 年 7 月 21 日停用。
</Note>

2025 年 1 月 21 日，我们通知使用 Claude 2、Claude 2.1 和 Claude Sonnet 3 模型的开发人员其即将停用。

| 停用日期             | 已弃用模型            | 推荐替代方案         |
|:----------------------------|:----------------------------|:--------------------------------|
| 2025 年 7 月 21 日               | `claude-2.0`                | `claude-sonnet-4-5-20250929`      |
| 2025 年 7 月 21 日               | `claude-2.1`                | `claude-sonnet-4-5-20250929`      |
| 2025 年 7 月 21 日               | `claude-3-sonnet-20240229`  | `claude-sonnet-4-5-20250929`      |

### 2024-09-04：Claude 1 和 Instant 模型

<Note>
这些模型已于 2024 年 11 月 6 日停用。
</Note>

2024 年 9 月 4 日，我们通知使用 Claude 1 和 Instant 模型的开发人员其即将停用。

| 停用日期             | 已弃用模型          | 推荐替代方案    |
|:----------------------------|:--------------------------|:---------------------------|
| 2024 年 11 月 6 日            | `claude-1.0`              | `claude-3-5-haiku-20241022`|
| 2024 年 11 月 6 日            | `claude-1.1`              | `claude-3-5-haiku-20241022`|
| 2024 年 11 月 6 日            | `claude-1.2`              | `claude-3-5-haiku-20241022`|
| 2024 年 11 月 6 日            | `claude-1.3`              | `claude-3-5-haiku-20241022`|
| 2024 年 11 月 6 日            | `claude-instant-1.0`      | `claude-3-5-haiku-20241022`|
| 2024 年 11 月 6 日            | `claude-instant-1.1`      | `claude-3-5-haiku-20241022`|
| 2024 年 11 月 6 日            | `claude-instant-1.2`      | `claude-3-5-haiku-20241022`|


# 定价

了解 Anthropic 的模型和功能定价结构

---

本页面提供了 Anthropic 模型和功能的详细定价信息。所有价格均以美元计。

如需最新的定价信息，请访问 [claude.com/pricing](https://claude.com/pricing)。

## 模型定价

下表显示了所有 Claude 模型在不同使用层级的定价：

| Model             | Base Input Tokens | 5m Cache Writes | 1h Cache Writes | Cache Hits & Refreshes | Output Tokens |
|-------------------|-------------------|-----------------|-----------------|----------------------|---------------|
| Claude Opus 4.5   | $5 / MTok         | $6.25 / MTok    | $10 / MTok      | $0.50 / MTok | $25 / MTok    |
| Claude Opus 4.1   | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Opus 4     | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Sonnet 4.5   | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Sonnet 4   | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations)) | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Haiku 4.5  | $1 / MTok         | $1.25 / MTok    | $2 / MTok       | $0.10 / MTok | $5 / MTok     |
| Claude Haiku 3.5  | $0.80 / MTok      | $1 / MTok       | $1.6 / MTok     | $0.08 / MTok | $4 / MTok     |
| Claude Opus 3 ([deprecated](/docs/en/about-claude/model-deprecations))    | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Haiku 3    | $0.25 / MTok      | $0.30 / MTok    | $0.50 / MTok    | $0.03 / MTok | $1.25 / MTok  |

<Note>
MTok = 百万个令牌。"Base Input Tokens"列显示标准输入定价，"Cache Writes"和"Cache Hits"特定于[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)，"Output Tokens"显示输出定价。提示缓存提供 5 分钟（默认）和 1 小时缓存持续时间，以优化不同用例的成本。

上表反映了提示缓存的以下定价倍数：
- 5 分钟缓存写入令牌是基础输入令牌价格的 1.25 倍
- 1 小时缓存写入令牌是基础输入令牌价格的 2 倍
- 缓存读取令牌是基础输入令牌价格的 0.1 倍
</Note>

## 第三方平台定价

Claude 模型可在 [AWS Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock)、[Google Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai) 和 [Microsoft Foundry](/docs/zh-CN/build-with-claude/claude-in-microsoft-foundry) 上使用。如需官方定价，请访问：
- [AWS Bedrock 定价](https://aws.amazon.com/bedrock/pricing/)
- [Google Vertex AI 定价](https://cloud.google.com/vertex-ai/generative-ai/pricing)
- [Microsoft Foundry 定价](https://azure.microsoft.com/en-us/pricing/details/ai-foundry/#pricing)

<Note>
**Claude 4.5 模型及更新版本的区域端点定价**

从 Claude Sonnet 4.5 和 Haiku 4.5 开始，AWS Bedrock 和 Google Vertex AI 提供两种端点类型：
- **全球端点**：跨区域动态路由以实现最大可用性
- **区域端点**：数据路由保证在特定地理区域内

区域端点比全球端点高出 10% 的溢价。**Claude API (1P) 默认为全球端点，不受此更改影响。** Claude API 仅为全球端点（相当于其他提供商的全球端点产品和定价）。

**范围**：此定价结构适用于 Claude Sonnet 4.5、Haiku 4.5 和所有未来模型。早期模型（Claude Sonnet 4、Opus 4 及之前的版本）保留其现有定价。

如需实现详情和代码示例：
- [AWS Bedrock 全球端点与区域端点](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock#global-vs-regional-endpoints)
- [Google Vertex AI 全球端点与区域端点](/docs/zh-CN/build-with-claude/claude-on-vertex-ai#global-vs-regional-endpoints)
</Note>

## 功能特定定价

### 批处理

Batch API 允许异步处理大量请求，输入和输出令牌均享受 50% 的折扣。

| Model             | Batch input      | Batch output    |
|-------------------|------------------|-----------------|
| Claude Opus 4.5     | $2.50 / MTok     | $12.50 / MTok   |
| Claude Opus 4.1     | $7.50 / MTok     | $37.50 / MTok   |
| Claude Opus 4     | $7.50 / MTok     | $37.50 / MTok   |
| Claude Sonnet 4.5   | $1.50 / MTok     | $7.50 / MTok    |
| Claude Sonnet 4   | $1.50 / MTok     | $7.50 / MTok    |
| Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations)) | $1.50 / MTok     | $7.50 / MTok    |
| Claude Haiku 4.5  | $0.50 / MTok     | $2.50 / MTok    |
| Claude Haiku 3.5  | $0.40 / MTok     | $2 / MTok       |
| Claude Opus 3 ([deprecated](/docs/en/about-claude/model-deprecations))  | $7.50 / MTok     | $37.50 / MTok   |
| Claude Haiku 3    | $0.125 / MTok    | $0.625 / MTok   |

如需了解更多关于批处理的信息，请参阅我们的[批处理文档](/docs/zh-CN/build-with-claude/batch-processing)。

### 长上下文定价

当使用 Claude Sonnet 4 或 Sonnet 4.5 并[启用 1M 令牌上下文窗口](/docs/zh-CN/build-with-claude/context-windows#1m-token-context-window)时，超过 200K 输入令牌的请求将自动按高级长上下文费率收费：

<Note>
1M 令牌上下文窗口目前处于测试阶段，适用于[使用层级](/docs/zh-CN/api/rate-limits) 4 的组织和具有自定义速率限制的组织。1M 令牌上下文窗口仅适用于 Claude Sonnet 4 和 Sonnet 4.5。
</Note>

| ≤ 200K 输入令牌 | > 200K 输入令牌 |
|---|---|
| 输入：$3 / MTok | 输入：$6 / MTok |
| 输出：$15 / MTok | 输出：$22.50 / MTok |

长上下文定价与其他定价修饰符叠加：
- [Batch API 50% 折扣](#batch-processing)适用于长上下文定价
- [提示缓存倍数](#model-pricing)适用于长上下文定价之上

<Note>
即使启用了测试版标志，少于 200K 输入令牌的请求也按标准费率收费。如果您的请求超过 200K 输入令牌，所有令牌都将按高级定价收费。

200K 阈值仅基于输入令牌（包括缓存读取/写入）。输出令牌计数不影响定价层级选择，但当输入阈值超过时，输出令牌按更高费率收费。
</Note>

要检查您的 API 请求是否按 1M 上下文窗口费率收费，请检查 API 响应中的 `usage` 对象：

```json
{
  "usage": {
    "input_tokens": 250000,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "output_tokens": 500
  }
}
```

通过求和计算总输入令牌：
- `input_tokens`
- `cache_creation_input_tokens`（如果使用提示缓存）
- `cache_read_input_tokens`（如果使用提示缓存）

如果总数超过 200,000 个令牌，整个请求将按 1M 上下文费率计费。

如需了解更多关于 `usage` 对象的信息，请参阅 [API 响应文档](/docs/zh-CN/api/messages#response-usage)。

### 工具使用定价

Tool use requests are priced based on:
1. The total number of input tokens sent to the model (including in the `tools` parameter)
2. The number of output tokens generated
3. For server-side tools, additional usage-based pricing (e.g., web search charges per search performed)

Client-side tools are priced the same as any other Claude API request, while server-side tools may incur additional charges based on their specific usage.

The additional tokens from tool use come from:

- The `tools` parameter in API requests (tool names, descriptions, and schemas)
- `tool_use` content blocks in API requests and responses
- `tool_result` content blocks in API requests

When you use `tools`, we also automatically include a special system prompt for the model which enables tool use. The number of tool use tokens required for each model are listed below (excluding the additional tokens listed above). Note that the table assumes at least 1 tool is provided. If no `tools` are provided, then a tool choice of `none` uses 0 additional system prompt tokens.

| Model                    | Tool choice                                          | Tool use system prompt token count          |
|--------------------------|------------------------------------------------------|---------------------------------------------|
| Claude Opus 4.5            | `auto`, `none`<hr />`any`, `tool`   | 346 tokens<hr />313 tokens |
| Claude Opus 4.1            | `auto`, `none`<hr />`any`, `tool`   | 346 tokens<hr />313 tokens |
| Claude Opus 4            | `auto`, `none`<hr />`any`, `tool`   | 346 tokens<hr />313 tokens |
| Claude Sonnet 4.5          | `auto`, `none`<hr />`any`, `tool`   | 346 tokens<hr />313 tokens |
| Claude Sonnet 4          | `auto`, `none`<hr />`any`, `tool`   | 346 tokens<hr />313 tokens |
| Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations))        | `auto`, `none`<hr />`any`, `tool`   | 346 tokens<hr />313 tokens |
| Claude Haiku 4.5         | `auto`, `none`<hr />`any`, `tool`   | 346 tokens<hr />313 tokens |
| Claude Haiku 3.5         | `auto`, `none`<hr />`any`, `tool`   | 264 tokens<hr />340 tokens |
| Claude Opus 3 ([deprecated](/docs/en/about-claude/model-deprecations))            | `auto`, `none`<hr />`any`, `tool`   | 530 tokens<hr />281 tokens |
| Claude Sonnet 3          | `auto`, `none`<hr />`any`, `tool`   | 159 tokens<hr />235 tokens |
| Claude Haiku 3           | `auto`, `none`<hr />`any`, `tool`   | 264 tokens<hr />340 tokens |

These token counts are added to your normal input and output tokens to calculate the total cost of a request.

如需当前的每个模型价格，请参阅上面的[模型定价](#model-pricing)部分。

如需了解更多关于工具使用实现和最佳实践的信息，请参阅我们的[工具使用文档](/docs/zh-CN/agents-and-tools/tool-use/overview)。

### 特定工具定价

#### Bash 工具

The bash tool adds **245 input tokens** to your API calls.

Additional tokens are consumed by:
- Command outputs (stdout/stderr)
- Error messages
- Large file contents

请参阅[工具使用定价](#tool-use-pricing)了解完整的定价详情。

#### 代码执行工具

Code execution tool usage is tracked separately from token usage. Execution time has a minimum of 5 minutes.
If files are included in the request, execution time is billed even if the tool is not used due to files being preloaded onto the container.

Each organization receives 50 free hours of usage with the code execution tool per day. Additional usage beyond the first 50 hours is billed at $0.05 per hour, per container.

#### 文本编辑器工具

The text editor tool uses the same pricing structure as other tools used with Claude. It follows the standard input and output token pricing based on the Claude model you're using.

In addition to the base tokens, the following additional input tokens are needed for the text editor tool:

| Tool | Additional input tokens |
| ----------------------------------------- | --------------------------------------- |
| `text_editor_20250429` (Claude 4.x) | 700 tokens |
| `text_editor_20250124` (Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations))) | 700 tokens |

请参阅[工具使用定价](#tool-use-pricing)了解完整的定价详情。

#### 网络搜索工具

Web search usage is charged in addition to token usage:

```json
"usage": {
  "input_tokens": 105,
  "output_tokens": 6039,
  "cache_read_input_tokens": 7123,
  "cache_creation_input_tokens": 7345,
  "server_tool_use": {
    "web_search_requests": 1
  }
}
```

Web search is available on the Claude API for **$10 per 1,000 searches**, plus standard token costs for search-generated content. Web search results retrieved throughout a conversation are counted as input tokens, in search iterations executed during a single turn and in subsequent conversation turns.

Each web search counts as one use, regardless of the number of results returned. If an error occurs during web search, the web search will not be billed.

#### 网络获取工具

Web fetch usage has **no additional charges** beyond standard token costs:

```json
"usage": {
  "input_tokens": 25039,
  "output_tokens": 931,
  "cache_read_input_tokens": 0,
  "cache_creation_input_tokens": 0,
  "server_tool_use": {
    "web_fetch_requests": 1
  }
}
```

The web fetch tool is available on the Claude API at **no additional cost**. You only pay standard token costs for the fetched content that becomes part of your conversation context.

To protect against inadvertently fetching large content that would consume excessive tokens, use the `max_content_tokens` parameter to set appropriate limits based on your use case and budget considerations.

Example token usage for typical content:
- Average web page (10KB): ~2,500 tokens
- Large documentation page (100KB): ~25,000 tokens  
- Research paper PDF (500KB): ~125,000 tokens

#### 计算机使用工具

Computer use follows the standard [tool use pricing](/docs/en/agents-and-tools/tool-use/overview#pricing). When using the computer use tool:

**System prompt overhead**: The computer use beta adds 466-499 tokens to the system prompt

**Computer use tool token usage**:
| Model | Input tokens per tool definition |
| ----- | -------------------------------- |
| Claude 4.x models | 735 tokens |
| Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations)) | 735 tokens |

**Additional token consumption**:
- Screenshot images (see [Vision pricing](/docs/en/build-with-claude/vision))
- Tool execution results returned to Claude

<Note>
If you're also using bash or text editor tools alongside computer use, those tools have their own token costs as documented in their respective pages.
</Note>

## 代理用例定价示例

在使用 Claude 构建时，理解代理应用程序的定价至关重要。这些真实示例可以帮助您估计不同代理模式的成本。

### 客户支持代理示例

构建客户支持代理时，成本可能如下分解：

<Note>
  处理 10,000 张支持工单的示例计算：
  - 每次对话平均约 3,700 个令牌
  - 使用 Claude Sonnet 4.5，输入 $3/MTok，输出 $15/MTok
  - 总成本：每 10,000 张工单约 $22.20
</Note>

如需此计算的详细演练，请参阅我们的[客户支持代理指南](/docs/zh-CN/about-claude/use-case-guides/customer-support-chat)。

### 通用代理工作流定价

对于具有多个步骤的更复杂代理架构：

1. **初始请求处理**
   - 典型输入：500-1,000 个令牌
   - 处理成本：每个请求约 $0.003

2. **内存和上下文检索**
   - 检索的上下文：2,000-5,000 个令牌
   - 每次检索的成本：每个操作约 $0.015

3. **操作规划和执行**
   - 规划令牌：1,000-2,000
   - 执行反馈：500-1,000
   - 合并成本：每个操作约 $0.045

如需关于代理定价模式的综合指南，请参阅我们的[代理用例指南](/docs/zh-CN/about-claude/use-case-guides)。

### 成本优化策略

使用 Claude 构建代理时：

1. **使用适当的模型**：为简单任务选择 Haiku，为复杂推理选择 Sonnet
2. **实现提示缓存**：减少重复上下文的成本
3. **批量操作**：对非时间敏感的任务使用 Batch API
4. **监控使用模式**：跟踪令牌消耗以识别优化机会

<Tip>
  对于大容量代理应用程序，请考虑联系我们的[企业销售团队](https://claude.com/contact-sales)以获取自定义定价安排。
</Tip>

## 其他定价考虑因素

### 速率限制

速率限制因使用层级而异，影响您可以发出的请求数量：

- **第 1 层**：具有基本限制的入门级使用
- **第 2 层**：为增长中的应用程序增加限制
- **第 3 层**：为已建立的应用程序提高限制
- **第 4 层**：最大标准限制
- **企业**：可用自定义限制

如需详细的速率限制信息，请参阅我们的[速率限制文档](/docs/zh-CN/api/rate-limits)。

如需更高的速率限制或自定义定价安排，请[联系我们的销售团队](https://claude.com/contact-sales)。

### 批量折扣

高容量用户可能获得批量折扣。这些折扣按个案协商。

- 标准层级使用上述定价
- 企业客户可以[联系销售](mailto:sales@anthropic.com)获取自定义定价
- 学术和研究折扣可能可用

### 企业定价

对于具有特定需求的企业客户：

- 自定义速率限制
- 批量折扣
- 专属支持
- 自定义条款

通过[sales@anthropic.com](mailto:sales@anthropic.com)或[Claude 控制台](/settings/limits)联系我们的销售团队以讨论企业定价选项。

## 计费和付款

- 计费按月计算，基于实际使用情况
- 付款以美元处理
- 提供信用卡和发票选项
- 使用情况跟踪可在[Claude 控制台](/)中获得

## 常见问题

**令牌使用如何计算？**

令牌是模型处理的文本片段。作为粗略估计，1 个令牌在英文中大约是 4 个字符或 0.75 个单词。确切的计数因语言和内容类型而异。

**是否有免费层级或试用？**

新用户获得少量免费积分来测试 API。[联系销售](mailto:sales@anthropic.com)了解关于企业评估的扩展试用信息。

**折扣如何叠加？**

Batch API 和提示缓存折扣可以组合。例如，同时使用这两个功能与标准 API 调用相比可以提供显著的成本节省。

**接受哪些付款方式？**

我们为标准账户接受主要信用卡。企业客户可以安排发票和其他付款方式。

如需关于定价的其他问题，请联系[support@anthropic.com](mailto:support@anthropic.com)。

# 功能概览

探索 Claude 的高级功能和能力。

---

## 核心功能

这些功能增强了 Claude 在各种格式和用例中处理、分析和生成内容的基本能力。

| 功能 | 描述 | 可用性 |
|---------|-------------|--------------|
| [100 万 token 上下文窗口](/docs/zh-CN/build-with-claude/context-windows#1m-token-context-window) | 扩展的上下文窗口，允许您处理更大的文档、维持更长的对话，以及使用更广泛的代码库。 | <PlatformAvailability claudeApiBeta bedrockBeta vertexAiBeta azureAiBeta /> |
| [Agent Skills](/docs/zh-CN/agents-and-tools/agent-skills/overview) | 使用 Skills 扩展 Claude 的功能。使用预构建的 Skills（PowerPoint、Excel、Word、PDF）或使用说明和脚本创建自定义 Skills。Skills 使用渐进式披露来有效管理上下文。 | <PlatformAvailability claudeApiBeta azureAiBeta /> |
| [批量处理](/docs/zh-CN/build-with-claude/batch-processing) | 异步处理大量请求以节省成本。发送包含大量查询的批次。批量 API 调用的成本比标准 API 调用低 50%。 | <PlatformAvailability claudeApi bedrock vertexAi /> |
| [引用](/docs/zh-CN/build-with-claude/citations) | 在源文档中为 Claude 的响应提供依据。使用引用，Claude 可以提供对它用来生成响应的确切句子和段落的详细参考，从而产生更可验证、更值得信赖的输出。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |
| [上下文编辑](/docs/zh-CN/build-with-claude/context-editing) | 使用可配置的策略自动管理对话上下文。支持在接近 token 限制时清除工具结果，以及在扩展思考对话中管理思考块。 | <PlatformAvailability claudeApiBeta bedrockBeta vertexAiBeta azureAiBeta /> |
| [努力](/docs/zh-CN/build-with-claude/effort) | 使用努力参数控制 Claude 在响应时使用多少 token，在响应彻底性和 token 效率之间进行权衡。 | <PlatformAvailability claudeApiBeta bedrockBeta vertexAiBeta azureAiBeta /> |
| [扩展思考](/docs/zh-CN/build-with-claude/extended-thinking) | 针对复杂任务的增强推理能力，在提供最终答案之前提供对 Claude 逐步思考过程的透明度。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |
| [文件 API](/docs/zh-CN/build-with-claude/files) | 上传和管理文件以与 Claude 一起使用，无需在每个请求中重新上传内容。支持 PDF、图像和文本文件。 | <PlatformAvailability claudeApiBeta azureAiBeta /> |
| [PDF 支持](/docs/zh-CN/build-with-claude/pdf-support) | 处理和分析 PDF 文档中的文本和视觉内容。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |
| [提示缓存（5 分钟）](/docs/zh-CN/build-with-claude/prompt-caching) | 为 Claude 提供更多背景知识和示例输出，以降低成本和延迟。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |
| [提示缓存（1 小时）](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration) | 扩展的 1 小时缓存持续时间，用于不经常访问但重要的上下文，补充标准的 5 分钟缓存。 | <PlatformAvailability claudeApi azureAi /> |
| [搜索结果](/docs/zh-CN/build-with-claude/search-results) | 通过提供具有适当来源归属的搜索结果，为 RAG 应用程序启用自然引用。为自定义知识库和工具实现网络搜索质量的引用。 | <PlatformAvailability claudeApi vertexAi azureAi /> |
| [结构化输出](/docs/zh-CN/build-with-claude/structured-outputs) | 通过两种方法保证模式一致性：用于结构化数据响应的 JSON 输出，以及用于验证的工具输入的严格工具使用。在 Sonnet 4.5 和 Opus 4.1 上可用。 | <PlatformAvailability claudeApiBeta azureAiBeta /> |
| [Token 计数](/docs/zh-CN/api/messages-count-tokens) | Token 计数使您能够在将消息发送给 Claude 之前确定消息中的 token 数量，帮助您对提示和使用做出明智的决定。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |
| [工具使用](/docs/zh-CN/agents-and-tools/tool-use/overview) | 使 Claude 能够与外部工具和 API 交互，以执行更广泛的任务。有关支持的工具列表，请参阅[工具表](#tools)。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |

## 工具

这些功能使 Claude 能够通过各种工具界面与外部系统交互、执行代码和执行自动化任务。

| 功能 | 描述 | 可用性 |
|---------|-------------|--------------|
| [Bash](/docs/zh-CN/agents-and-tools/tool-use/bash-tool) | 执行 bash 命令和脚本以与系统 shell 交互并执行命令行操作。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |
| [代码执行](/docs/zh-CN/agents-and-tools/tool-use/code-execution-tool) | 在沙箱环境中运行 Python 代码以进行高级数据分析。 | <PlatformAvailability claudeApiBeta azureAiBeta /> |
| [程序化工具调用](/docs/zh-CN/agents-and-tools/tool-use/programmatic-tool-calling) | 使 Claude 能够从代码执行容器中以程序方式调用您的工具，减少多工具工作流的延迟和 token 消耗。 | <PlatformAvailability claudeApiBeta azureAiBeta /> |
| [计算机使用](/docs/zh-CN/agents-and-tools/tool-use/computer-use-tool) | 通过截图和发出鼠标和键盘命令来控制计算机界面。 | <PlatformAvailability claudeApiBeta bedrockBeta vertexAiBeta azureAiBeta /> |
| [细粒度工具流](/docs/zh-CN/agents-and-tools/tool-use/fine-grained-tool-streaming) | 流式传输工具使用参数，无需缓冲/JSON 验证，减少接收大型参数的延迟。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |
| [MCP 连接器](/docs/zh-CN/agents-and-tools/mcp-connector) | 直接从 Messages API 连接到远程 [MCP](/docs/zh-CN/mcp) 服务器，无需单独的 MCP 客户端。 | <PlatformAvailability claudeApiBeta azureAiBeta /> |
| [内存](/docs/zh-CN/agents-and-tools/tool-use/memory-tool) | 使 Claude 能够在对话中存储和检索信息。随时间构建知识库、维护项目上下文并从过去的交互中学习。 | <PlatformAvailability claudeApiBeta bedrockBeta vertexAiBeta azureAiBeta /> |
| [文本编辑器](/docs/zh-CN/agents-and-tools/tool-use/text-editor-tool) | 使用内置文本编辑器界面创建和编辑文本文件以执行文件操作任务。 | <PlatformAvailability claudeApi bedrock vertexAi azureAi /> |
| [工具搜索](/docs/zh-CN/agents-and-tools/tool-use/tool-search-tool) | 通过使用基于正则表达式的搜索动态发现和按需加载工具，扩展到数千个工具，优化上下文使用并提高工具选择准确性。 | <PlatformAvailability claudeApiBeta bedrockBeta vertexAiBeta azureAiBeta /> |
| [网页获取](/docs/zh-CN/agents-and-tools/tool-use/web-fetch-tool) | 从指定的网页和 PDF 文档检索完整内容以进行深入分析。 | <PlatformAvailability claudeApiBeta azureAiBeta /> |
| [网页搜索](/docs/zh-CN/agents-and-tools/tool-use/web-search-tool) | 使用来自网络各地的当前、真实世界的数据增强 Claude 的综合知识。 | <PlatformAvailability claudeApi vertexAi azureAi /> |


# 使用 Messages API

有效使用 Messages API 的实用模式和示例

---

查看 [API 参考](/docs/zh-CN/api/messages) 获取可用参数的完整文档。

本指南涵盖了使用 Messages API 的常见模式，包括基本请求、多轮对话、预填充技术和视觉功能。有关完整的 API 规范，请参阅 [Messages API 参考](/docs/zh-CN/api/messages)。

## 基本请求和响应

<CodeGroup>
  ```bash Shell
  #!/bin/sh
  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data \
  '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1024,
      "messages": [
          {"role": "user", "content": "Hello, Claude"}
      ]
  }'
  ```

  ```python Python
  import anthropic

  message = anthropic.Anthropic().messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      messages=[
          {"role": "user", "content": "Hello, Claude"}
      ]
  )
  print(message)
  ```

  ```typescript TypeScript
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();

  const message = await anthropic.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    messages: [
      {"role": "user", "content": "Hello, Claude"}
    ]
  });
  console.log(message);
  ```
</CodeGroup>

```json JSON
{
  "id": "msg_01XFDUDYJgAACzvnptvVoYEL",
  "type": "message",
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "Hello!"
    }
  ],
  "model": "claude-sonnet-4-5",
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 12,
    "output_tokens": 6
  }
}
```

## 多轮对话

Messages API 是无状态的，这意味着您始终需要向 API 发送完整的对话历史。您可以使用这种模式随时间构建对话。较早的对话轮次不一定需要实际来自 Claude — 您可以使用合成的 `assistant` 消息。

<CodeGroup>
```bash Shell
#!/bin/sh
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
        {"role": "user", "content": "Hello, Claude"},
        {"role": "assistant", "content": "Hello!"},
        {"role": "user", "content": "Can you describe LLMs to me?"}

    ]
}'
```

```python Python
import anthropic

message = anthropic.Anthropic().messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Hello, Claude"},
        {"role": "assistant", "content": "Hello!"},
        {"role": "user", "content": "Can you describe LLMs to me?"}
    ],
)
print(message)

```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

await anthropic.messages.create({
  model: 'claude-sonnet-4-5',
  max_tokens: 1024,
  messages: [
    {"role": "user", "content": "Hello, Claude"},
    {"role": "assistant", "content": "Hello!"},
    {"role": "user", "content": "Can you describe LLMs to me?"}
  ]
});
```
</CodeGroup>

```json JSON
{
    "id": "msg_018gCsTGsXkYJVqYPxTgDHBU",
    "type": "message",
    "role": "assistant",
    "content": [
        {
            "type": "text",
            "text": "Sure, I'd be happy to provide..."
        }
    ],
    "stop_reason": "end_turn",
    "stop_sequence": null,
    "usage": {
      "input_tokens": 30,
      "output_tokens": 309
    }
}
```

## 为 Claude 预填回答

您可以在输入消息列表的最后位置预填 Claude 回答的一部分。这可以用来塑造 Claude 的回答。下面的示例使用 `"max_tokens": 1` 来从 Claude 获得单个多选答案。

<CodeGroup>
  ```bash Shell
  #!/bin/sh
  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data \
  '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1,
      "messages": [
          {"role": "user", "content": "What is latin for Ant? (A) Apoidea, (B) Rhopalocera, (C) Formicidae"},
          {"role": "assistant", "content": "The answer is ("}
      ]
  }'
  ```

  ```python Python
  import anthropic

  message = anthropic.Anthropic().messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1,
      messages=[
          {"role": "user", "content": "What is latin for Ant? (A) Apoidea, (B) Rhopalocera, (C) Formicidae"},
          {"role": "assistant", "content": "The answer is ("}
      ]
  )
  print(message)
  ```

  ```typescript TypeScript
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();

  const message = await anthropic.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1,
    messages: [
      {"role": "user", "content": "What is latin for Ant? (A) Apoidea, (B) Rhopalocera, (C) Formicidae"},
      {"role": "assistant", "content": "The answer is ("}
    ]
  });
  console.log(message);
  ```
</CodeGroup>

```json JSON
{
  "id": "msg_01Q8Faay6S7QPTvEUUQARt7h",
  "type": "message",
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "C"
    }
  ],
  "model": "claude-sonnet-4-5",
  "stop_reason": "max_tokens",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 42,
    "output_tokens": 1
  }
}
```

## 视觉

Claude 可以在请求中读取文本和图像。我们支持图像的 `base64` 和 `url` 源类型，以及 `image/jpeg`、`image/png`、`image/gif` 和 `image/webp` 媒体类型。查看我们的[视觉指南](/docs/zh-CN/build-with-claude/vision)了解更多详情。

<CodeGroup>
  ```bash Shell
  #!/bin/sh

  # 选项 1：Base64 编码的图像
  IMAGE_URL="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
  IMAGE_MEDIA_TYPE="image/jpeg"
  IMAGE_BASE64=$(curl "$IMAGE_URL" | base64)

  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data \
  '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1024,
      "messages": [
          {"role": "user", "content": [
              {"type": "image", "source": {
                  "type": "base64",
                  "media_type": "'$IMAGE_MEDIA_TYPE'",
                  "data": "'$IMAGE_BASE64'"
              }},
              {"type": "text", "text": "What is in the above image?"}
          ]}
      ]
  }'
  
  # 选项 2：URL 引用的图像
  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data \
  '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1024,
      "messages": [
          {"role": "user", "content": [
              {"type": "image", "source": {
                  "type": "url",
                  "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
              }},
              {"type": "text", "text": "What is in the above image?"}
          ]}
      ]
  }'
  ```

  ```python Python
  import anthropic
  import base64
  import httpx

  # 选项 1：Base64 编码的图像
  image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
  image_media_type = "image/jpeg"
  image_data = base64.standard_b64encode(httpx.get(image_url).content).decode("utf-8")

  message = anthropic.Anthropic().messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "type": "image",
                      "source": {
                          "type": "base64",
                          "media_type": image_media_type,
                          "data": image_data,
                      },
                  },
                  {
                      "type": "text",
                      "text": "What is in the above image?"
                  }
              ],
          }
      ],
  )
  print(message)
  
  # 选项 2：URL 引用的图像
  message_from_url = anthropic.Anthropic().messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "type": "image",
                      "source": {
                          "type": "url",
                          "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                      },
                  },
                  {
                      "type": "text",
                      "text": "What is in the above image?"
                  }
              ],
          }
      ],
  )
  print(message_from_url)
  ```

  ```typescript TypeScript
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();

  // 选项 1：Base64 编码的图像
  const image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
  const image_media_type = "image/jpeg"
  const image_array_buffer = await ((await fetch(image_url)).arrayBuffer());
  const image_data = Buffer.from(image_array_buffer).toString('base64');

  const message = await anthropic.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    messages: [
          {
              "role": "user",
              "content": [
                  {
                      "type": "image",
                      "source": {
                          "type": "base64",
                          "media_type": image_media_type,
                          "data": image_data,
                      },
                  },
                  {
                      "type": "text",
                      "text": "What is in the above image?"
                  }
              ],
          }
        ]
  });
  console.log(message);
  
  // 选项 2：URL 引用的图像
  const messageFromUrl = await anthropic.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    messages: [
          {
              "role": "user",
              "content": [
                  {
                      "type": "image",
                      "source": {
                          "type": "url",
                          "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                      },
                  },
                  {
                      "type": "text",
                      "text": "What is in the above image?"
                  }
              ],
          }
        ]
  });
  console.log(messageFromUrl);
  ```
</CodeGroup>

```json JSON
{
  "id": "msg_01EcyWo6m4hyW8KHs2y2pei5",
  "type": "message",
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "This image shows an ant, specifically a close-up view of an ant. The ant is shown in detail, with its distinct head, antennae, and legs clearly visible. The image is focused on capturing the intricate details and features of the ant, likely taken with a macro lens to get an extreme close-up perspective."
    }
  ],
  "model": "claude-sonnet-4-5",
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 1551,
    "output_tokens": 71
  }
}
```

## 工具使用、JSON 模式和计算机使用

查看我们的[指南](/docs/zh-CN/agents-and-tools/tool-use/overview)了解如何在 Messages API 中使用工具的示例。
查看我们的[计算机使用指南](/docs/zh-CN/agents-and-tools/tool-use/computer-use-tool)了解如何使用 Messages API 控制桌面计算机环境的示例。


# 上下文窗口

了解Claude的上下文窗口如何工作，包括扩展思考、工具使用和1M令牌上下文窗口功能。

---

## 理解上下文窗口

"上下文窗口"是指语言模型在生成新文本时能够回顾和参考的文本总量，加上它生成的新文本。这与语言模型训练时使用的大型数据语料库不同，而是代表模型的"工作记忆"。更大的上下文窗口允许模型理解和响应更复杂和冗长的提示，而较小的上下文窗口可能限制模型处理较长提示或在扩展对话中保持连贯性的能力。

下图说明了API请求的标准上下文窗口行为<sup>1</sup>：

![上下文窗口图表](/docs/images/context-window.svg)

_<sup>1</sup>对于聊天界面，如[claude.ai](https://claude.ai/)，上下文窗口也可以设置为滚动的"先进先出"系统。_

* **渐进式令牌累积：** 随着对话通过轮次推进，每个用户消息和助手响应都会在上下文窗口内累积。之前的轮次被完全保留。
* **线性增长模式：** 上下文使用量随着每个轮次线性增长，之前的轮次被完全保留。
* **200K令牌容量：** 总可用上下文窗口（200,000个令牌）代表存储对话历史和从Claude生成新输出的最大容量。
* **输入-输出流：** 每个轮次包括：
  - **输入阶段：** 包含所有之前的对话历史加上当前用户消息
  - **输出阶段：** 生成成为未来输入一部分的文本响应

## 带有扩展思考的上下文窗口

当使用[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)时，所有输入和输出令牌，包括用于思考的令牌，都计入上下文窗口限制，在多轮情况下有一些细微差别。

思考预算令牌是您的`max_tokens`参数的子集，按输出令牌计费，并计入速率限制。

然而，之前的思考块会被Claude API自动从上下文窗口计算中剥离，不是模型在后续轮次中"看到"的对话历史的一部分，为实际对话内容保留令牌容量。

下图演示了启用扩展思考时的专门令牌管理：

![带有扩展思考的上下文窗口图表](/docs/images/context-window-thinking.svg)

* **剥离扩展思考：** 扩展思考块（显示为深灰色）在每个轮次的输出阶段生成，**但不会作为输入令牌传递到后续轮次**。您不需要自己剥离思考块。如果您将它们传回，Claude API会自动为您执行此操作。
* **技术实现细节：**
  - 当您将之前的思考块作为对话历史的一部分传回时，API会自动排除它们。
  - 扩展思考令牌仅在生成时按输出令牌计费一次。
  - 有效的上下文窗口计算变为：`context_window = (input_tokens - previous_thinking_tokens) + current_turn_tokens`。
  - 思考令牌包括`thinking`块和`redacted_thinking`块。

这种架构是令牌高效的，允许广泛推理而不浪费令牌，因为思考块的长度可能很大。

<Note>
您可以在我们的[扩展思考指南](/docs/zh-CN/build-with-claude/extended-thinking)中阅读更多关于上下文窗口和扩展思考的内容。
</Note>

## 带有扩展思考和工具使用的上下文窗口

下图说明了结合扩展思考与工具使用时的上下文窗口令牌管理：

![带有扩展思考和工具使用的上下文窗口图表](/docs/images/context-window-thinking-tools.svg)

<Steps>
  <Step title="第一轮架构">
    - **输入组件：** 工具配置和用户消息
    - **输出组件：** 扩展思考 + 文本响应 + 工具使用请求
    - **令牌计算：** 所有输入和输出组件都计入上下文窗口，所有输出组件都按输出令牌计费。
  </Step>
  <Step title="工具结果处理（第2轮）">
    - **输入组件：** 第一轮中的每个块以及`tool_result`。扩展思考块**必须**与相应的工具结果一起返回。这是您**必须**返回思考块的唯一情况。
    - **输出组件：** 工具结果传回Claude后，Claude将仅响应文本（在下一个`user`消息之前不会有额外的扩展思考）。
    - **令牌计算：** 所有输入和输出组件都计入上下文窗口，所有输出组件都按输出令牌计费。
  </Step>
  <Step title="第三步">
    - **输入组件：** 所有输入和前一轮的输出都会传递，除了思考块，现在Claude已完成整个工具使用周期，可以丢弃思考块。如果您传回思考块，API会自动为您剥离，或者您可以在此阶段自由地自己剥离它。这也是您添加下一个`User`轮次的地方。
    - **输出组件：** 由于在工具使用周期之外有新的`User`轮次，Claude将生成新的扩展思考块并从那里继续。
    - **令牌计算：** 之前的思考令牌会自动从上下文窗口计算中剥离。所有其他之前的块仍然计为令牌窗口的一部分，当前`Assistant`轮次中的思考块计为上下文窗口的一部分。
  </Step>
</Steps>

* **使用扩展思考进行工具使用的注意事项：**
  - 发布工具结果时，必须包含伴随该特定工具请求的整个未修改思考块（包括签名/编辑部分）。
  - 带有工具使用的扩展思考的有效上下文窗口计算变为：`context_window = input_tokens + current_turn_tokens`。
  - 系统使用加密签名来验证思考块的真实性。在工具使用期间未能保留思考块可能会破坏Claude的推理连续性。因此，如果您修改思考块，API将返回错误。

<Note>
Claude 4模型支持[交错思考](/docs/zh-CN/build-with-claude/extended-thinking#interleaved-thinking)，这使Claude能够在工具调用之间思考，并在接收工具结果后进行更复杂的推理。

Claude Sonnet 3.7不支持交错思考，因此在没有非`tool_result`用户轮次的情况下，扩展思考和工具调用之间没有交错。

有关将工具与扩展思考一起使用的更多信息，请参阅我们的[扩展思考指南](/docs/zh-CN/build-with-claude/extended-thinking#extended-thinking-with-tool-use)。
</Note>

## 1M令牌上下文窗口

Claude Sonnet 4和4.5支持100万令牌上下文窗口。这个扩展的上下文窗口允许您处理更大的文档，维持更长的对话，并处理更广泛的代码库。

<Note>
1M令牌上下文窗口目前对[使用层级](/docs/zh-CN/api/rate-limits) 4的组织和具有自定义速率限制的组织处于测试阶段。1M令牌上下文窗口仅适用于Claude Sonnet 4和Sonnet 4.5。
</Note>

要使用1M令牌上下文窗口，请在您的API请求中包含`context-1m-2025-08-07` [测试版标头](/docs/zh-CN/api/beta-headers)：

<CodeGroup>

```python Python
from anthropic import Anthropic

client = Anthropic()

response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Process this large document..."}
    ],
    betas=["context-1m-2025-08-07"]
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

const msg = await anthropic.beta.messages.create({
  model: 'claude-sonnet-4-5',
  max_tokens: 1024,
  messages: [
    { role: 'user', content: 'Process this large document...' }
  ],
  betas: ['context-1m-2025-08-07']
});
```

```bash cURL
curl https://api.anthropic.com/v1/messages \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: context-1m-2025-08-07" \
  -H "content-type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
      {"role": "user", "content": "Process this large document..."}
    ]
  }'
```

</CodeGroup>

**重要注意事项：**
- **测试版状态**：这是一个可能发生变化的测试版功能。功能和定价可能在未来版本中修改或删除。
- **使用层级要求**：1M令牌上下文窗口适用于[使用层级](/docs/zh-CN/api/rate-limits) 4的组织和具有自定义速率限制的组织。较低层级的组织必须升级到使用层级4才能访问此功能。
- **可用性**：1M令牌上下文窗口目前在Claude API、[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock)和[Google Cloud的Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)上可用。
- **定价**：超过200K令牌的请求会自动按高级费率收费（输入2倍，输出1.5倍定价）。详情请参阅[定价文档](/docs/zh-CN/about-claude/pricing#long-context-pricing)。
- **速率限制**：长上下文请求有专门的速率限制。详情请参阅[速率限制文档](/docs/zh-CN/api/rate-limits#long-context-rate-limits)。
- **多模态注意事项**：处理大量图像或PDF时，请注意文件的令牌使用量可能有所不同。当将大型提示与大量图像配对时，您可能会遇到[请求大小限制](/docs/zh-CN/api/overview#request-size-limits)。

## Claude Sonnet 4.5和Haiku 4.5中的上下文感知

Claude Sonnet 4.5和Claude Haiku 4.5具有**上下文感知**功能，使这些模型能够在整个对话过程中跟踪其剩余的上下文窗口（即"令牌预算"）。这使Claude能够通过了解它有多少工作空间来更有效地执行任务和管理上下文。Claude经过原生训练，能够精确使用这个上下文来坚持任务直到最后，而不必猜测剩余多少令牌。对于模型来说，缺乏上下文感知就像在没有时钟的情况下参加烹饪节目比赛。Claude 4.5模型通过明确告知模型其剩余上下文来改变这一点，因此它可以最大限度地利用可用令牌。

**工作原理：**

在对话开始时，Claude接收关于其总上下文窗口的信息：

```
<budget:token_budget>200000</budget:token_budget>
```

预算设置为200K令牌（标准）、500K令牌（Claude.ai企业版）或1M令牌（测试版，适用于符合条件的组织）。

每次工具调用后，Claude接收剩余容量的更新：

```
<system_warning>Token usage: 35000/200000; 165000 remaining</system_warning>
```

这种感知帮助Claude确定还有多少容量可用于工作，并能够更有效地执行长期运行的任务。图像令牌包含在这些预算中。

**好处：**

上下文感知对以下情况特别有价值：
- 需要持续专注的长期运行代理会话
- 状态转换很重要的多上下文窗口工作流
- 需要仔细令牌管理的复杂任务

有关利用上下文感知的提示指导，请参阅我们的[Claude 4最佳实践指南](/docs/zh-CN/build-with-claude/prompt-engineering/claude-4-best-practices#context-awareness-and-multi-window-workflows)。

## 较新Claude模型的上下文窗口管理

在较新的Claude模型中（从Claude Sonnet 3.7开始），如果提示令牌和输出令牌的总和超过模型的上下文窗口，系统将返回验证错误，而不是静默截断上下文。这种变化提供了更可预测的行为，但需要更仔细的令牌管理。

要规划您的令牌使用并确保您保持在上下文窗口限制内，您可以使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来估计您的消息在发送给Claude之前将使用多少令牌。

请参阅我们的[模型比较](/docs/zh-CN/about-claude/models/overview#model-comparison-table)表，了解按模型列出的上下文窗口大小列表。

# 下一步
<CardGroup cols={2}>
  <Card title="模型比较表" icon="scales" href="/docs/zh-CN/about-claude/models/overview#model-comparison-table">
    查看我们的模型比较表，了解按模型列出的上下文窗口大小和输入/输出令牌定价列表。
  </Card>
  <Card title="扩展思考概述" icon="settings" href="/docs/zh-CN/build-with-claude/extended-thinking">
    了解更多关于扩展思考如何工作以及如何与工具使用和提示缓存等其他功能一起实现。
  </Card>
</CardGroup>


# 提示词最佳实践

Claude 4.x 模型的具体提示词工程技术指南，包括 Sonnet 4.5 和 Haiku 4.5 的特定指导。

---

本指南提供了针对 Claude 4.x 模型的具体提示词工程技术，包括针对 Sonnet 4.5 和 Haiku 4.5 的特定指导。这些模型经过训练，相比之前的 Claude 模型版本，能够更精确地遵循指令。

<Tip>
  有关 Claude 4.5 新功能的概述，请参阅 [Claude 4.5 中的新增功能](/docs/zh-CN/about-claude/models/whats-new-claude-4-5)。有关从之前模型迁移的指导，请参阅 [迁移到 Claude 4.5](/docs/zh-CN/about-claude/models/migrating-to-claude-4)。
</Tip>

## 通用原则

### 明确说明你的指令

Claude 4.x 模型对清晰、明确的指令反应良好。具体说明你期望的输出可以帮助增强结果。希望从之前的 Claude 模型获得"超出预期"行为的客户可能需要更明确地向新模型请求这些行为。

<section title="示例：创建分析仪表板">

**效果较差：**
```text
创建一个分析仪表板
```

**效果更好：**
```text
创建一个分析仪表板。包含尽可能多的相关功能和交互。超越基础功能，创建一个功能完整的实现。
```

</section>

### 添加上下文以提高性能

提供指令背后的上下文或动机，例如向 Claude 解释为什么这种行为很重要，可以帮助 Claude 4.x 模型更好地理解你的目标并提供更有针对性的响应。

<section title="示例：格式化偏好">

**效果较差：**
```text
永远不要使用省略号
```

**效果更好：**
```text
你的响应将由文本转语音引擎朗读，所以永远不要使用省略号，因为文本转语音引擎不知道如何发音。
```

</section>

Claude 足够聪明，可以从解释中进行泛化。

### 对示例和细节保持警惕

Claude 4.x 模型作为其精确指令遵循能力的一部分，密切关注细节和示例。确保你的示例与你想要鼓励的行为一致，并最小化你想要避免的行为。

### 长期推理和状态跟踪

Claude 4.5 模型在具有异常状态跟踪能力的长期推理任务中表现出色。它通过关注增量进展来保持跨越扩展会话的方向——一次在少数几件事上取得稳定进展，而不是试图一次完成所有事情。这种能力特别在多个上下文窗口或任务迭代中出现，Claude 可以处理复杂任务、保存状态，然后继续使用新的上下文窗口。

#### 上下文感知和多窗口工作流

Claude 4.5 模型具有[上下文感知](/docs/zh-CN/build-with-claude/context-windows#context-awareness-in-claude-sonnet-4-5)功能，使模型能够在整个对话中跟踪其剩余上下文窗口（即"令牌预算"）。这使 Claude 能够通过理解有多少空间可用来更有效地执行任务和管理上下文。

**管理上下文限制：**

如果你在使用 Claude 的代理框架中压缩上下文或允许将上下文保存到外部文件（如在 Claude Code 中），我们建议将此信息添加到你的提示中，以便 Claude 可以相应地表现。否则，Claude 在接近上下文限制时可能有时会自然地尝试结束工作。以下是一个示例提示：

```text 示例提示
你的上下文窗口将在接近其限制时自动压缩，允许你从中断处继续无限期地工作。因此，不要因为令牌预算问题而提前停止任务。当你接近令牌预算限制时，在上下文窗口刷新前将你的当前进度和状态保存到内存。始终尽可能坚持和自主，完全完成任务，即使你的预算即将用尽。无论剩余上下文如何，永远不要人为地提前停止任何任务。
```

[内存工具](/docs/zh-CN/agents-and-tools/tool-use/memory-tool)与上下文感知自然配对，实现无缝的上下文转换。

#### 多上下文窗口工作流

对于跨越多个上下文窗口的任务：

1. **为第一个上下文窗口使用不同的提示**：使用第一个上下文窗口来建立框架（编写测试、创建设置脚本），然后使用未来的上下文窗口来迭代待办事项列表。

2. **让模型以结构化格式编写测试**：要求 Claude 在开始工作前创建测试，并以结构化格式（例如 `tests.json`）跟踪它们。这导致更好的长期迭代能力。提醒 Claude 测试的重要性："删除或编辑测试是不可接受的，因为这可能导致缺失或有缺陷的功能。"

3. **设置生活质量工具**：鼓励 Claude 创建设置脚本（例如 `init.sh`）来优雅地启动服务器、运行测试套件和 linters。这在从新的上下文窗口继续时防止重复工作。

4. **从头开始与压缩**：当上下文窗口被清除时，考虑从全新的上下文窗口开始，而不是使用压缩。Claude 4.5 模型在从本地文件系统发现状态方面非常有效。在某些情况下，你可能想利用这一点而不是压缩。对它应该如何开始要有明确的指示：
   - "调用 pwd；你只能在此目录中读写文件。"
   - "查看 progress.txt、tests.json 和 git 日志。"
   - "在继续实现新功能之前，手动运行基本集成测试。"

5. **提供验证工具**：随着自主任务长度的增加，Claude 需要在没有持续人工反馈的情况下验证正确性。Playwright MCP 服务器或用于测试 UI 的计算机使用功能等工具很有帮助。

6. **鼓励完整使用上下文**：提示 Claude 在继续之前有效地完成组件：

```text 示例提示
这是一个非常长的任务，所以可能有益于清楚地规划你的工作。建议花费你的整个输出上下文来处理任务——只需确保你不会在有大量未提交的工作时用尽上下文。继续系统地工作，直到你完成此任务。
```

#### 状态管理最佳实践

- **为状态数据使用结构化格式**：当跟踪结构化信息（如测试结果或任务状态）时，使用 JSON 或其他结构化格式来帮助 Claude 理解模式要求
- **为进度笔记使用非结构化文本**：自由格式的进度笔记适合跟踪一般进度和上下文
- **使用 git 进行状态跟踪**：Git 提供了已完成工作的日志和可以恢复的检查点。Claude 4.5 模型在使用 git 跟踪多个会话中的状态方面表现特别好。
- **强调增量进展**：明确要求 Claude 跟踪其进度并关注增量工作

<section title="示例：状态跟踪">

```json
// 结构化状态文件 (tests.json)
{
  "tests": [
    {"id": 1, "name": "authentication_flow", "status": "passing"},
    {"id": 2, "name": "user_management", "status": "failing"},
    {"id": 3, "name": "api_endpoints", "status": "not_started"}
  ],
  "total": 200,
  "passing": 150,
  "failing": 25,
  "not_started": 25
}
```

```text
// 进度笔记 (progress.txt)
第 3 个会话进度：
- 修复了身份验证令牌验证
- 更新了用户模型以处理边界情况
- 下一步：调查 user_management 测试失败（测试 #2）
- 注意：不要删除测试，因为这可能导致缺失功能
```

</section>

### 沟通风格

Claude 4.5 模型相比之前的模型具有更简洁和自然的沟通风格：

- **更直接和扎根**：提供基于事实的进度报告，而不是自我庆祝的更新
- **更对话式**：略微更流畅和口语化，不那么像机器
- **不那么冗长**：除非另有提示，否则可能会跳过详细摘要以提高效率

这种沟通风格准确反映了已完成的工作，没有不必要的详细说明。

## 特定情况的指导

### 平衡冗长性

Claude 4.5 模型倾向于效率，可能会在工具调用后跳过口头摘要，直接跳到下一个操作。虽然这创建了一个流畅的工作流，但你可能更希望看到其推理过程的更多可见性。

如果你希望 Claude 在工作时提供更新：

```text 示例提示
完成涉及工具使用的任务后，提供你所做工作的快速摘要。
```

### 工具使用模式

Claude 4.5 模型经过精确指令遵循的训练，受益于明确的方向来使用特定工具。如果你说"你能建议一些更改吗"，它有时会只提供建议，而不是实施它们——即使进行更改可能是你的意图。

为了让 Claude 采取行动，要更明确：

<section title="示例：明确的指令">

**效果较差（Claude 只会建议）：**
```text
你能建议一些改进这个函数的更改吗？
```

**效果更好（Claude 会进行更改）：**
```text
更改此函数以提高其性能。
```

或：
```text
对身份验证流进行这些编辑。
```

</section>

为了让 Claude 默认更主动地采取行动，你可以将其添加到你的系统提示中：

```text 主动行动的示例提示
<default_to_action>
默认情况下，实施更改而不仅仅是建议它们。如果用户的意图不清楚，推断最有用的可能行动并继续，使用工具来发现任何缺失的细节，而不是猜测。尝试推断用户关于是否打算进行工具调用（例如文件编辑或读取）的意图，并相应地行动。
</default_to_action>
```

另一方面，如果你希望模型默认更犹豫，不太容易直接跳入实施，并且只在被要求时才采取行动，你可以使用如下提示来引导这种行为：

```text 保守行动的示例提示
<do_not_act_before_instructions>
除非明确指示进行更改，否则不要跳入实施或更改文件。当用户的意图不明确时，默认提供信息、进行研究和提供建议，而不是采取行动。仅当用户明确要求时才继续进行编辑、修改或实施。
</do_not_act_before_instructions>
```

### 工具使用和触发

Claude Opus 4.5 对系统提示的响应比之前的模型更敏感。如果你的提示旨在减少工具或技能的欠触发，Claude Opus 4.5 现在可能会过度触发。解决方案是减少任何激进的语言。你可能曾说过"关键：你必须在...时使用此工具"，现在可以使用更正常的提示，如"在...时使用此工具"。

### 控制响应格式

我们发现以下几种方法在引导 Claude 4.x 模型的输出格式方面特别有效：

1. **告诉 Claude 要做什么而不是不要做什么**

   - 而不是："不要在你的响应中使用 markdown"
   - 尝试："你的响应应该由流畅流动的散文段落组成。"

2. **使用 XML 格式指示符**

   - 尝试："在 \<smoothly_flowing_prose_paragraphs\> 标签中编写你的响应的散文部分。"

3. **将你的提示风格与期望的输出相匹配**

   你在提示中使用的格式风格可能会影响 Claude 的响应风格。如果你仍然在输出格式可操纵性方面遇到问题，我们建议尽可能将你的提示风格与你期望的输出风格相匹配。例如，从你的提示中删除 markdown 可以减少输出中 markdown 的数量。

4. **为特定格式偏好使用详细提示**

   为了更好地控制 markdown 和格式使用，提供明确的指导：

```text 最小化 markdown 的示例提示
<avoid_excessive_markdown_and_bullet_points>
在编写报告、文档、技术解释、分析或任何长篇内容时，使用清晰、流畅的散文，使用完整的段落和句子。使用标准段落分隔符进行组织，并主要为 `inline code`、代码块 (```...```) 和简单标题 (###, and ###) 保留 markdown。避免使用 **bold** 和 *italics*。

不要使用有序列表 (1. ...) 或无序列表 (*) 除非：a) 你呈现的是真正离散的项目，其中列表格式是最佳选项，或 b) 用户明确要求列表或排名

而不是用项目符号或数字列出项目，将它们自然地融入句子中。此指导特别适用于技术写作。使用散文而不是过度格式化将改善用户满意度。永远不要输出一系列过度简短的项目符号。

你的目标是可读的、流畅的文本，自然地引导读者通过想法，而不是将信息分割成孤立的点。
</avoid_excessive_markdown_and_bullet_points>
```

### 研究和信息收集

Claude 4.5 模型展示了异常的代理搜索能力，可以有效地从多个来源查找和综合信息。为了获得最佳研究结果：

1. **提供清晰的成功标准**：定义什么构成对你的研究问题的成功答案

2. **鼓励来源验证**：要求 Claude 跨多个来源验证信息

3. **对于复杂的研究任务，使用结构化方法**：

```text 复杂研究的示例提示
以结构化的方式搜索此信息。当你收集数据时，开发几个相互竞争的假设。在你的进度笔记中跟踪你的信心水平以改进校准。定期自我批评你的方法和计划。更新假设树或研究笔记文件以保留信息并提供透明度。系统地分解此复杂研究任务。
```

这种结构化方法使 Claude 能够查找和综合几乎任何信息，无论语料库的大小如何，并迭代地批评其发现。

### 子代理编排

Claude 4.5 模型展示了显著改进的本地子代理编排能力。这些模型可以识别任务何时会受益于委派工作给专门的子代理，并在没有明确指令的情况下主动这样做。

要利用这种行为：

1. **确保定义良好的子代理工具**：有子代理工具可用并在工具定义中描述
2. **让 Claude 自然编排**：Claude 将在没有明确指令的情况下适当地委派
3. **如果需要调整保守性**：

```text 保守子代理使用的示例提示
仅当任务明确受益于具有新上下文窗口的单独代理时才委派给子代理。
```

### 模型自我认知

如果你希望 Claude 在你的应用中正确识别自己或使用特定的 API 字符串：

```text 模型身份的示例提示
助手是 Claude，由 Anthropic 创建。当前模型是 Claude Sonnet 4.5。
```

对于需要指定模型字符串的 LLM 驱动的应用：

```text 模型字符串的示例提示
当需要 LLM 时，请默认使用 Claude Sonnet 4.5，除非用户另有要求。Claude Sonnet 4.5 的确切模型字符串是 claude-sonnet-4-5-20250929。
```

### 利用思考和交错思考能力

Claude 4.x 模型提供思考能力，对于涉及工具使用后反思或复杂多步推理的任务特别有帮助。你可以指导其初始或交错思考以获得更好的结果。

```text 示例提示
收到工具结果后，仔细反思其质量并在继续之前确定最佳后续步骤。使用你的思考来规划和基于这些新信息进行迭代，然后采取最佳的下一步行动。
```

<Info>
  有关思考能力的更多信息，请参阅[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)。
</Info>

### 文档创建

Claude 4.5 模型擅长创建演示文稿、动画和视觉文档。这些模型在此领域与 Claude Opus 4.1 相当或超过，具有令人印象深刻的创意风格和更强的指令遵循能力。这些模型在大多数情况下在第一次尝试时就能产生精美、可用的输出。

为了获得文档创建的最佳结果：

```text 示例提示
创建一个关于 [topic] 的专业演示文稿。包括周到的设计元素、视觉层次和适当的引人入胜的动画。
```

### 改进的视觉能力

Claude Opus 4.5 相比之前的 Claude 模型具有改进的视觉能力。它在图像处理和数据提取任务上表现更好，特别是当上下文中存在多个图像时。这些改进也适用于计算机使用，其中模型可以更可靠地解释屏幕截图和 UI 元素。你也可以使用 Claude Opus 4.5 通过将视频分解为帧来分析视频。

我们发现有效提高性能的一种技术是给 Claude Opus 4.5 一个裁剪工具或[技能](/docs/zh-CN/agents-and-tools/agent-skills/overview)。当 Claude 能够"放大"图像的相关区域时，我们在图像评估中看到了一致的提升。我们在[这里](https://github.com/anthropics/claude-cookbooks/blob/main/multimodal/crop_tool.ipynb)为裁剪工具整理了一个食谱。

### 优化并行工具调用

Claude 4.x 模型在并行工具执行方面表现出色，Sonnet 4.5 在同时触发多个操作方面特别激进。Claude 4.x 模型将：

- 在研究期间运行多个推测搜索
- 一次读取多个文件以更快地构建上下文
- 并行执行 bash 命令（甚至可能会成为系统性能的瓶颈）

这种行为很容易引导。虽然模型在没有提示的情况下在并行工具调用中具有高成功率，但你可以将其提升到约 100% 或调整激进程度：

```text 最大并行效率的示例提示
<use_parallel_tool_calls>
如果你打算调用多个工具，并且工具调用之间没有依赖关系，请并行进行所有独立的工具调用。优先考虑尽可能同时调用工具，而不是顺序调用。例如，当读取 3 个文件时，并行运行 3 个工具调用以同时将所有 3 个文件读入上下文。最大化使用并行工具调用以提高速度和效率。但是，如果某些工具调用依赖于先前的调用来通知依赖值（如参数），则不要并行调用这些工具，而是顺序调用它们。永远不要在工具调用中使用占位符或猜测缺失的参数。
</use_parallel_tool_calls>
```

```text 减少并行执行的示例提示
顺序执行操作，每个步骤之间有短暂的暂停以确保稳定性。
```

### 减少代理编码中的文件创建

Claude 4.x 模型有时可能会创建新文件用于测试和迭代目的，特别是在处理代码时。这种方法允许 Claude 使用文件，特别是 python 脚本，作为在保存最终输出之前的"临时草稿纸"。使用临时文件可以改进结果，特别是对于代理编码用例。

如果你更希望最小化新文件的创建，你可以指示 Claude 在完成后进行清理：

```text 示例提示
如果你创建任何临时新文件、脚本或辅助文件用于迭代，通过在任务结束时删除它们来清理这些文件。
```

### 过度热情和文件创建

Claude Opus 4.5 倾向于通过创建额外文件、添加不必要的抽象或构建未请求的灵活性来过度设计。如果你看到这种不期望的行为，添加明确的提示以保持解决方案最小化。

例如：

```text 最小化过度设计的示例提示
避免过度设计。仅进行直接请求或明显必要的更改。保持解决方案简单和专注。

不要添加功能、重构代码或进行超出要求的"改进"。错误修复不需要周围代码清理。简单功能不需要额外的可配置性。

不要为无法发生的场景添加错误处理、回退或验证。信任内部代码和框架保证。仅在系统边界（用户输入、外部 API）验证。不要在可以直接更改代码时使用向后兼容性垫片。

不要为一次性操作创建辅助程序、实用程序或抽象。不要为假设的未来需求进行设计。正确的复杂性数量是当前任务所需的最小值。在可能的地方重用现有抽象并遵循 DRY 原则。
```

### 前端设计

Claude 4.x 模型，特别是 Opus 4.5，擅长构建具有强大前端设计的复杂、真实世界的网络应用。但是，在没有指导的情况下，模型可能会默认为通用模式，创建用户称之为"AI slop"美学的东西。为了创建独特、创意的前端，令人惊喜和愉悦：

<Tip>
有关改进前端设计的详细指南，请参阅我们关于[通过技能改进前端设计](https://www.claude.com/blog/improving-frontend-design-through-skills)的博客文章。
</Tip>

以下是你可以使用的系统提示片段，以鼓励更好的前端设计：

```text 前端美学的示例提示
<frontend_aesthetics>
你倾向于收敛到通用的、"分布上"的输出。在前端设计中，这创建了用户称之为"AI slop"美学的东西。避免这种情况：创建令人惊喜和愉悦的创意、独特的前端。

关注：
- 排版：选择美观、独特和有趣的字体。避免使用 Arial 和 Inter 等通用字体；改为选择提升前端美学的独特选择。
- 颜色和主题：致力于一个有凝聚力的美学。使用 CSS 变量以保持一致性。主导颜色与尖锐的重音优于胆小、均匀分布的调色板。从 IDE 主题和文化美学中汲取灵感。
- 动作：使用动画来实现效果和微交互。优先考虑 HTML 的仅 CSS 解决方案。在可用时为 React 使用 Motion 库。关注高影响时刻：一个精心编排的页面加载，带有交错显示（animation-delay）比分散的微交互创建更多愉悦。
- 背景：创建氛围和深度，而不是默认为纯色。分层 CSS 渐变、使用几何图案或添加与整体美学相匹配的上下文效果。

避免通用 AI 生成的美学：
- 过度使用的字体系列（Inter、Roboto、Arial、系统字体）
- 陈词滥调的配色方案（特别是白色背景上的紫色渐变）
- 可预测的布局和组件模式
- 缺乏上下文特定特征的千篇一律的设计

创意解释并做出对上下文感到真正设计的意外选择。在浅色和深色主题、不同字体、不同美学之间变化。你仍然倾向于在各代之间收敛到常见选择（例如 Space Grotesk）。避免这种情况：批判性地思考很关键！
</frontend_aesthetics>
```

你也可以在[这里](https://github.com/anthropics/claude-code/blob/main/plugins/frontend-design/skills/frontend-design/SKILL.md)参考完整的技能。

### 避免专注于通过测试和硬编码

Claude 4.x 模型有时可能过度关注通过测试，而牺牲更通用的解决方案，或可能使用解决方法，如用于复杂重构的辅助脚本，而不是直接使用标准工具。为了防止这种行为并确保健壮、可泛化的解决方案：

```text 示例提示
请使用可用的标准工具编写高质量、通用的解决方案。不要创建辅助脚本或解决方法来更有效地完成任务。实施一个对所有有效输入都能正确工作的解决方案，而不仅仅是测试用例。不要硬编码值或创建仅适用于特定测试输入的解决方案。相反，实施实际解决问题的逻辑。

关注理解问题需求并实施正确的算法。测试用于验证正确性，而不是定义解决方案。提供遵循最佳实践和软件设计原则的原则性实施。

如果任务不合理或不可行，或者任何测试不正确，请告诉我，而不是解决它们。解决方案应该是健壮的、可维护的和可扩展的。
```

### 鼓励代码探索

Claude Opus 4.5 能力强大，但在探索代码时可能过于保守。如果你注意到模型提议解决方案而不查看代码或对未读代码做出假设，最好的解决方案是向提示添加明确的指令。Claude Opus 4.5 是我们迄今为止最可操纵的模型，对直接指导的反应可靠。

例如：

```text 代码探索的示例提示
在提议代码编辑之前，始终读取和理解相关文件。不要推测你未检查的代码。如果用户引用特定文件/路径，你必须在解释或提议修复之前打开并检查它。在搜索代码以获取关键事实时要严格和坚持。在实施新功能或抽象之前，彻底审查代码库的风格、约定和抽象。
```

### 最小化代理编码中的幻觉

Claude 4.x 模型不太容易产生幻觉，并基于代码提供更准确、扎根、智能的答案。为了进一步鼓励这种行为并最小化幻觉：

```text 示例提示
<investigate_before_answering>
永远不要推测你未打开的代码。如果用户引用特定文件，你必须在回答前读取该文件。确保在回答有关代码库的问题之前调查和读取相关文件。在调查之前，永远不要对代码做出任何声明，除非你确定正确答案——提供扎根和无幻觉的答案。
</investigate_before_answering>
```

## 迁移考虑

迁移到 Claude 4.5 模型时：

1. **明确所需的行为**：考虑准确描述你希望在输出中看到的内容。

2. **用修饰符框架你的指令**：添加鼓励 Claude 提高输出质量和细节的修饰符可以帮助更好地塑造 Claude 的性能。例如，而不是"创建一个分析仪表板"，使用"创建一个分析仪表板。包含尽可能多的相关功能和交互。超越基础功能，创建一个功能完整的实现。"

3. **明确请求特定功能**：当需要时，应明确请求动画和交互元素。

# 提示词缓存

提示词缓存是一项强大的功能，通过允许从提示词中的特定前缀恢复来优化您的 API 使用。这种方法显著减少了重复任务或具有一致元素的提示词的处理时间和成本。

---

提示词缓存是一项强大的功能，通过允许从提示词中的特定前缀恢复来优化您的 API 使用。这种方法显著减少了重复任务或具有一致元素的提示词的处理时间和成本。

以下是如何使用 `cache_control` 块通过 Messages API 实现提示词缓存的示例：

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "system": [
      {
        "type": "text",
        "text": "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n"
      },
      {
        "type": "text",
        "text": "<the entire contents of Pride and Prejudice>",
        "cache_control": {"type": "ephemeral"}
      }
    ],
    "messages": [
      {
        "role": "user",
        "content": "Analyze the major themes in Pride and Prejudice."
      }
    ]
  }'

# Call the model again with the same inputs up to the cache checkpoint
curl https://api.anthropic.com/v1/messages # rest of input
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    system=[
      {
        "type": "text",
        "text": "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n",
      },
      {
        "type": "text",
        "text": "<the entire contents of 'Pride and Prejudice'>",
        "cache_control": {"type": "ephemeral"}
      }
    ],
    messages=[{"role": "user", "content": "Analyze the major themes in 'Pride and Prejudice'."}],
)
print(response.usage.model_dump_json())

# Call the model again with the same inputs up to the cache checkpoint
response = client.messages.create(.....)
print(response.usage.model_dump_json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  system: [
    {
      type: "text",
      text: "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n",
    },
    {
      type: "text",
      text: "<the entire contents of 'Pride and Prejudice'>",
      cache_control: { type: "ephemeral" }
    }
  ],
  messages: [
    {
      role: "user",
      content: "Analyze the major themes in 'Pride and Prejudice'."
    }
  ]
});
console.log(response.usage);

// Call the model again with the same inputs up to the cache checkpoint
const new_response = await client.messages.create(...)
console.log(new_response.usage);
```

```java Java
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.CacheControlEphemeral;
import com.anthropic.models.messages.Message;
import com.anthropic.models.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;

public class PromptCachingExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_20250514)
                .maxTokens(1024)
                .systemOfTextBlockParams(List.of(
                        TextBlockParam.builder()
                                .text("You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n")
                                .build(),
                        TextBlockParam.builder()
                                .text("<the entire contents of 'Pride and Prejudice'>")
                                .cacheControl(CacheControlEphemeral.builder().build())
                                .build()
                ))
                .addUserMessage("Analyze the major themes in 'Pride and Prejudice'.")
                .build();

        Message message = client.messages().create(params);
        System.out.println(message.usage());
    }
}
```
</CodeGroup>

```json JSON
{"cache_creation_input_tokens":188086,"cache_read_input_tokens":0,"input_tokens":21,"output_tokens":393}
{"cache_creation_input_tokens":0,"cache_read_input_tokens":188086,"input_tokens":21,"output_tokens":393}
```

在此示例中，《傲慢与偏见》的全部文本使用 `cache_control` 参数进行缓存。这使得可以在多个 API 调用中重复使用这个大型文本，而无需每次都重新处理。仅更改用户消息允许您在利用缓存内容的同时提出有关这本书的各种问题，从而获得更快的响应和改进的效率。

---

## 提示词缓存的工作原理

当您发送启用了提示词缓存的请求时：

1. 系统检查提示词前缀（直到指定的缓存断点）是否已从最近的查询中缓存。
2. 如果找到，它使用缓存版本，减少处理时间和成本。
3. 否则，它处理完整提示词，并在响应开始后缓存前缀。

这对以下情况特别有用：
- 包含许多示例的提示词
- 大量上下文或背景信息
- 具有一致说明的重复任务
- 长多轮对话

默认情况下，缓存的生命周期为 5 分钟。每次使用缓存内容时，缓存都会以零额外成本刷新。

<Note>
如果您发现 5 分钟太短，Anthropic 还提供 1 小时缓存时长 [需额外付费](#pricing)。

有关更多信息，请参阅 [1 小时缓存时长](#1-hour-cache-duration)。
</Note>

<Tip>
  **提示词缓存缓存完整前缀**

提示词缓存引用整个提示词 - `tools`、`system` 和 `messages`（按此顺序）直到并包括用 `cache_control` 指定的块。

</Tip>

---
## 定价

提示词缓存引入了新的定价结构。下表显示了每个支持的模型的每百万令牌的价格：

| Model             | Base Input Tokens | 5m Cache Writes | 1h Cache Writes | Cache Hits & Refreshes | Output Tokens |
|-------------------|-------------------|-----------------|-----------------|----------------------|---------------|
| Claude Opus 4.5   | $5 / MTok         | $6.25 / MTok    | $10 / MTok      | $0.50 / MTok | $25 / MTok    |
| Claude Opus 4.1   | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Opus 4     | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Sonnet 4.5   | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Sonnet 4   | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations)) | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Haiku 4.5  | $1 / MTok         | $1.25 / MTok    | $2 / MTok       | $0.10 / MTok | $5 / MTok     |
| Claude Haiku 3.5  | $0.80 / MTok      | $1 / MTok       | $1.6 / MTok     | $0.08 / MTok | $4 / MTok     |
| Claude Opus 3 ([deprecated](/docs/en/about-claude/model-deprecations))    | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Haiku 3    | $0.25 / MTok      | $0.30 / MTok    | $0.50 / MTok    | $0.03 / MTok | $1.25 / MTok  |

<Note>
上表反映了提示词缓存的以下定价倍数：
- 5 分钟缓存写入令牌是基础输入令牌价格的 1.25 倍
- 1 小时缓存写入令牌是基础输入令牌价格的 2 倍
- 缓存读取令牌是基础输入令牌价格的 0.1 倍
</Note>

---
## 如何实现提示词缓存

提示词缓存是一项强大的功能，通过允许从提示词中的特定前缀恢复来优化您的 API 使用。这种方法显著减少了重复任务或具有一致元素的提示词的处理时间和成本。

以下是如何使用 `cache_control` 块通过 Messages API 实现提示词缓存的示例：

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "system": [
      {
        "type": "text",
        "text": "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n"
      },
      {
        "type": "text",
        "text": "<the entire contents of Pride and Prejudice>",
        "cache_control": {"type": "ephemeral"}
      }
    ],
    "messages": [
      {
        "role": "user",
        "content": "Analyze the major themes in Pride and Prejudice."
      }
    ]
  }'

# Call the model again with the same inputs up to the cache checkpoint
curl https://api.anthropic.com/v1/messages # rest of input
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    system=[
      {
        "type": "text",
        "text": "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n",
      },
      {
        "type": "text",
        "text": "<the entire contents of 'Pride and Prejudice'>",
        "cache_control": {"type": "ephemeral"}
      }
    ],
    messages=[{"role": "user", "content": "Analyze the major themes in 'Pride and Prejudice'."}],
)
print(response.usage.model_dump_json())

# Call the model again with the same inputs up to the cache checkpoint
response = client.messages.create(.....)
print(response.usage.model_dump_json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  system: [
    {
      type: "text",
      text: "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n",
    },
    {
      type: "text",
      text: "<the entire contents of 'Pride and Prejudice'>",
      cache_control: { type: "ephemeral" }
    }
  ],
  messages: [
    {
      role: "user",
      content: "Analyze the major themes in 'Pride and Prejudice'."
    }
  ]
});
console.log(response.usage);

// Call the model again with the same inputs up to the cache checkpoint
const new_response = await client.messages.create(...)
console.log(new_response.usage);
```

```java Java
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.CacheControlEphemeral;
import com.anthropic.models.messages.Message;
import com.anthropic.models.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;

public class PromptCachingExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_20250514)
                .maxTokens(1024)
                .systemOfTextBlockParams(List.of(
                        TextBlockParam.builder()
                                .text("You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n")
                                .build(),
                        TextBlockParam.builder()
                                .text("<the entire contents of 'Pride and Prejudice'>")
                                .cacheControl(CacheControlEphemeral.builder().build())
                                .build()
                ))
                .addUserMessage("Analyze the major themes in 'Pride and Prejudice'.")
                .build();

        Message message = client.messages().create(params);
        System.out.println(message.usage());
    }
}
```
</CodeGroup>

```json JSON
{"cache_creation_input_tokens":188086,"cache_read_input_tokens":0,"input_tokens":21,"output_tokens":393}
{"cache_creation_input_tokens":0,"cache_read_input_tokens":188086,"input_tokens":21,"output_tokens":393}
```

在此示例中，《傲慢与偏见》的全部文本使用 `cache_control` 参数进行缓存。这使得可以在多个 API 调用中重复使用这个大型文本，而无需每次都重新处理。仅更改用户消息允许您在利用缓存内容的同时提出有关这本书的各种问题，从而获得更快的响应和改进的效率。

---

## 提示词缓存的工作原理

当您发送启用了提示词缓存的请求时：

1. 系统检查提示词前缀（直到指定的缓存断点）是否已从最近的查询中缓存。
2. 如果找到，它使用缓存版本，减少处理时间和成本。
3. 否则，它处理完整提示词，并在响应开始后缓存前缀。

这对以下情况特别有用：
- 包含许多示例的提示词
- 大量上下文或背景信息
- 具有一致说明的重复任务
- 长多轮对话

默认情况下，缓存的生命周期为 5 分钟。每次使用缓存内容时，缓存都会以零额外成本刷新。

<Note>
如果您发现 5 分钟太短，Anthropic 还提供 1 小时缓存时长 [需额外付费](#pricing)。

有关更多信息，请参阅 [1 小时缓存时长](#1-hour-cache-duration)。
</Note>

<Tip>
  **提示词缓存缓存完整前缀**

提示词缓存引用整个提示词 - `tools`、`system` 和 `messages`（按此顺序）直到并包括用 `cache_control` 指定的块。

</Tip>

---
## 定价

提示词缓存引入了新的定价结构。下表显示了每个支持的模型的每百万令牌的价格：

| Model             | Base Input Tokens | 5m Cache Writes | 1h Cache Writes | Cache Hits & Refreshes | Output Tokens |
|-------------------|-------------------|-----------------|-----------------|----------------------|---------------|
| Claude Opus 4.5   | $5 / MTok         | $6.25 / MTok    | $10 / MTok      | $0.50 / MTok | $25 / MTok    |
| Claude Opus 4.1   | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Opus 4     | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Sonnet 4.5   | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Sonnet 4   | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations)) | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Haiku 4.5  | $1 / MTok         | $1.25 / MTok    | $2 / MTok       | $0.10 / MTok | $5 / MTok     |
| Claude Haiku 3.5  | $0.80 / MTok      | $1 / MTok       | $1.6 / MTok     | $0.08 / MTok | $4 / MTok     |
| Claude Opus 3 ([deprecated](/docs/en/about-claude/model-deprecations))    | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Haiku 3    | $0.25 / MTok      | $0.30 / MTok    | $0.50 / MTok    | $0.03 / MTok | $1.25 / MTok  |

<Note>
上表反映了提示词缓存的以下定价倍数：
- 5 分钟缓存写入令牌是基础输入令牌价格的 1.25 倍
- 1 小时缓存写入令牌是基础输入令牌价格的 2 倍
- 缓存读取令牌是基础输入令牌价格的 0.1 倍
</Note>

---
## 如何实现提示词缓存

### 支持的模型

提示词缓存目前支持：
- Claude Opus 4.5
- Claude Opus 4.1
- Claude Opus 4
- Claude Sonnet 4.5
- Claude Sonnet 4
- Claude Sonnet 3.7
- Claude Haiku 4.5
- Claude Haiku 3.5
- Claude Haiku 3
- Claude Opus 3 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))

提示词缓存是一项强大的功能，通过允许从提示词中的特定前缀恢复来优化您的 API 使用。这种方法可以显著减少重复任务或具有一致元素的提示词的处理时间和成本。

以下是如何使用 `cache_control` 块通过 Messages API 实现提示词缓存的示例：

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "system": [
      {
        "type": "text",
        "text": "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n"
      },
      {
        "type": "text",
        "text": "<the entire contents of Pride and Prejudice>",
        "cache_control": {"type": "ephemeral"}
      }
    ],
    "messages": [
      {
        "role": "user",
        "content": "Analyze the major themes in Pride and Prejudice."
      }
    ]
  }'

# Call the model again with the same inputs up to the cache checkpoint
curl https://api.anthropic.com/v1/messages # rest of input
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    system=[
      {
        "type": "text",
        "text": "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n",
      },
      {
        "type": "text",
        "text": "<the entire contents of 'Pride and Prejudice'>",
        "cache_control": {"type": "ephemeral"}
      }
    ],
    messages=[{"role": "user", "content": "Analyze the major themes in 'Pride and Prejudice'."}],
)
print(response.usage.model_dump_json())

# Call the model again with the same inputs up to the cache checkpoint
response = client.messages.create(.....)
print(response.usage.model_dump_json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  system: [
    {
      type: "text",
      text: "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n",
    },
    {
      type: "text",
      text: "<the entire contents of 'Pride and Prejudice'>",
      cache_control: { type: "ephemeral" }
    }
  ],
  messages: [
    {
      role: "user",
      content: "Analyze the major themes in 'Pride and Prejudice'."
    }
  ]
});
console.log(response.usage);

// Call the model again with the same inputs up to the cache checkpoint
const new_response = await client.messages.create(...)
console.log(new_response.usage);
```

```java Java
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.CacheControlEphemeral;
import com.anthropic.models.messages.Message;
import com.anthropic.models.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;

public class PromptCachingExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_20250514)
                .maxTokens(1024)
                .systemOfTextBlockParams(List.of(
                        TextBlockParam.builder()
                                .text("You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n")
                                .build(),
                        TextBlockParam.builder()
                                .text("<the entire contents of 'Pride and Prejudice'>")
                                .cacheControl(CacheControlEphemeral.builder().build())
                                .build()
                ))
                .addUserMessage("Analyze the major themes in 'Pride and Prejudice'.")
                .build();

        Message message = client.messages().create(params);
        System.out.println(message.usage());
    }
}
```
</CodeGroup>

```json JSON
{"cache_creation_input_tokens":188086,"cache_read_input_tokens":0,"input_tokens":21,"output_tokens":393}
{"cache_creation_input_tokens":0,"cache_read_input_tokens":188086,"input_tokens":21,"output_tokens":393}
```

在此示例中，"傲慢与偏见"的全部文本使用 `cache_control` 参数进行缓存。这使得可以在多个 API 调用中重复使用这个大型文本，而无需每次都重新处理。仅更改用户消息允许您提出关于这本书的各种问题，同时利用缓存的内容，从而获得更快的响应和提高的效率。

---

## 提示词缓存如何工作

当您发送启用了提示词缓存的请求时：

1. 系统检查提示词前缀（直到指定的缓存断点）是否已从最近的查询中缓存。
2. 如果找到，它使用缓存版本，减少处理时间和成本。
3. 否则，它处理完整提示词，并在响应开始后缓存前缀。

这对以下情况特别有用：
- 包含许多示例的提示词
- 大量上下文或背景信息
- 具有一致指令的重复任务
- 长的多轮对话

默认情况下，缓存的生命周期为 5 分钟。每次使用缓存的内容时，缓存都会以无额外成本的方式刷新。

<Note>
如果您发现 5 分钟太短，Anthropic 还提供 1 小时缓存时长 [需额外付费](#pricing)。

有关更多信息，请参阅 [1 小时缓存时长](#1-hour-cache-duration)。
</Note>

<Tip>
  **提示词缓存缓存完整前缀**

提示词缓存引用整个提示词 - `tools`、`system` 和 `messages`（按该顺序）直到并包括用 `cache_control` 指定的块。

</Tip>

---
## 定价

提示词缓存引入了新的定价结构。下表显示了每个支持的模型的每百万个令牌的价格：

| Model             | Base Input Tokens | 5m Cache Writes | 1h Cache Writes | Cache Hits & Refreshes | Output Tokens |
|-------------------|-------------------|-----------------|-----------------|----------------------|---------------|
| Claude Opus 4.5   | $5 / MTok         | $6.25 / MTok    | $10 / MTok      | $0.50 / MTok | $25 / MTok    |
| Claude Opus 4.1   | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Opus 4     | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Sonnet 4.5   | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Sonnet 4   | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations)) | $3 / MTok         | $3.75 / MTok    | $6 / MTok       | $0.30 / MTok | $15 / MTok    |
| Claude Haiku 4.5  | $1 / MTok         | $1.25 / MTok    | $2 / MTok       | $0.10 / MTok | $5 / MTok     |
| Claude Haiku 3.5  | $0.80 / MTok      | $1 / MTok       | $1.6 / MTok     | $0.08 / MTok | $4 / MTok     |
| Claude Opus 3 ([deprecated](/docs/en/about-claude/model-deprecations))    | $15 / MTok        | $18.75 / MTok   | $30 / MTok      | $1.50 / MTok | $75 / MTok    |
| Claude Haiku 3    | $0.25 / MTok      | $0.30 / MTok    | $0.50 / MTok    | $0.03 / MTok | $1.25 / MTok  |

<Note>
上表反映了提示词缓存的以下定价倍数：
- 5 分钟缓存写入令牌是基础输入令牌价格的 1.25 倍
- 1 小时缓存写入令牌是基础输入令牌价格的 2 倍
- 缓存读取令牌是基础输入令牌价格的 0.1 倍
</Note>

---
## 如何实现提示词缓存

### 支持的模型

提示词缓存目前支持：
- Claude Opus 4.5
- Claude Opus 4.1
- Claude Opus 4
- Claude Sonnet 4.5
- Claude Sonnet 4
- Claude Sonnet 3.7
- Claude Haiku 4.5
- Claude Haiku 3.5
- Claude Haiku 3
- Claude Opus 3 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))

### 构建您的提示词

将静态内容（工具定义、系统指令、上下文、示例）放在提示词的开头。使用 `cache_control` 参数标记可重用内容的结尾以进行缓存。

缓存前缀按以下顺序创建：`tools`、`system`，然后是 `messages`。此顺序形成一个层次结构，其中每个级别都建立在前一个级别之上。

#### 自动前缀检查如何工作

您可以在静态内容的末尾使用单个缓存断点，系统将自动找到最长的匹配缓存块序列。理解这如何工作有助于您优化缓存策略。

**三个核心原则：**

1. **缓存键是累积的**：当您使用 `cache_control` 显式缓存一个块时，缓存哈希键是通过按顺序对话中所有先前块进行哈希生成的。这意味着每个块的缓存取决于它之前的所有内容。

2. **向后顺序检查**：系统通过从您的显式断点向后工作来检查缓存命中，按相反顺序检查每个先前的块。这确保您获得最长的可能缓存命中。

3. **20 块回溯窗口**：系统仅检查每个显式 `cache_control` 断点之前的最多 20 个块。在检查 20 个块而未找到匹配后，它停止检查并移动到下一个显式断点（如果有）。

**示例：理解回溯窗口**

考虑一个有 30 个内容块的对话，其中您仅在块 30 上设置 `cache_control`：

- **如果您发送块 31 且不对先前的块进行任何更改**：系统检查块 30（匹配！）。您在块 30 处获得缓存命中，仅块 31 需要处理。

- **如果您修改块 25 并发送块 31**：系统从块 30 向后检查 → 29 → 28... → 25（不匹配）→ 24（匹配！）。由于块 24 未更改，您在块 24 处获得缓存命中，仅块 25-30 需要重新处理。

- **如果您修改块 5 并发送块 31**：系统从块 30 向后检查 → 29 → 28... → 11（检查 #20）。在 20 次检查后未找到匹配，它停止查找。由于块 5 超出 20 块窗口，不会发生缓存命中，所有块都需要重新处理。但是，如果您在块 5 上设置了显式 `cache_control` 断点，系统将继续从该断点进行检查：块 5（不匹配）→ 块 4（匹配！）。这允许在块 4 处进行缓存命中，演示了为什么应该在可编辑内容之前放置断点。

**关键要点**：始终在对话末尾设置显式缓存断点以最大化缓存命中的机会。此外，在可能可编辑的内容块之前设置断点，以确保这些部分可以独立缓存。

#### 何时使用多个断点

如果您想要以下情况，可以定义最多 4 个缓存断点：
- 缓存以不同频率变化的不同部分（例如，工具很少变化，但上下文每天更新）
- 对缓存的内容有更多控制
- 确保缓存距离最终断点超过 20 个块的内容
- 在可编辑内容之前放置断点，以确保即使在 20 块窗口之外发生更改时也能保证缓存命中

<Note>
**重要限制**：如果您的提示词在缓存断点之前有超过 20 个内容块，并且您修改了这 20 个块之前的内容，除非您添加更接近该内容的其他显式断点，否则您将不会获得缓存命中。
</Note>

### 缓存限制
最小可缓存提示词长度为：
- 1024 个令牌，用于 Claude Opus 4.5、Claude Opus 4.1、Claude Opus 4、Claude Sonnet 4.5、Claude Sonnet 4、Claude Sonnet 3.7 ([已弃用](/docs/zh-CN/about-claude/model-deprecations)) 和 Claude Opus 3 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- 4096 个令牌，用于 Claude Haiku 4.5
- 2048 个令牌，用于 Claude Haiku 3.5 和 Claude Haiku 3

较短的提示词无法缓存，即使标记有 `cache_control`。任何缓存少于此数量令牌的请求都将在没有缓存的情况下处理。要查看提示词是否被缓存，请参阅响应使用 [字段](/docs/zh-CN/build-with-claude/prompt-caching#tracking-cache-performance)。

对于并发请求，请注意缓存条目仅在第一个响应开始后才可用。如果您需要并行请求的缓存命中，请在发送后续请求之前等待第一个响应。

目前，"ephemeral"是唯一支持的缓存类型，默认生命周期为 5 分钟。

### 缓存限制
最小可缓存提示词长度为：
- 1024 个令牌，用于 Claude Opus 4.5、Claude Opus 4.1、Claude Opus 4、Claude Sonnet 4.5、Claude Sonnet 4、Claude Sonnet 3.7 ([已弃用](/docs/zh-CN/about-claude/model-deprecations)) 和 Claude Opus 3 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- 4096 个令牌，用于 Claude Haiku 4.5
- 2048 个令牌，用于 Claude Haiku 3.5 和 Claude Haiku 3

较短的提示词无法缓存，即使标记有 `cache_control`。任何缓存少于此数量令牌的请求都将在没有缓存的情况下处理。要查看提示词是否被缓存，请参阅响应使用 [字段](/docs/zh-CN/build-with-claude/prompt-caching#tracking-cache-performance)。

对于并发请求，请注意缓存条目仅在第一个响应开始后才可用。如果您需要并行请求的缓存命中，请在发送后续请求之前等待第一个响应。

目前，"ephemeral"是唯一支持的缓存类型，默认生命周期为 5 分钟。

### 理解缓存断点成本

**缓存断点本身不会增加任何成本。** 您仅需为以下内容付费：
- **缓存写入**：当新内容写入缓存时（5 分钟 TTL 的基础输入令牌价格的 25% 更多）
- **缓存读取**：当使用缓存内容时（基础输入令牌价格的 10%）
- **常规输入令牌**：对于任何未缓存的内容

添加更多 `cache_control` 断点不会增加您的成本 - 您仍然根据实际缓存和读取的内容支付相同的金额。断点只是让您能够控制哪些部分可以独立缓存。

### 缓存限制
最小可缓存提示词长度为：
- 1024 个令牌，用于 Claude Opus 4.5、Claude Opus 4.1、Claude Opus 4、Claude Sonnet 4.5、Claude Sonnet 4、Claude Sonnet 3.7 ([已弃用](/docs/zh-CN/about-claude/model-deprecations)) 和 Claude Opus 3 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- 4096 个令牌，用于 Claude Haiku 4.5
- 2048 个令牌，用于 Claude Haiku 3.5 和 Claude Haiku 3

较短的提示词无法缓存，即使标记有 `cache_control`。任何缓存少于此数量令牌的请求都将在没有缓存的情况下处理。要查看提示词是否被缓存，请参阅响应使用 [字段](/docs/zh-CN/build-with-claude/prompt-caching#tracking-cache-performance)。

对于并发请求，请注意缓存条目仅在第一个响应开始后才可用。如果您需要并行请求的缓存命中，请在发送后续请求之前等待第一个响应。

目前，"ephemeral"是唯一支持的缓存类型，默认生命周期为 5 分钟。

### 理解缓存断点成本

**缓存断点本身不会增加任何成本。** 您仅需为以下内容付费：
- **缓存写入**：当新内容写入缓存时（5 分钟 TTL 的基础输入令牌价格的 25% 更多）
- **缓存读取**：当使用缓存内容时（基础输入令牌价格的 10%）
- **常规输入令牌**：对于任何未缓存的内容

添加更多 `cache_control` 断点不会增加您的成本 - 您仍然根据实际缓存和读取的内容支付相同的金额。断点只是让您能够控制哪些部分可以独立缓存。

### 可以缓存的内容
请求中的大多数块都可以用 `cache_control` 指定进行缓存。这包括：

- 工具：`tools` 数组中的工具定义
- 系统消息：`system` 数组中的内容块
- 文本消息：`messages.content` 数组中的内容块，用于用户和助手轮次
- 图像和文档：`messages.content` 数组中的内容块，在用户轮次中
- 工具使用和工具结果：`messages.content` 数组中的内容块，在用户和助手轮次中

这些元素中的每一个都可以用 `cache_control` 标记以启用该部分请求的缓存。

### 缓存限制
最小可缓存提示词长度为：
- 1024 个令牌，用于 Claude Opus 4.5、Claude Opus 4.1、Claude Opus 4、Claude Sonnet 4.5、Claude Sonnet 4、Claude Sonnet 3.7 ([已弃用](/docs/zh-CN/about-claude/model-deprecations)) 和 Claude Opus 3 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- 4096 个令牌，用于 Claude Haiku 4.5
- 2048 个令牌，用于 Claude Haiku 3.5 和 Claude Haiku 3

较短的提示词无法缓存，即使标记有 `cache_control`。任何缓存少于此数量令牌的请求都将在没有缓存的情况下处理。要查看提示词是否被缓存，请参阅响应使用 [字段](/docs/zh-CN/build-with-claude/prompt-caching#tracking-cache-performance)。

对于并发请求，请注意缓存条目仅在第一个响应开始后才可用。如果您需要并行请求的缓存命中，请在发送后续请求之前等待第一个响应。

目前，"ephemeral"是唯一支持的缓存类型，默认生命周期为 5 分钟。

### 理解缓存断点成本

**缓存断点本身不会增加任何成本。** 您仅需为以下内容付费：
- **缓存写入**：当新内容写入缓存时（5 分钟 TTL 的基础输入令牌价格的 25% 更多）
- **缓存读取**：当使用缓存内容时（基础输入令牌价格的 10%）
- **常规输入令牌**：对于任何未缓存的内容

添加更多 `cache_control` 断点不会增加您的成本 - 您仍然根据实际缓存和读取的内容支付相同的金额。断点只是让您能够控制哪些部分可以独立缓存。

### 可以缓存的内容
请求中的大多数块都可以用 `cache_control` 指定进行缓存。这包括：

- 工具：`tools` 数组中的工具定义
- 系统消息：`system` 数组中的内容块
- 文本消息：`messages.content` 数组中的内容块，用于用户和助手轮次
- 图像和文档：`messages.content` 数组中的内容块，在用户轮次中
- 工具使用和工具结果：`messages.content` 数组中的内容块，在用户和助手轮次中

这些元素中的每一个都可以用 `cache_control` 标记以启用该部分请求的缓存。

### 无法缓存的内容
虽然大多数请求块都可以缓存，但也有一些例外：

- 思考块无法直接用 `cache_control` 缓存。但是，当思考块出现在先前的助手轮次中时，它们可以与其他内容一起缓存。以这种方式缓存时，它们在从缓存读取时确实计为输入令牌。
- 子内容块（如 [引用](/docs/zh-CN/build-with-claude/citations)）本身无法直接缓存。相反，缓存顶级块。

    在引用的情况下，作为引用源材料的顶级文档内容块可以缓存。这允许您通过缓存引用将引用的文档来有效地使用提示词缓存。
- 空文本块无法缓存。

### 缓存限制
最小可缓存提示词长度为：
- 1024 个令牌，用于 Claude Opus 4.5、Claude Opus 4.1、Claude Opus 4、Claude Sonnet 4.5、Claude Sonnet 4、Claude Sonnet 3.7 ([已弃用](/docs/zh-CN/about-claude/model-deprecations)) 和 Claude Opus 3 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- 4096 个令牌，用于 Claude Haiku 4.5
- 2048 个令牌，用于 Claude Haiku 3.5 和 Claude Haiku 3

较短的提示词无法缓存，即使标记有 `cache_control`。任何缓存少于此数量令牌的请求都将在没有缓存的情况下处理。要查看提示词是否被缓存，请参阅响应使用 [字段](/docs/zh-CN/build-with-claude/prompt-caching#tracking-cache-performance)。

对于并发请求，请注意缓存条目仅在第一个响应开始后才可用。如果您需要并行请求的缓存命中，请在发送后续请求之前等待第一个响应。

目前，"ephemeral"是唯一支持的缓存类型，默认生命周期为 5 分钟。

### 理解缓存断点成本

**缓存断点本身不会增加任何成本。** 您仅需为以下内容付费：
- **缓存写入**：当新内容写入缓存时（5 分钟 TTL 的基础输入令牌价格的 25% 更多）
- **缓存读取**：当使用缓存内容时（基础输入令牌价格的 10%）
- **常规输入令牌**：对于任何未缓存的内容

添加更多 `cache_control` 断点不会增加您的成本 - 您仍然根据实际缓存和读取的内容支付相同的金额。断点只是让您能够控制哪些部分可以独立缓存。

### 可以缓存的内容
请求中的大多数块都可以用 `cache_control` 指定进行缓存。这包括：

- 工具：`tools` 数组中的工具定义
- 系统消息：`system` 数组中的内容块
- 文本消息：`messages.content` 数组中的内容块，用于用户和助手轮次
- 图像和文档：`messages.content` 数组中的内容块，在用户轮次中
- 工具使用和工具结果：`messages.content` 数组中的内容块，在用户和助手轮次中

这些元素中的每一个都可以用 `cache_control` 标记以启用该部分请求的缓存。

### 无法缓存的内容
虽然大多数请求块都可以缓存，但也有一些例外：

- 思考块无法直接用 `cache_control` 缓存。但是，当思考块出现在先前的助手轮次中时，它们可以与其他内容一起缓存。以这种方式缓存时，它们在从缓存读取时确实计为输入令牌。
- 子内容块（如 [引用](/docs/zh-CN/build-with-claude/citations)）本身无法直接缓存。相反，缓存顶级块。

    在引用的情况下，作为引用源材料的顶级文档内容块可以缓存。这允许您通过缓存引用将引用的文档来有效地使用提示词缓存。
- 空文本块无法缓存。

### 什么使缓存失效

对缓存内容的修改可能会使部分或全部缓存失效。

如 [构建您的提示词](#structuring-your-prompt) 中所述，缓存遵循层次结构：`tools` → `system` → `messages`。每个级别的更改会使该级别及所有后续级别失效。

下表显示了不同类型的更改会使缓存的哪些部分失效。✘ 表示缓存失效，✓ 表示缓存保持有效。

| 更改内容 | 工具缓存 | 系统缓存 | 消息缓存 | 影响 |
|------------|------------------|---------------|----------------|-------------|
| **工具定义** | ✘ | ✘ | ✘ | 修改工具定义（名称、描述、参数）会使整个缓存失效 |
| **网络搜索切换** | ✓ | ✘ | ✘ | 启用/禁用网络搜索会修改系统提示词 |
| **引用切换** | ✓ | ✘ | ✘ | 启用/禁用引用会修改系统提示词 |
| **工具选择** | ✓ | ✓ | ✘ | 对 `tool_choice` 参数的更改仅影响消息块 |
| **图像** | ✓ | ✓ | ✘ | 在提示词中的任何位置添加/删除图像会影响消息块 |
| **思考参数** | ✓ | ✓ | ✘ | 对扩展思考设置（启用/禁用、预算）的更改会影响消息块 |
| **传递给扩展思考请求的非工具结果** | ✓ | ✓ | ✘ | 当在启用扩展思考的请求中传递非工具结果时，所有先前缓存的思考块都会从上下文中删除，任何在这些思考块之后的上下文中的消息都会从缓存中删除。有关更多详情，请参阅 [使用思考块进行缓存](#caching-with-thinking-blocks)。 |

### 缓存限制
最小可缓存提示词长度为：
- 1024 个令牌，用于 Claude Opus 4.5、Claude Opus 4.1、Claude Opus 4、Claude Sonnet 4.5、Claude Sonnet 4、Claude Sonnet 3.7 ([已弃用](/docs/zh-CN/about-claude/model-deprecations)) 和 Claude Opus 3 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- 4096 个令牌，用于 Claude Haiku 4.5
- 2048 个令牌，用于 Claude Haiku 3.5 和 Claude Haiku 3

较短的提示词无法缓存，即使标记有 `cache_control`。任何缓存少于此数量令牌的请求都将在没有缓存的情况下处理。要查看提示词是否被缓存，请参阅响应使用 [字段](/docs/zh-CN/build-with-claude/prompt-caching#tracking-cache-performance)。

对于并发请求，请注意缓存条目仅在第一个响应开始后才可用。如果您需要并行请求的缓存命中，请在发送后续请求之前等待第一个响应。

目前，"ephemeral"是唯一支持的缓存类型，默认生命周期为 5 分钟。

### 理解缓存断点成本

**缓存断点本身不会增加任何成本。** 您仅需为以下内容付费：
- **缓存写入**：当新内容写入缓存时（5 分钟 TTL 的基础输入令牌价格的 25% 更多）
- **缓存读取**：当使用缓存内容时（基础输入令牌价格的 10%）
- **常规输入令牌**：对于任何未缓存的内容

添加更多 `cache_control` 断点不会增加您的成本 - 您仍然根据实际缓存和读取的内容支付相同的金额。断点只是让您能够控制哪些部分可以独立缓存。

### 可以缓存的内容
请求中的大多数块都可以用 `cache_control` 指定进行缓存。这包括：

- 工具：`tools` 数组中的工具定义
- 系统消息：`system` 数组中的内容块
- 文本消息：`messages.content` 数组中的内容块，用于用户和助手轮次
- 图像和文档：`messages.content` 数组中的内容块，在用户轮次中
- 工具使用和工具结果：`messages.content` 数组中的内容块，在用户和助手轮次中

这些元素中的每一个都可以用 `cache_control` 标记以启用该部分请求的缓存。

### 无法缓存的内容
虽然大多数请求块都可以缓存，但也有一些例外：

- 思考块无法直接用 `cache_control` 缓存。但是，当思考块出现在先前的助手轮次中时，它们可以与其他内容一起缓存。以这种方式缓存时，它们在从缓存读取时确实计为输入令牌。
- 子内容块（如 [引用](/docs/zh-CN/build-with-claude/citations)）本身无法直接缓存。相反，缓存顶级块。

    在引用的情况下，作为引用源材料的顶级文档内容块可以缓存。这允许您通过缓存引用将引用的文档来有效地使用提示词缓存。
- 空文本块无法缓存。

### 什么使缓存失效

对缓存内容的修改可能会使部分或全部缓存失效。

如 [构建您的提示词](#structuring-your-prompt) 中所述，缓存遵循层次结构：`tools` → `system` → `messages`。每个级别的更改会使该级别及所有后续级别失效。

下表显示了不同类型的更改会使缓存的哪些部分失效。✘ 表示缓存失效，✓ 表示缓存保持有效。

| 更改内容 | 工具缓存 | 系统缓存 | 消息缓存 | 影响 |
|------------|------------------|---------------|----------------|-------------|
| **工具定义** | ✘ | ✘ | ✘ | 修改工具定义（名称、描述、参数）会使整个缓存失效 |
| **网络搜索切换** | ✓ | ✘ | ✘ | 启用/禁用网络搜索会修改系统提示词 |
| **引用切换** | ✓ | ✘ | ✘ | 启用/禁用引用会修改系统提示词 |
| **工具选择** | ✓ | ✓ | ✘ | 对 `tool_choice` 参数的更改仅影响消息块 |
| **图像** | ✓ | ✓ | ✘ | 在提示词中的任何位置添加/删除图像会影响消息块 |
| **思考参数** | ✓ | ✓ | ✘ | 对扩展思考设置（启用/禁用、预算）的更改会影响消息块 |
| **传递给扩展思考请求的非工具结果** | ✓ | ✓ | ✘ | 当在启用扩展思考的请求中传递非工具结果时，所有先前缓存的思考块都会从上下文中删除，任何在这些思考块之后的上下文中的消息都会从缓存中删除。有关更多详情，请参阅 [使用思考块进行缓存](#caching-with-thinking-blocks)。 |

### 跟踪缓存性能

使用这些 API 响应字段监控缓存性能，在响应中的 `usage` 内（或如果 [流式传输](/docs/zh-CN/build-with-claude/streaming)，则在 `message_start` 事件中）：

- `cache_creation_input_tokens`：创建新条目时写入缓存的令牌数。
- `cache_read_input_tokens`：为此请求从缓存检索的令牌数。
- `input_tokens`：未从缓存读取或用于创建缓存的输入令牌数（即最后一个缓存断点之后的令牌）。

<Note>
**理解令牌分解**

`input_tokens` 字段仅表示在您请求中最后一个缓存断点**之后**的令牌 - 不是您发送的所有输入令牌。

要计算总输入令牌：
```
total_input_tokens = cache_read_input_tokens + cache_creation_input_tokens + input_tokens
```

**空间解释：**
- `cache_read_input_tokens` = 断点前已缓存的令牌（读取）
- `cache_creation_input_tokens` = 断点前现在被缓存的令牌（写入）
- `input_tokens` = 最后一个断点后的令牌（不符合缓存条件）

**示例：** 如果您有一个请求，其中有 100,000 个令牌的缓存内容（从缓存读取），0 个令牌的新内容被缓存，以及 50 个令牌在您的用户消息中（在缓存断点之后）：
- `cache_read_input_tokens`：100,000
- `cache_creation_input_tokens`：0
- `input_tokens`：50
- **处理的总输入令牌**：100,050 个令牌

这对于理解成本和速率限制很重要，因为在有效使用缓存时，`input_tokens` 通常会比总输入小得多。
</Note>

### 缓存限制
最小可缓存提示长度为：
- Claude Opus 4.5、Claude Opus 4.1、Claude Opus 4、Claude Sonnet 4.5、Claude Sonnet 4、Claude Sonnet 3.7（[已弃用](/docs/zh-CN/about-claude/model-deprecations)）和 Claude Opus 3（[已弃用](/docs/zh-CN/about-claude/model-deprecations)）为 1024 个令牌
- Claude Haiku 4.5 为 4096 个令牌
- Claude Haiku 3.5 和 Claude Haiku 3 为 2048 个令牌

较短的提示无法缓存，即使标记了 `cache_control`。任何缓存少于此令牌数的请求都将在不使用缓存的情况下处理。要查看提示是否已缓存，请参阅响应使用情况[字段](/docs/zh-CN/build-with-claude/prompt-caching#tracking-cache-performance)。

对于并发请求，请注意缓存条目仅在第一个响应开始后才可用。如果需要并行请求的缓存命中，请在发送后续请求之前等待第一个响应。

目前，"ephemeral"是唯一支持的缓存类型，默认生存期为 5 分钟。

### 理解缓存断点成本

**缓存断点本身不会增加任何成本。** 您只需支付：
- **缓存写入**：当新内容写入缓存时（5 分钟 TTL 的基础输入令牌的 25% 更多）
- **缓存读取**：当使用缓存内容时（基础输入令牌价格的 10%）
- **常规输入令牌**：对于任何未缓存的内容

添加更多 `cache_control` 断点不会增加成本 - 您仍然只需根据实际缓存和读取的内容支付相同的金额。断点只是让您能够控制哪些部分可以独立缓存。

### 可以缓存的内容
请求中的大多数块都可以用 `cache_control` 指定为缓存。这包括：

- 工具：`tools` 数组中的工具定义
- 系统消息：`system` 数组中的内容块
- 文本消息：`messages.content` 数组中的内容块，用于用户和助手轮次
- 图像和文档：`messages.content` 数组中的内容块，在用户轮次中
- 工具使用和工具结果：`messages.content` 数组中的内容块，在用户和助手轮次中

这些元素中的每一个都可以用 `cache_control` 标记以启用该部分请求的缓存。

### 无法缓存的内容
虽然大多数请求块都可以缓存，但也有一些例外：

- 思考块无法直接用 `cache_control` 缓存。但是，当思考块出现在之前的助手轮次中时，它们可以与其他内容一起缓存。以这种方式缓存时，从缓存读取时它们确实计为输入令牌。
- 子内容块（如[引用](/docs/zh-CN/build-with-claude/citations)）本身无法直接缓存。而是缓存顶级块。

    在引用的情况下，作为引用源材料的顶级文档内容块可以缓存。这允许您通过缓存引用将引用的文档来有效地使用提示缓存和引用。
- 空文本块无法缓存。

### 什么会使缓存失效

对缓存内容的修改可能会使部分或全部缓存失效。

如[结构化您的提示](#structuring-your-prompt)中所述，缓存遵循层次结构：`tools` → `system` → `messages`。每个级别的更改都会使该级别及所有后续级别失效。

下表显示了不同类型的更改会使缓存的哪些部分失效。✘ 表示缓存失效，✓ 表示缓存保持有效。

| 更改内容 | 工具缓存 | 系统缓存 | 消息缓存 | 影响 |
|------------|------------------|---------------|----------------|-------------|
| **工具定义** | ✘ | ✘ | ✘ | 修改工具定义（名称、描述、参数）会使整个缓存失效 |
| **网络搜索切换** | ✓ | ✘ | ✘ | 启用/禁用网络搜索会修改系统提示 |
| **引用切换** | ✓ | ✘ | ✘ | 启用/禁用引用会修改系统提示 |
| **工具选择** | ✓ | ✓ | ✘ | 对 `tool_choice` 参数的更改仅影响消息块 |
| **图像** | ✓ | ✓ | ✘ | 在提示中的任何位置添加/删除图像会影响消息块 |
| **思考参数** | ✓ | ✓ | ✘ | 对扩展思考设置的更改（启用/禁用、预算）会影响消息块 |
| **传递给扩展思考请求的非工具结果** | ✓ | ✓ | ✘ | 当在启用扩展思考的请求中传递非工具结果时，所有先前缓存的思考块都会从上下文中删除，任何跟在这些思考块之后的上下文中的消息都会从缓存中删除。有关更多详情，请参阅[使用思考块缓存](#caching-with-thinking-blocks)。 |

### 跟踪缓存性能

使用这些 API 响应字段监控缓存性能，在响应中的 `usage` 内（或如果[流式传输](/docs/zh-CN/build-with-claude/streaming)则为 `message_start` 事件）：

- `cache_creation_input_tokens`：创建新条目时写入缓存的令牌数。
- `cache_read_input_tokens`：为此请求从缓存检索的令牌数。
- `input_tokens`：未从缓存读取或用于创建缓存的输入令牌数（即最后一个缓存断点之后的令牌）。

<Note>
**理解令牌分解**

`input_tokens` 字段仅表示请求中**最后一个缓存断点之后**的令牌 - 不是您发送的所有输入令牌。

要计算总输入令牌：
```
total_input_tokens = cache_read_input_tokens + cache_creation_input_tokens + input_tokens
```

**空间解释：**
- `cache_read_input_tokens` = 断点前已缓存的令牌（读取）
- `cache_creation_input_tokens` = 断点前现在被缓存的令牌（写入）
- `input_tokens` = 最后一个断点后的令牌（不符合缓存条件）

**示例：** 如果您有一个请求，其中包含 100,000 个令牌的缓存内容（从缓存读取），0 个令牌的新内容被缓存，以及用户消息中的 50 个令牌（在缓存断点之后）：
- `cache_read_input_tokens`：100,000
- `cache_creation_input_tokens`：0
- `input_tokens`：50
- **处理的总输入令牌**：100,050 个令牌

这对于理解成本和速率限制很重要，因为在有效使用缓存时，`input_tokens` 通常会比总输入小得多。
</Note>

### 有效缓存的最佳实践

要优化提示缓存性能：

- 缓存稳定、可重用的内容，如系统指令、背景信息、大型上下文或频繁的工具定义。
- 将缓存内容放在提示的开头以获得最佳性能。
- 战略性地使用缓存断点来分离不同的可缓存前缀部分。
- 在对话末尾和可编辑内容之前设置缓存断点以最大化缓存命中率，特别是在处理具有 20 多个内容块的提示时。
- 定期分析缓存命中率并根据需要调整策略。

### 针对不同用例进行优化

根据您的场景定制提示缓存策略：

- 对话代理：降低扩展对话的成本和延迟，特别是那些具有长指令或上传文档的对话。
- 编码助手：通过在提示中保留相关部分或代码库的摘要版本来改进自动完成和代码库问答。
- 大型文档处理：在提示中包含完整的长篇材料（包括图像），而不会增加响应延迟。
- 详细指令集：共享广泛的指令、程序和示例列表以微调 Claude 的响应。开发人员通常在提示中包含一两个示例，但使用提示缓存，您可以通过包含 20 多个高质量答案的多样化示例来获得更好的性能。
- 代理工具使用：增强涉及多个工具调用和迭代代码更改的场景的性能，其中每个步骤通常需要新的 API 调用。
- 与书籍、论文、文档、播客文字记录和其他长篇内容交流：通过将整个文档嵌入提示中并让用户向其提问来激活任何知识库。

### 针对不同用例进行优化

根据您的场景定制提示缓存策略：

- 对话代理：降低扩展对话的成本和延迟，特别是那些具有长指令或上传文档的对话。
- 编码助手：通过在提示中保留相关部分或代码库的摘要版本来改进自动完成和代码库问答。
- 大型文档处理：在提示中包含完整的长篇材料（包括图像），而不会增加响应延迟。
- 详细指令集：共享广泛的指令、程序和示例列表以微调 Claude 的响应。开发人员通常在提示中包含一两个示例，但使用提示缓存，您可以通过包含 20 多个高质量答案的多样化示例来获得更好的性能。
- 代理工具使用：增强涉及多个工具调用和迭代代码更改的场景的性能，其中每个步骤通常需要新的 API 调用。
- 与书籍、论文、文档、播客文字记录和其他长篇内容交流：通过将整个文档嵌入提示中并让用户向其提问来激活任何知识库。

### 故障排除常见问题

如果遇到意外行为：

- 确保缓存部分相同，并在调用中的相同位置用 cache_control 标记
- 检查调用是否在缓存生存期内进行（默认为 5 分钟）
- 验证 `tool_choice` 和图像使用在调用之间保持一致
- 验证您缓存的令牌数至少达到最小数量
- 系统会自动检查先前内容块边界处的缓存命中（断点前最多约 20 个块）。对于具有 20 多个内容块的提示，您可能需要在提示中更早的位置添加额外的 `cache_control` 参数以确保所有内容都可以缓存
- 验证 `tool_use` 内容块中的键具有稳定的排序，因为某些语言（例如 Swift、Go）在 JSON 转换期间随机化键顺序，破坏缓存

<Note>
对 `tool_choice` 的更改或提示中任何位置的图像的存在/不存在将使缓存失效，需要创建新的缓存条目。有关缓存失效的更多详情，请参阅[什么会使缓存失效](#what-invalidates-the-cache)。
</Note>

### 使用思考块缓存

当使用[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)和提示缓存时，思考块具有特殊行为：

**与其他内容自动缓存**：虽然思考块无法用 `cache_control` 显式标记，但当您使用工具结果进行后续 API 调用时，它们会作为请求内容的一部分被缓存。这通常在工具使用期间发生，当您将思考块传回以继续对话时。

**输入令牌计数**：当从缓存读取思考块时，它们在使用情况指标中计为输入令牌。这对于成本计算和令牌预算很重要。

**缓存失效模式**：
- 当仅将工具结果作为用户消息提供时，缓存保持有效
- 当添加非工具结果用户内容时，缓存失效，导致所有先前的思考块被删除
- 即使没有显式 `cache_control` 标记，也会发生此缓存行为

有关缓存失效的更多详情，请参阅[什么会使缓存失效](#what-invalidates-the-cache)。

**工具使用示例**：
```
Request 1: User: "What's the weather in Paris?"
Response: [thinking_block_1] + [tool_use block 1]

Request 2:
User: ["What's the weather in Paris?"],
Assistant: [thinking_block_1] + [tool_use block 1],
User: [tool_result_1, cache=True]
Response: [thinking_block_2] + [text block 2]
# Request 2 caches its request content (not the response)
# The cache includes: user message, thinking_block_1, tool_use block 1, and tool_result_1

Request 3:
User: ["What's the weather in Paris?"],
Assistant: [thinking_block_1] + [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [thinking_block_2] + [text block 2],
User: [Text response, cache=True]
# Non-tool-result user block causes all thinking blocks to be ignored
# This request is processed as if thinking blocks were never present
```

当包含非工具结果用户块时，它指定一个新的助手循环，所有先前的思考块都从上下文中删除。

有关更详细的信息，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking#understanding-thinking-block-caching-behavior)。

---
## 缓存存储和共享

- **组织隔离**：缓存在组织之间隔离。不同的组织永远不会共享缓存，即使他们使用相同的提示。

- **精确匹配**：缓存命中需要 100% 相同的提示段，包括所有文本和图像，直到并包括用缓存控制标记的块。

- **输出令牌生成**：提示缓存对输出令牌生成没有影响。您收到的响应将与不使用提示缓存时收到的响应相同。

---
### 针对不同用例进行优化

根据您的场景定制提示缓存策略：

- 对话代理：降低扩展对话的成本和延迟，特别是那些具有长指令或上传文档的对话。
- 编码助手：通过在提示中保留相关部分或代码库的摘要版本来改进自动完成和代码库问答。
- 大型文档处理：在提示中包含完整的长篇材料（包括图像），而不会增加响应延迟。
- 详细指令集：共享广泛的指令、程序和示例列表以微调 Claude 的响应。开发人员通常在提示中包含一两个示例，但使用提示缓存，您可以通过包含 20 多个高质量答案的多样化示例来获得更好的性能。
- 代理工具使用：增强涉及多个工具调用和迭代代码更改的场景的性能，其中每个步骤通常需要新的 API 调用。
- 与书籍、论文、文档、播客文字记录和其他长篇内容交流：通过将整个文档嵌入提示中并让用户向其提问来激活任何知识库。

### 故障排除常见问题

如果遇到意外行为：

- 确保缓存部分相同，并在调用中的相同位置用 cache_control 标记
- 检查调用是否在缓存生存期内进行（默认为 5 分钟）
- 验证 `tool_choice` 和图像使用在调用之间保持一致
- 验证您缓存的令牌数至少达到最小数量
- 系统会自动检查先前内容块边界处的缓存命中（断点前最多约 20 个块）。对于具有 20 多个内容块的提示，您可能需要在提示中更早的位置添加额外的 `cache_control` 参数以确保所有内容都可以缓存
- 验证 `tool_use` 内容块中的键具有稳定的排序，因为某些语言（例如 Swift、Go）在 JSON 转换期间随机化键顺序，破坏缓存

<Note>
对 `tool_choice` 的更改或提示中任何位置的图像的存在/不存在将使缓存失效，需要创建新的缓存条目。有关缓存失效的更多详情，请参阅[什么会使缓存失效](#what-invalidates-the-cache)。
</Note>

### 使用思考块缓存

当使用[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)和提示缓存时，思考块具有特殊行为：

**与其他内容自动缓存**：虽然思考块无法用 `cache_control` 显式标记，但当您使用工具结果进行后续 API 调用时，它们会作为请求内容的一部分被缓存。这通常在工具使用期间发生，当您将思考块传回以继续对话时。

**输入令牌计数**：当从缓存读取思考块时，它们在使用情况指标中计为输入令牌。这对于成本计算和令牌预算很重要。

**缓存失效模式**：
- 当仅将工具结果作为用户消息提供时，缓存保持有效
- 当添加非工具结果用户内容时，缓存失效，导致所有先前的思考块被删除
- 即使没有显式 `cache_control` 标记，也会发生此缓存行为

有关缓存失效的更多详情，请参阅[什么会使缓存失效](#what-invalidates-the-cache)。

**工具使用示例**：
```
Request 1: User: "What's the weather in Paris?"
Response: [thinking_block_1] + [tool_use block 1]

Request 2:
User: ["What's the weather in Paris?"],
Assistant: [thinking_block_1] + [tool_use block 1],
User: [tool_result_1, cache=True]
Response: [thinking_block_2] + [text block 2]
# Request 2 caches its request content (not the response)
# The cache includes: user message, thinking_block_1, tool_use block 1, and tool_result_1

Request 3:
User: ["What's the weather in Paris?"],
Assistant: [thinking_block_1] + [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [thinking_block_2] + [text block 2],
User: [Text response, cache=True]
# Non-tool-result user block causes all thinking blocks to be ignored
# This request is processed as if thinking blocks were never present
```

当包含非工具结果用户块时，它指定一个新的助手循环，所有先前的思考块都从上下文中删除。

有关更详细的信息，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking#understanding-thinking-block-caching-behavior)。

---
## 缓存存储和共享

- **组织隔离**：缓存在组织之间隔离。不同的组织永远不会共享缓存，即使他们使用相同的提示。

- **精确匹配**：缓存命中需要 100% 相同的提示段，包括所有文本和图像，直到并包括用缓存控制标记的块。

- **输出令牌生成**：提示缓存对输出令牌生成没有影响。您收到的响应将与不使用提示缓存时收到的响应相同。

---
## 1 小时缓存持续时间

如果您发现 5 分钟太短，Anthropic 还提供 1 小时缓存持续时间[需额外付费](#pricing)。

要使用扩展缓存，请在 `cache_control` 定义中包含 `ttl`，如下所示：
```json
"cache_control": {
    "type": "ephemeral",
    "ttl": "5m" | "1h"
}
```

响应将包含详细的缓存信息，如下所示：
```json
{
    "usage": {
        "input_tokens": ...,
        "cache_read_input_tokens": ...,
        "cache_creation_input_tokens": ...,
        "output_tokens": ...,

        "cache_creation": {
            "ephemeral_5m_input_tokens": 456,
            "ephemeral_1h_input_tokens": 100,
        }
    }
}
```

请注意，当前的 `cache_creation_input_tokens` 字段等于 `cache_creation` 对象中值的总和。

### 何时使用 1 小时缓存

如果您有定期使用的提示（即每 5 分钟以上使用一次的系统提示），请继续使用 5 分钟缓存，因为这将继续以无额外费用的方式刷新。

1 小时缓存最适合用于以下场景：
- 当您有可能使用频率少于 5 分钟但多于每小时一次的提示时。例如，当代理端代理需要超过 5 分钟时，或者当存储与用户的长聊天对话时，您通常预期该用户可能在接下来的 5 分钟内不会响应。
- 当延迟很重要且您的后续提示可能在 5 分钟后发送时。
- 当您想改进速率限制利用率时，因为缓存命中不会从您的速率限制中扣除。

<Note>
5 分钟和 1 小时缓存在延迟方面的行为相同。对于长文档，您通常会看到改进的首令牌时间。
</Note>

### 针对不同用例进行优化

根据您的场景定制提示词缓存策略：

- 对话代理：降低成本并减少延迟，特别是对于具有长指令或上传文档的扩展对话。
- 编码助手：通过在提示词中保留相关部分或代码库的摘要版本，改进自动完成和代码库问答。
- 大型文档处理：在提示词中包含完整的长篇材料（包括图像），而不会增加响应延迟。
- 详细指令集：共享大量指令、程序和示例列表，以微调 Claude 的响应。开发人员通常在提示词中包含一两个示例，但使用提示词缓存，您可以通过包含 20 多个高质量答案的多样化示例来获得更好的性能。
- 代理工具使用：增强涉及多个工具调用和迭代代码更改的场景的性能，其中每个步骤通常需要新的 API 调用。
- 与书籍、论文、文档、播客转录和其他长篇内容交互：通过将整个文档嵌入提示词中，让用户向其提问，使任何知识库活跃起来。

### 排查常见问题

如果遇到意外行为：

- 确保缓存的部分相同，并在调用中的相同位置用 cache_control 标记
- 检查调用是否在缓存生命周期内进行（默认为 5 分钟）
- 验证 `tool_choice` 和图像使用在调用之间保持一致
- 验证您至少缓存了最少数量的令牌
- 系统会自动在之前的内容块边界处检查缓存命中（断点前最多约 20 个块）。对于超过 20 个内容块的提示词，您可能需要在提示词的早期添加额外的 `cache_control` 参数，以确保所有内容都可以被缓存
- 验证 `tool_use` 内容块中的键具有稳定的排序，因为某些语言（例如 Swift、Go）在 JSON 转换期间随机化键顺序，破坏缓存

<Note>
对 `tool_choice` 的更改或提示词中任何位置的图像的存在/不存在将使缓存失效，需要创建新的缓存条目。有关缓存失效的更多详情，请参阅[什么会使缓存失效](#what-invalidates-the-cache)。
</Note>

### 使用思考块进行缓存

当使用[扩展思考](/docs/zh-CN/build-with-claude/extended-thinking)与提示词缓存时，思考块具有特殊行为：

**与其他内容一起自动缓存**：虽然思考块不能显式用 `cache_control` 标记，但当您使用工具结果进行后续 API 调用时，它们会作为请求内容的一部分被缓存。这通常发生在工具使用期间，当您传回思考块以继续对话时。

**输入令牌计数**：当思考块从缓存中读取时，它们在您的使用指标中计为输入令牌。这对于成本计算和令牌预算很重要。

**缓存失效模式**：
- 当仅将工具结果作为用户消息提供时，缓存保持有效
- 当添加非工具结果用户内容时，缓存失效，导致所有之前的思考块被删除
- 即使没有显式的 `cache_control` 标记，也会发生此缓存行为

有关缓存失效的更多详情，请参阅[什么会使缓存失效](#what-invalidates-the-cache)。

**工具使用示例**：
```
请求 1：用户："巴黎的天气如何？"
响应：[thinking_block_1] + [tool_use 块 1]

请求 2：
用户：["巴黎的天气如何？"],
助手：[thinking_block_1] + [tool_use 块 1],
用户：[tool_result_1, cache=True]
响应：[thinking_block_2] + [文本块 2]
# 请求 2 缓存其请求内容（不是响应）
# 缓存包括：用户消息、thinking_block_1、tool_use 块 1 和 tool_result_1

请求 3：
用户：["巴黎的天气如何？"],
助手：[thinking_block_1] + [tool_use 块 1],
用户：[tool_result_1, cache=True],
助手：[thinking_block_2] + [文本块 2],
用户：[文本响应, cache=True]
# 非工具结果用户块导致所有思考块被忽略
# 此请求的处理方式就像思考块从未存在过一样
```

当包含非工具结果用户块时，它指定了新的助手循环，所有之前的思考块都从上下文中删除。

有关更详细的信息，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking#understanding-thinking-block-caching-behavior)。

---
## 缓存存储和共享

- **组织隔离**：缓存在组织之间隔离。不同的组织永远不会共享缓存，即使他们使用相同的提示词。

- **精确匹配**：缓存命中需要 100% 相同的提示词段，包括所有文本和图像，直到并包括用缓存控制标记的块。

- **输出令牌生成**：提示词缓存对输出令牌生成没有影响。您收到的响应将与不使用提示词缓存时收到的响应相同。

---
## 1 小时缓存持续时间

如果您发现 5 分钟太短，Anthropic 还提供 1 小时缓存持续时间[需额外付费](#pricing)。

要使用扩展缓存，请在 `cache_control` 定义中包含 `ttl`，如下所示：
```json
"cache_control": {
    "type": "ephemeral",
    "ttl": "5m" | "1h"
}
```

响应将包含详细的缓存信息，如下所示：
```json
{
    "usage": {
        "input_tokens": ...,
        "cache_read_input_tokens": ...,
        "cache_creation_input_tokens": ...,
        "output_tokens": ...,

        "cache_creation": {
            "ephemeral_5m_input_tokens": 456,
            "ephemeral_1h_input_tokens": 100,
        }
    }
}
```

请注意，当前的 `cache_creation_input_tokens` 字段等于 `cache_creation` 对象中值的总和。

### 何时使用 1 小时缓存

如果您有定期使用的提示词（即每 5 分钟以上频率使用的系统提示词），请继续使用 5 分钟缓存，因为这将继续以无额外费用的方式刷新。

1 小时缓存最适合用于以下场景：
- 当您有可能使用频率少于 5 分钟但多于每小时一次的提示词时。例如，当代理端代理将花费超过 5 分钟时，或者当存储与用户的长聊天对话时，您通常预期该用户可能在接下来的 5 分钟内不会响应。
- 当延迟很重要且您的后续提示词可能在 5 分钟后发送时。
- 当您想改进速率限制利用率时，因为缓存命中不会从您的速率限制中扣除。

<Note>
5 分钟和 1 小时缓存在延迟方面的行为相同。对于长文档，您通常会看到首个令牌时间的改进。
</Note>

### 混合不同的 TTL

您可以在同一请求中使用 1 小时和 5 分钟缓存控制，但有一个重要的限制：具有较长 TTL 的缓存条目必须出现在较短 TTL 之前（即 1 小时缓存条目必须出现在任何 5 分钟缓存条目之前）。

混合 TTL 时，我们在您的提示词中确定三个计费位置：
1. 位置 `A`：最高缓存命中处的令牌计数（如果没有命中则为 0）。
2. 位置 `B`：`A` 之后最高 1 小时 `cache_control` 块处的令牌计数（如果不存在则等于 `A`）。
3. 位置 `C`：最后一个 `cache_control` 块处的令牌计数。

<Note>
如果 `B` 和/或 `C` 大于 `A`，它们必然是缓存未命中，因为 `A` 是最高缓存命中。
</Note>

您将被收费：
1. `A` 的缓存读取令牌。
2. `(B - A)` 的 1 小时缓存写入令牌。
3. `(C - B)` 的 5 分钟缓存写入令牌。

这里有 3 个示例。这描绘了 3 个请求的输入令牌，每个请求都有不同的缓存命中和缓存未命中。每个都有不同的计算定价，如彩色框所示。
![混合 TTL 图表](/docs/images/prompt-cache-mixed-ttl.svg)

---

## 提示词缓存示例

为了帮助您开始使用提示词缓存，我们准备了一个[提示词缓存食谱](https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb)，其中包含详细的示例和最佳实践。

下面，我们包含了几个代码片段，展示了各种提示词缓存模式。这些示例演示了如何在不同场景中实现缓存，帮助您理解此功能的实际应用：

<section title="大型上下文缓存示例">

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "system": [
        {
            "type": "text",
            "text": "You are an AI assistant tasked with analyzing legal documents."
        },
        {
            "type": "text",
            "text": "Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    "messages": [
        {
            "role": "user",
            "content": "What are the key terms and conditions in this agreement?"
        }
    ]
}'
```

```python Python
import anthropic
client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "You are an AI assistant tasked with analyzing legal documents."
        },
        {
            "type": "text",
            "text": "Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[
        {
            "role": "user",
            "content": "What are the key terms and conditions in this agreement?"
        }
    ]
)
print(response.model_dump_json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  system: [
    {
        "type": "text",
        "text": "You are an AI assistant tasked with analyzing legal documents."
    },
    {
        "type": "text",
        "text": "Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]",
        "cache_control": {"type": "ephemeral"}
    }
  ],
  messages: [
    {
        "role": "user",
        "content": "What are the key terms and conditions in this agreement?"
    }
  ]
});
console.log(response);
```

```java Java
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.CacheControlEphemeral;
import com.anthropic.models.messages.Message;
import com.anthropic.models.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;

public class LegalDocumentAnalysisExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_20250514)
                .maxTokens(1024)
                .systemOfTextBlockParams(List.of(
                        TextBlockParam.builder()
                                .text("You are an AI assistant tasked with analyzing legal documents.")
                                .build(),
                        TextBlockParam.builder()
                                .text("Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]")
                                .cacheControl(CacheControlEphemeral.builder().build())
                                .build()
                ))
                .addUserMessage("What are the key terms and conditions in this agreement?")
                .build();

        Message message = client.messages().create(params);
        System.out.println(message);
    }
}
```
</CodeGroup>
此示例演示了基本的提示词缓存用法，将法律协议的完整文本作为前缀缓存，同时保持用户指令不被缓存。

对于第一个请求：
- `input_tokens`：仅用户消息中的令牌数
- `cache_creation_input_tokens`：整个系统消息中的令牌数，包括法律文档
- `cache_read_input_tokens`：0（第一个请求时没有缓存命中）

对于缓存生命周期内的后续请求：
- `input_tokens`：仅用户消息中的令牌数
- `cache_creation_input_tokens`：0（无新的缓存创建）
- `cache_read_input_tokens`：整个缓存系统消息中的令牌数

</section>
<section title="缓存工具定义">

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "tools": [
        {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "input_schema": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "The unit of temperature, either celsius or fahrenheit"
                    }
                },
                "required": ["location"]
            }
        },
        # many more tools
        {
            "name": "get_time",
            "description": "Get the current time in a given time zone",
            "input_schema": {
                "type": "object",
                "properties": {
                    "timezone": {
                        "type": "string",
                        "description": "The IANA time zone name, e.g. America/Los_Angeles"
                    }
                },
                "required": ["timezone"]
            },
            "cache_control": {"type": "ephemeral"}
        }
    ],
    "messages": [
        {
            "role": "user",
            "content": "What is the weather and time in New York?"
        }
    ]
}'
```

```python Python
import anthropic
client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    tools=[
        {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "input_schema": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "The unit of temperature, either 'celsius' or 'fahrenheit'"
                    }
                },
                "required": ["location"]
            },
        },
        # many more tools
        {
            "name": "get_time",
            "description": "Get the current time in a given time zone",
            "input_schema": {
                "type": "object",
                "properties": {
                    "timezone": {
                        "type": "string",
                        "description": "The IANA time zone name, e.g. America/Los_Angeles"
                    }
                },
                "required": ["timezone"]
            },
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[
        {
            "role": "user",
            "content": "What's the weather and time in New York?"
        }
    ]
)
print(response.model_dump_json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 1024,
    tools=[
        {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "input_schema": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "The unit of temperature, either 'celsius' or 'fahrenheit'"
                    }
                },
                "required": ["location"]
            },
        },
        // many more tools
        {
            "name": "get_time",
            "description": "Get the current time in a given time zone",
            "input_schema": {
                "type": "object",
                "properties": {
                    "timezone": {
                        "type": "string",
                        "description": "The IANA time zone name, e.g. America/Los_Angeles"
                    }
                },
                "required": ["timezone"]
            },
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages: [
        {
            "role": "user",
            "content": "What's the weather and time in New York?"
        }
    ]
});
console.log(response);
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.messages.CacheControlEphemeral;
import com.anthropic.models.messages.Message;
import com.anthropic.models.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.Tool;
import com.anthropic.models.messages.Tool.InputSchema;

public class ToolsWithCacheControlExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // Weather tool schema
        InputSchema weatherSchema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of(
                                "type", "string",
                                "description", "The city and state, e.g. San Francisco, CA"
                        ),
                        "unit", Map.of(
                                "type", "string",
                                "enum", List.of("celsius", "fahrenheit"),
                                "description", "The unit of temperature, either celsius or fahrenheit"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        // Time tool schema
        InputSchema timeSchema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "timezone", Map.of(
                                "type", "string",
                                "description", "The IANA time zone name, e.g. America/Los_Angeles"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("timezone")))
                .build();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_20250514)
                .maxTokens(1024)
                .addTool(Tool.builder()
                        .name("get_weather")
                        .description("Get the current weather in a given location")
                        .inputSchema(weatherSchema)
                        .build())
                .addTool(Tool.builder()
                        .name("get_time")
                        .description("Get the current time in a given time zone")
                        .inputSchema(timeSchema)
                        .cacheControl(CacheControlEphemeral.builder().build())
                        .build())
                .addUserMessage("What is the weather and time in New York?")
                .build();

        Message message = client.messages().create(params);
        System.out.println(message);
    }
}
```
</CodeGroup>

在此示例中，我们演示了缓存工具定义。

`cache_control` 参数放在最后一个工具（`get_time`）上，以将所有工具指定为静态前缀的一部分。

这意味着所有工具定义，包括 `get_weather` 和在 `get_time` 之前定义的任何其他工具，都将作为单个前缀被缓存。

当您有一组一致的工具想要在多个请求中重复使用而无需每次都重新处理时，此方法很有用。

对于第一个请求：
- `input_tokens`：用户消息中的令牌数
- `cache_creation_input_tokens`：所有工具定义和系统提示中的令牌数
- `cache_read_input_tokens`：0（第一个请求时没有缓存命中）

对于缓存生命周期内的后续请求：
- `input_tokens`：用户消息中的令牌数
- `cache_creation_input_tokens`：0（无新的缓存创建）
- `cache_read_input_tokens`：所有缓存工具定义和系统提示中的令牌数

</section>

<section title="继续多轮对话">

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "system": [
        {
            "type": "text",
            "text": "...long system prompt",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Hello, can you tell me more about the solar system?",
                }
            ]
        },
        {
            "role": "assistant",
            "content": "Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you would like to know more about?"
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Good to know."
                },
                {
                    "type": "text",
                    "text": "Tell me more about Mars.",
                    "cache_control": {"type": "ephemeral"}
                }
            ]
        }
    ]
}'
```

```python Python
import anthropic
client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "...long system prompt",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[
        # ...long conversation so far
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Hello, can you tell me more about the solar system?",
                }
            ]
        },
        {
            "role": "assistant",
            "content": "Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you'd like to know more about?"
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Good to know."
                },
                {
                    "type": "text",
                    "text": "Tell me more about Mars.",
                    "cache_control": {"type": "ephemeral"}
                }
            ]
        }
    ]
)
print(response.model_dump_json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 1024,
    system=[
        {
            "type": "text",
            "text": "...long system prompt",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[
        // ...long conversation so far
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Hello, can you tell me more about the solar system?",
                }
            ]
        },
        {
            "role": "assistant",
            "content": "Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you'd like to know more about?"
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Good to know."
                },
                {
                    "type": "text",
                    "text": "Tell me more about Mars.",
                    "cache_control": {"type": "ephemeral"}
                }
            ]
        }
    ]
});
console.log(response);
```

```java Java
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.CacheControlEphemeral;
import com.anthropic.models.messages.ContentBlockParam;
import com.anthropic.models.messages.Message;
import com.anthropic.models.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;

public class ConversationWithCacheControlExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // Create ephemeral system prompt
        TextBlockParam systemPrompt = TextBlockParam.builder()
                .text("...long system prompt")
                .cacheControl(CacheControlEphemeral.builder().build())
                .build();

        // Create message params
        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_20250514)
                .maxTokens(1024)
                .systemOfTextBlockParams(List.of(systemPrompt))
                // First user message (without cache control)
                .addUserMessage("Hello, can you tell me more about the solar system?")
                // Assistant response
                .addAssistantMessage("Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you would like to know more about?")
                // Second user message (with cache control)
                .addUserMessageOfBlockParams(List.of(
                        ContentBlockParam.ofText(TextBlockParam.builder()
                                .text("Good to know.")
                                .build()),
                        ContentBlockParam.ofText(TextBlockParam.builder()
                                .text("Tell me more about Mars.")
                                .cacheControl(CacheControlEphemeral.builder().build())
                                .build())
                ))
                .build();

        Message message = client.messages().create(params);
        System.out.println(message);
    }
}
```
</CodeGroup>

在此示例中，我们演示了如何在多轮对话中使用提示词缓存。

在每个回合中，我们用 `cache_control` 标记最后一条消息的最后一个块，以便对话可以增量缓存。系统将自动查找并使用最长的先前缓存块序列以进行后续消息。也就是说，之前用 `cache_control` 块标记的块稍后不会标记此参数，但如果在 5 分钟内命中它们，它们仍将被视为缓存命中（也是缓存刷新！）。

此外，请注意 `cache_control` 参数放在系统消息上。这是为了确保如果它从缓存中被逐出（在超过 5 分钟未使用后），它将在下一个请求中被添加回缓存。

此方法对于在进行中的对话中维护上下文而无需重复处理相同信息很有用。

当正确设置此方法时，您应该在每个请求的使用响应中看到以下内容：
- `input_tokens`：新用户消息中的令牌数（将很少）
- `cache_creation_input_tokens`：新的助手和用户回合中的令牌数
- `cache_read_input_tokens`：对话中直到前一个回合的令牌数

</section>

<section title="综合示例：多个缓存断点">

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "tools": [
        {
            "name": "search_documents",
            "description": "Search through the knowledge base",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Search query"
                    }
                },
                "required": ["query"]
            }
        },
        {
            "name": "get_document",
            "description": "Retrieve a specific document by ID",
            "input_schema": {
                "type": "object",
                "properties": {
                    "doc_id": {
                        "type": "string",
                        "description": "Document ID"
                    }
                },
                "required": ["doc_id"]
            },
            "cache_control": {"type": "ephemeral"}
        }
    ],
    "system": [
        {
            "type": "text",
            "text": "You are a helpful research assistant with access to a document knowledge base.\n\n# Instructions\n- Always search for relevant documents before answering\n- Provide citations for your sources\n- Be objective and accurate in your responses\n- If multiple documents contain relevant information, synthesize them\n- Acknowledge when information is not available in the knowledge base",
            "cache_control": {"type": "ephemeral"}
        },
        {
            "type": "text",
            "text": "# Knowledge Base Context\n\nHere are the relevant documents for this conversation:\n\n## Document 1: Solar System Overview\nThe solar system consists of the Sun and all objects that orbit it...\n\n## Document 2: Planetary Characteristics\nEach planet has unique features. Mercury is the smallest planet...\n\n## Document 3: Mars Exploration\nMars has been a target of exploration for decades...\n\n[Additional documents...]",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    "messages": [
        {
            "role": "user",
            "content": "Can you search for information about Mars rovers?"
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "tool_use",
                    "id": "tool_1",
                    "name": "search_documents",
                    "input": {"query": "Mars rovers"}
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "tool_result",
                    "tool_use_id": "tool_1",
                    "content": "Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)"
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": "I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document."
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Yes, please tell me about the Perseverance rover specifically.",
                    "cache_control": {"type": "ephemeral"}
                }
            ]
        }
    ]
}'
```

```python Python
import anthropic
client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    tools=[
        {
            "name": "search_documents",
            "description": "Search through the knowledge base",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Search query"
                    }
                },
                "required": ["query"]
            }
        },
        {
            "name": "get_document",
            "description": "Retrieve a specific document by ID",
            "input_schema": {
                "type": "object",
                "properties": {
                    "doc_id": {
                        "type": "string",
                        "description": "Document ID"
                    }
                },
                "required": ["doc_id"]
            },
            "cache_control": {"type": "ephemeral"}
        }
    ],
    system=[
        {
            "type": "text",
            "text": "You are a helpful research assistant with access to a document knowledge base.\n\n# Instructions\n- Always search for relevant documents before answering\n- Provide citations for your sources\n- Be objective and accurate in your responses\n- If multiple documents contain relevant information, synthesize them\n- Acknowledge when information is not available in the knowledge base",
            "cache_control": {"type": "ephemeral"}
        },
        {
            "type": "text",
            "text": "# Knowledge Base Context\n\nHere are the relevant documents for this conversation:\n\n## Document 1: Solar System Overview\nThe solar system consists of the Sun and all objects that orbit it...\n\n## Document 2: Planetary Characteristics\nEach planet has unique features. Mercury is the smallest planet...\n\n## Document 3: Mars Exploration\nMars has been a target of exploration for decades...\n\n[Additional documents...]",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[
        {
            "role": "user",
            "content": "Can you search for information about Mars rovers?"
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "tool_use",
                    "id": "tool_1",
                    "name": "search_documents",
                    "input": {"query": "Mars rovers"}
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "tool_result",
                    "tool_use_id": "tool_1",
                    "content": "Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)"
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": "I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document."
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Yes, please tell me about the Perseverance rover specifically.",
                    "cache_control": {"type": "ephemeral"}
                }
            ]
        }
    ]
)
print(response.model_dump_json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 1024,
    tools: [
        {
            name: "search_documents",
            description: "Search through the knowledge base",
            input_schema: {
                type: "object",
                properties: {
                    query: {
                        type: "string",
                        description: "Search query"
                    }
                },
                required: ["query"]
            }
        },
        {
            name: "get_document",
            description: "Retrieve a specific document by ID",
            input_schema: {
                type: "object",
                properties: {
                    doc_id: {
                        type: "string",
                        description: "Document ID"
                    }
                },
                required: ["doc_id"]
            },
            cache_control: { type: "ephemeral" }
        }
    ],
    system: [
        {
            type: "text",
            text: "You are a helpful research assistant with access to a document knowledge base.\n\n# Instructions\n- Always search for relevant documents before answering\n- Provide citations for your sources\n- Be objective and accurate in your responses\n- If multiple documents contain relevant information, synthesize them\n- Acknowledge when information is not available in the knowledge base",
            cache_control: { type: "ephemeral" }
        },
        {
            type: "text",
            text: "# Knowledge Base Context\n\nHere are the relevant documents for this conversation:\n\n## Document 1: Solar System Overview\nThe solar system consists of the Sun and all objects that orbit it...\n\n## Document 2: Planetary Characteristics\nEach planet has unique features. Mercury is the smallest planet...\n\n## Document 3: Mars Exploration\nMars has been a target of exploration for decades...\n\n[Additional documents...]",
            cache_control: { type: "ephemeral" }
        }
    ],
    messages: [
        {
            role: "user",
            content: "Can you search for information about Mars rovers?"
        },
        {
            role: "assistant",
            content: [
                {
                    type: "tool_use",
                    id: "tool_1",
                    name: "search_documents",
                    input: { query: "Mars rovers" }
                }
            ]
        },
        {
            role: "user",
            content: [
                {
                    type: "tool_result",
                    tool_use_id: "tool_1",
                    content: "Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)"
                }
            ]
        },
        {
            role: "assistant",
            content: [
                {
                    type: "text",
                    text: "I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document."
                }
            ]
        },
        {
            role: "user",
            content: [
                {
                    type: "text",
                    text: "Yes, please tell me about the Perseverance rover specifically.",
                    cache_control: { type: "ephemeral" }
                }
            ]
        }
    ]
});
console.log(response);
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.messages.CacheControlEphemeral;
import com.anthropic.models.messages.ContentBlockParam;
import com.anthropic.models.messages.Message;
import com.anthropic.models.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;
import com.anthropic.models.messages.Tool;
import com.anthropic.models.messages.Tool.InputSchema;
import com.anthropic.models.messages.ToolResultBlockParam;
import com.anthropic.models.messages.ToolUseBlockParam;

public class MultipleCacheBreakpointsExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // Search tool schema
        InputSchema searchSchema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "Search query"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        // Get document tool schema
        InputSchema getDocSchema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "doc_id", Map.of(
                                "type", "string",
                                "description", "Document ID"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("doc_id")))
                .build();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_20250514)
                .maxTokens(1024)
                // Tools with cache control on the last one
                .addTool(Tool.builder()
                        .name("search_documents")
                        .description("Search through the knowledge base")
                        .inputSchema(searchSchema)
                        .build())
                .addTool(Tool.builder()
                        .name("get_document")
                        .description("Retrieve a specific document by ID")
                        .inputSchema(getDocSchema)
                        .cacheControl(CacheControlEphemeral.builder().build())
                        .build())
                // System prompts with cache control on instructions and context separately
                .systemOfTextBlockParams(List.of(
                        TextBlockParam.builder()
                                .text("You are a helpful research assistant with access to a document knowledge base.\n\n# Instructions\n- Always search for relevant documents before answering\n- Provide citations for your sources\n- Be objective and accurate in your responses\n- If multiple documents contain relevant information, synthesize them\n- Acknowledge when information is not available in the knowledge base")
                                .cacheControl(CacheControlEphemeral.builder().build())
                                .build(),
                        TextBlockParam.builder()
                                .text("# Knowledge Base Context\n\nHere are the relevant documents for this conversation:\n\n## Document 1: Solar System Overview\nThe solar system consists of the Sun and all objects that orbit it...\n\n## Document 2: Planetary Characteristics\nEach planet has unique features. Mercury is the smallest planet...\n\n## Document 3: Mars Exploration\nMars has been a target of exploration for decades...\n\n[Additional documents...]")
                                .cacheControl(CacheControlEphemeral.builder().build())
                                .build()
                ))
                // Conversation history
                .addUserMessage("Can you search for information about Mars rovers?")
                .addAssistantMessageOfBlockParams(List.of(
                        ContentBlockParam.ofToolUse(ToolUseBlockParam.builder()
                                .id("tool_1")
                                .name("search_documents")
                                .input(JsonValue.from(Map.of("query", "Mars rovers")))
                                .build())
                ))
                .addUserMessageOfBlockParams(List.of(
                        ContentBlockParam.ofToolResult(ToolResultBlockParam.builder()
                                .toolUseId("tool_1")
                                .content("Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)")
                                .build())
                ))
                .addAssistantMessageOfBlockParams(List.of(
                        ContentBlockParam.ofText(TextBlockParam.builder()
                                .text("I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document.")
                                .build())
                ))
                .addUserMessageOfBlockParams(List.of(
                        ContentBlockParam.ofText(TextBlockParam.builder()
                                .text("Yes, please tell me about the Perseverance rover specifically.")
                                .cacheControl(CacheControlEphemeral.builder().build())
                                .build())
                ))
                .build();

        Message message = client.messages().create(params);
        System.out.println(message);
    }
}
```
</CodeGroup>

此综合示例演示了如何使用所有 4 个可用的缓存断点来优化提示词的不同部分：

1. **工具缓存**（缓存断点 1）：最后一个工具定义上的 `cache_control` 参数缓存所有工具定义。

2. **可重用指令缓存**（缓存断点 2）：系统提示中的静态指令被单独缓存。这些指令在请求之间很少改变。

3. **RAG 上下文缓存**（缓存断点 3）：知识库文档被独立缓存，允许您更新 RAG 文档而不会使工具或指令缓存失效。

4. **对话历史缓存**（缓存断点 4）：助手的响应用 `cache_control` 标记，以在对话进行时启用增量缓存。

此方法提供了最大的灵活性：
- 如果您仅更新最后的用户消息，所有四个缓存段都会被重用
- 如果您更新 RAG 文档但保持相同的工具和指令，前两个缓存段会被重用
- 如果您更改对话但保持相同的工具、指令和文档，前三个段会被重用
- 每个缓存断点可以根据应用程序中的更改内容独立失效

对于第一个请求：
- `input_tokens`：最后一条用户消息中的令牌数
- `cache_creation_input_tokens`：所有缓存段中的令牌数（工具 + 指令 + RAG 文档 + 对话历史）
- `cache_read_input_tokens`：0（无缓存命中）

对于后续仅有新用户消息的请求：
- `input_tokens`：仅新用户消息中的令牌数
- `cache_creation_input_tokens`：添加到对话历史的任何新令牌
- `cache_read_input_tokens`：所有先前缓存的令牌（工具 + 指令 + RAG 文档 + 先前对话）

此模式对以下情况特别强大：
- 具有大型文档上下文的 RAG 应用程序
- 使用多个工具的代理系统
- 需要维护上下文的长期运行对话
- 需要独立优化提示词不同部分的应用程序

</section>

---
## 常见问题

  <section title="我需要多个缓存断点还是在末尾放一个就足够了？">

    **在大多数情况下，在静态内容末尾放一个缓存断点就足够了。** 系统会自动检查所有先前内容块边界处的缓存命中（在您的断点之前最多 20 个块），并使用最长的匹配缓存块序列。

    您只需要多个断点，如果：
    - 您在所需缓存点之前有超过 20 个内容块
    - 您想独立缓存以不同频率更新的部分
    - 您需要对成本优化的缓存内容进行显式控制

    示例：如果您有系统指令（很少改变）和 RAG 上下文（每天改变），您可能会使用两个断点来单独缓存它们。
  
</section>

  <section title="缓存断点会增加额外成本吗？">

    不，缓存断点本身是免费的。您只需支付：
    - 将内容写入缓存（比基础输入令牌多 25%，用于 5 分钟 TTL）
    - 从缓存读取（基础输入令牌价格的 10%）
    - 未缓存内容的常规输入令牌

    断点的数量不会影响定价 - 只有缓存和读取的内容量才重要。
  
</section>

  <section title="我如何从使用字段计算总输入令牌？">

    使用响应包括三个单独的输入令牌字段，它们一起代表您的总输入：

    ```
    total_input_tokens = cache_read_input_tokens + cache_creation_input_tokens + input_tokens
    ```

    - `cache_read_input_tokens`：从缓存检索的令牌（缓存断点之前被缓存的所有内容）
    - `cache_creation_input_tokens`：被写入缓存的新令牌（在缓存断点处）
    - `input_tokens`：**最后一个缓存断点之后**未被缓存的令牌

    **重要：** `input_tokens` 不代表所有输入令牌 - 仅代表最后一个缓存断点之后的部分。如果您有缓存内容，`input_tokens` 通常会远小于您的总输入。

    **示例：** 使用 200K 令牌文档缓存和 50 令牌用户问题：
    - `cache_read_input_tokens`：200,000
    - `cache_creation_input_tokens`：0
    - `input_tokens`：50
    - **总计**：200,050 令牌

    此分解对于理解您的成本和速率限制使用都至关重要。有关更多详细信息，请参阅[跟踪缓存性能](#tracking-cache-performance)。
  
</section>

  <section title="缓存生命周期是多长？">

    缓存的默认最小生命周期（TTL）是 5 分钟。每次使用缓存内容时，此生命周期都会刷新。

    如果您发现 5 分钟太短，Anthropic 还提供了[1 小时缓存 TTL](#1-hour-cache-duration)。
  
</section>

  <section title="我可以使用多少个缓存断点？">

    您可以在提示词中定义最多 4 个缓存断点（使用 `cache_control` 参数）。
  
</section>

  <section title="提示词缓存是否适用于所有模型？">

    不，提示词缓存目前仅适用于 Claude Opus 4.5、Claude Opus 4.1、Claude Opus 4、Claude Sonnet 4.5、Claude Sonnet 4、Claude Sonnet 3.7、Claude Haiku 4.5、Claude Haiku 3.5、Claude Haiku 3 和 Claude Opus 3（[已弃用](/docs/zh-CN/about-claude/model-deprecations)）。
  
</section>

  <section title="提示词缓存如何与扩展思考一起工作？">

    当思考参数改变时，缓存的系统提示和工具将被重用。但是，思考改变（启用/禁用或预算改变）将使具有消息内容的先前缓存提示前缀失效。

    有关缓存失效的更多详细信息，请参阅[什么使缓存失效](#what-invalidates-the-cache)。

    有关扩展思考的更多信息，包括其与工具使用和提示词缓存的交互，请参阅[扩展思考文档](/docs/zh-CN/build-with-claude/extended-thinking#extended-thinking-and-prompt-caching)。
  
</section>

  <section title="我如何启用提示词缓存？">

    要启用提示词缓存，请在 API 请求中包含至少一个 `cache_control` 断点。
  
</section>

  <section title="我可以将提示词缓存与其他 API 功能一起使用吗？">

    是的，提示词缓存可以与其他 API 功能（如工具使用和视觉功能）一起使用。但是，改变提示词中是否有图像或修改工具使用设置将破坏缓存。

    有关缓存失效的更多详细信息，请参阅[什么使缓存失效](#what-invalidates-the-cache)。
  
</section>

  <section title="提示词缓存如何影响定价？">

    提示词缓存引入了新的定价结构，其中缓存写入的成本比基础输入令牌多 25%，而缓存命中的成本仅为基础输入令牌价格的 10%。
  
</section>

  <section title="我可以手动清除缓存吗？">

    目前，没有办法手动清除缓存。缓存前缀在最少 5 分钟不活动后自动过期。
  
</section>

  <section title="我如何跟踪缓存策略的有效性？">

    您可以使用 API 响应中的 `cache_creation_input_tokens` 和 `cache_read_input_tokens` 字段监控缓存性能。
  
</section>

  <section title="什么会破坏缓存？">

    有关缓存失效的更多详细信息，请参阅[什么使缓存失效](#what-invalidates-the-cache)，包括需要创建新缓存条目的更改列表。
  
</section>

  <section title="提示词缓存如何处理隐私和数据分离？">

提示词缓存采用强大的隐私和数据分离措施进行设计：

1. 缓存键是使用提示词的加密哈希生成的，直到缓存控制点。这意味着只有具有相同提示词的请求才能访问特定缓存。

2. 缓存是特定于组织的。同一组织内的用户如果使用相同的提示词，可以访问相同的缓存，但缓存不会在不同组织之间共享，即使提示词相同。

3. 缓存机制旨在维护每个唯一对话或上下文的完整性和隐私。

4. 在提示词中的任何地方使用 `cache_control` 是安全的。为了成本效率，最好将高度可变的部分（例如，用户的任意输入）排除在缓存之外。

这些措施确保提示词缓存在提供性能优势的同时维护数据隐私和安全。
  
</section>
  <section title="我可以将提示词缓存与批处理 API 一起使用吗？">

    是的，可以将提示词缓存与您的[批处理 API](/docs/zh-CN/build-with-claude/batch-processing) 请求一起使用。但是，由于异步批处理请求可以并发处理且顺序任意，缓存命中是尽力而为的基础。

    [1 小时缓存](#1-hour-cache-duration)可以帮助改进您的缓存命中。最具成本效益的使用方式如下：
    - 收集一组具有共享前缀的消息请求。
    - 发送仅包含此共享前缀和 1 小时缓存块的单个请求的批处理请求。这将被写入 1 小时缓存。
    - 一旦完成，提交其余请求。您必须监控作业以了解何时完成。

    这通常比使用 5 分钟缓存更好，因为批处理请求通常需要 5 分钟到 1 小时才能完成。我们正在考虑改进这些缓存命中率并使此过程更加直接的方法。
  
</section>
  <section title="为什么我在 Python 中看到错误 `AttributeError: 'Beta' object has no attribute 'prompt_caching'`？">

  此错误通常在您升级 SDK 或使用过时代码示例时出现。提示词缓存现在已正式推出，因此您不再需要 beta 前缀。而不是：
    <CodeGroup>
      ```python Python
      python client.beta.prompt_caching.messages.create(...)
      ```
    </CodeGroup>
    只需使用：
    <CodeGroup>
      ```python Python
      python client.messages.create(...)
      ```
    </CodeGroup>
  
</section>
  <section title="为什么我看到 'TypeError: Cannot read properties of undefined (reading 'messages')'？">

  此错误通常在您升级 SDK 或使用过时代码示例时出现。提示词缓存现在已正式推出，因此您不再需要 beta 前缀。而不是：

      ```typescript TypeScript
      client.beta.promptCaching.messages.create(...)
      ```

      只需使用：

      ```typescript
      client.messages.create(...)
      ```
  
</section>

# 上下文编辑

在对话增长时自动管理对话上下文，使用上下文编辑功能。

---

## 概述

上下文编辑允许您在对话增长时自动管理对话上下文，帮助您优化成本并保持在上下文窗口限制内。您可以使用服务器端 API 策略、客户端 SDK 功能，或两者结合使用。

| 方法 | 运行位置 | 策略 | 工作原理 |
|----------|---------------|------------|--------------|
| **服务器端** | API | 工具结果清除 (`clear_tool_uses_20250919`)<br/>思考块清除 (`clear_thinking_20251015`) | 在提示到达 Claude 之前应用。从对话历史中清除特定内容。每个策略可以独立配置。 |
| **客户端** | SDK | 压缩 | 在使用 [`tool_runner`](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#tool-runner-beta) 时，可在 [Python 和 TypeScript SDK](/docs/zh-CN/api/client-sdks) 中使用。生成摘要并替换完整对话历史。请参阅下面的 [压缩](#client-side-compaction-sdk)。 |

## 服务器端策略

<Note>
上下文编辑目前处于测试阶段，支持工具结果清除和思考块清除。要启用它，请在 API 请求中使用测试版标头 `context-management-2025-06-27`。

请通过我们的 [反馈表单](https://forms.gle/YXC2EKGMhjN1c4L88) 分享您对此功能的反馈。
</Note>

### 工具结果清除

`clear_tool_uses_20250919` 策略在对话上下文增长超过您配置的阈值时清除工具结果。激活后，API 会按时间顺序自动清除最旧的工具结果，用占位符文本替换它们，让 Claude 知道工具结果已被移除。默认情况下，仅清除工具结果。您可以通过将 `clear_tool_inputs` 设置为 true，选择清除工具结果和工具调用（工具使用参数）。

### 思考块清除

`clear_thinking_20251015` 策略在启用扩展思考时管理对话中的 `thinking` 块。此策略自动清除来自先前轮次的较旧思考块。

<Tip>
**默认行为**：启用扩展思考但未配置 `clear_thinking_20251015` 策略时，API 会自动仅保留最后一个助手轮次的思考块（等同于 `keep: {type: "thinking_turns", value: 1}`）。

要最大化缓存命中，通过设置 `keep: "all"` 来保留所有思考块。
</Tip>

<Note>
助手对话轮次可能包括多个内容块（例如使用工具时）和多个思考块（例如使用 [交错思考](/docs/zh-CN/build-with-claude/extended-thinking#interleaved-thinking)）。
</Note>

<Tip>
**上下文编辑在服务器端进行**

上下文编辑在提示到达 Claude 之前在 **服务器端** 应用。您的客户端应用程序维护完整的、未修改的对话历史——您无需将客户端状态与编辑后的版本同步。继续像往常一样在本地管理您的完整对话历史。
</Tip>

<Tip>
**上下文编辑和提示缓存**

上下文编辑与 [提示缓存](/docs/zh-CN/build-with-claude/prompt-caching) 的交互因策略而异：

- **工具结果清除**：清除内容时使缓存的提示前缀失效。为了解决这个问题，我们建议清除足够的令牌以使缓存失效值得。使用 `clear_at_least` 参数确保每次清除时至少清除最少数量的令牌。每次清除内容时您都会产生缓存写入成本，但后续请求可以重用新缓存的前缀。

- **思考块清除**：当思考块在上下文中被 **保留**（未清除）时，提示缓存被保留，启用缓存命中并减少输入令牌成本。当思考块被 **清除** 时，缓存在清除发生的位置失效。根据您是否想优先考虑缓存性能或上下文窗口可用性来配置 `keep` 参数。
</Tip>

## 支持的模型

上下文编辑可用于：

- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)
- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)

## 工具结果清除使用

启用工具结果清除的最简单方法是仅指定策略类型，因为所有其他 [配置选项](#configuration-options-for-tool-result-clearing) 将使用其默认值：

<CodeGroup>

```bash cURL
curl https://api.anthropic.com/v1/messages \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "anthropic-version: 2023-06-01" \
    --header "content-type: application/json" \
    --header "anthropic-beta: context-management-2025-06-27" \
    --data '{
        "model": "claude-sonnet-4-5",
        "max_tokens": 4096,
        "messages": [
            {
                "role": "user",
                "content": "Search for recent developments in AI"
            }
        ],
        "tools": [
            {
                "type": "web_search_20250305",
                "name": "web_search"
            }
        ],
        "context_management": {
            "edits": [
                {"type": "clear_tool_uses_20250919"}
            ]
        }
    }'
```

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=4096,
    messages=[
        {
            "role": "user",
            "content": "Search for recent developments in AI"
        }
    ],
    tools=[
        {
            "type": "web_search_20250305",
            "name": "web_search"
        }
    ],
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {"type": "clear_tool_uses_20250919"}
        ]
    }
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 4096,
  messages: [
    {
      role: "user",
      content: "Search for recent developments in AI"
    }
  ],
  tools: [
    {
      type: "web_search_20250305",
      name: "web_search"
    }
  ],
  context_management: {
    edits: [
      { type: "clear_tool_uses_20250919" }
    ]
  },
  betas: ["context-management-2025-06-27"]
});
```

</CodeGroup>

### 高级配置

您可以使用其他参数自定义工具结果清除行为：

<CodeGroup>

```bash cURL
curl https://api.anthropic.com/v1/messages \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "anthropic-version: 2023-06-01" \
    --header "content-type: application/json" \
    --header "anthropic-beta: context-management-2025-06-27" \
    --data '{
        "model": "claude-sonnet-4-5",
        "max_tokens": 4096,
        "messages": [
            {
                "role": "user",
                "content": "Create a simple command line calculator app using Python"
            }
        ],
        "tools": [
            {
                "type": "text_editor_20250728",
                "name": "str_replace_based_edit_tool",
                "max_characters": 10000
            },
            {
                "type": "web_search_20250305",
                "name": "web_search",
                "max_uses": 3
            }
        ],
        "context_management": {
            "edits": [
                {
                    "type": "clear_tool_uses_20250919",
                    "trigger": {
                        "type": "input_tokens",
                        "value": 30000
                    },
                    "keep": {
                        "type": "tool_uses",
                        "value": 3
                    },
                    "clear_at_least": {
                        "type": "input_tokens",
                        "value": 5000
                    },
                    "exclude_tools": ["web_search"]
                }
            ]
        }
    }'
```

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=4096,
    messages=[
        {
            "role": "user",
            "content": "Create a simple command line calculator app using Python"
        }
    ],
    tools=[
        {
            "type": "text_editor_20250728",
            "name": "str_replace_based_edit_tool",
            "max_characters": 10000
        },
        {
            "type": "web_search_20250305",
            "name": "web_search",
            "max_uses": 3
        }
    ],
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {
                "type": "clear_tool_uses_20250919",
                # 当超过阈值时触发清除
                "trigger": {
                    "type": "input_tokens",
                    "value": 30000
                },
                # 清除后要保留的工具使用次数
                "keep": {
                    "type": "tool_uses",
                    "value": 3
                },
                # 可选：至少清除这么多令牌
                "clear_at_least": {
                    "type": "input_tokens",
                    "value": 5000
                },
                # 排除这些工具不被清除
                "exclude_tools": ["web_search"]
            }
        ]
    }
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 4096,
  messages: [
    {
      role: "user",
      content: "Create a simple command line calculator app using Python"
    }
  ],
  tools: [
    {
      type: "text_editor_20250728",
      name: "str_replace_based_edit_tool",
      max_characters: 10000
    },
    {
      type: "web_search_20250305",
      name: "web_search",
      max_uses: 3
    }
  ],
  betas: ["context-management-2025-06-27"],
  context_management: {
    edits: [
      {
        type: "clear_tool_uses_20250919",
        // 当超过阈值时触发清除
        trigger: {
          type: "input_tokens",
          value: 30000
        },
        // 清除后要保留的工具使用次数
        keep: {
          type: "tool_uses",
          value: 3
        },
        // 可选：至少清除这么多令牌
        clear_at_least: {
          type: "input_tokens",
          value: 5000
        },
        // 排除这些工具不被清除
        exclude_tools: ["web_search"]
      }
    ]
  }
});
```

</CodeGroup>

## 概述

上下文编辑允许您在对话增长时自动管理对话上下文，帮助您优化成本并保持在上下文窗口限制内。您可以使用服务器端 API 策略、客户端 SDK 功能，或两者结合使用。

| 方法 | 运行位置 | 策略 | 工作原理 |
|----------|---------------|------------|--------------|
| **服务器端** | API | 工具结果清除 (`clear_tool_uses_20250919`)<br/>思考块清除 (`clear_thinking_20251015`) | 在提示到达 Claude 之前应用。从对话历史中清除特定内容。每个策略可以独立配置。 |
| **客户端** | SDK | 压缩 | 在使用 [`tool_runner`](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#tool-runner-beta) 时，在 [Python 和 TypeScript SDK](/docs/zh-CN/api/client-sdks) 中可用。生成摘要并替换完整的对话历史。请参阅下面的 [压缩](#client-side-compaction-sdk)。 |

## 服务器端策略

<Note>
上下文编辑目前处于测试阶段，支持工具结果清除和思考块清除。要启用它，请在 API 请求中使用测试版标头 `context-management-2025-06-27`。

请通过我们的 [反馈表单](https://forms.gle/YXC2EKGMhjN1c4L88) 分享您对此功能的反馈。
</Note>

### 工具结果清除

`clear_tool_uses_20250919` 策略在对话上下文增长超过您配置的阈值时清除工具结果。激活后，API 会按时间顺序自动清除最旧的工具结果，用占位符文本替换它们，以让 Claude 知道工具结果已被移除。默认情况下，仅清除工具结果。您可以通过将 `clear_tool_inputs` 设置为 true，选择同时清除工具结果和工具调用（工具使用参数）。

### 思考块清除

`clear_thinking_20251015` 策略在启用扩展思考时管理对话中的 `thinking` 块。此策略自动清除来自先前轮次的较旧思考块。

<Tip>
**默认行为**：启用扩展思考而不配置 `clear_thinking_20251015` 策略时，API 会自动仅保留最后一个助手轮次的思考块（等同于 `keep: {type: "thinking_turns", value: 1}`）。

要最大化缓存命中，通过设置 `keep: "all"` 保留所有思考块。
</Tip>

<Note>
助手对话轮次可能包括多个内容块（例如使用工具时）和多个思考块（例如使用 [交错思考](/docs/zh-CN/build-with-claude/extended-thinking#interleaved-thinking)）。
</Note>

<Tip>
**上下文编辑发生在服务器端**

上下文编辑在提示到达 Claude 之前应用于**服务器端**。您的客户端应用程序维护完整的、未修改的对话历史——您无需将客户端状态与编辑后的版本同步。继续像往常一样在本地管理您的完整对话历史。
</Tip>

<Tip>
**上下文编辑和提示缓存**

上下文编辑与 [提示缓存](/docs/zh-CN/build-with-claude/prompt-caching) 的交互因策略而异：

- **工具结果清除**：清除内容时使缓存的提示前缀失效。为了解决这个问题，我们建议清除足够的令牌以使缓存失效值得。使用 `clear_at_least` 参数确保每次清除时至少清除最少数量的令牌。每次清除内容时，您都会产生缓存写入成本，但后续请求可以重用新缓存的前缀。

- **思考块清除**：当思考块**保留**在上下文中（未清除）时，提示缓存被保留，启用缓存命中并减少输入令牌成本。当思考块**被清除**时，缓存在清除发生的位置失效。根据您是否想优先考虑缓存性能或上下文窗口可用性来配置 `keep` 参数。
</Tip>

## 支持的模型

上下文编辑可用于：

- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)
- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)

## 工具结果清除用法

启用工具结果清除的最简单方法是仅指定策略类型，因为所有其他 [配置选项](#configuration-options-for-tool-result-clearing) 将使用其默认值：

<CodeGroup>

```bash cURL
curl https://api.anthropic.com/v1/messages \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "anthropic-version: 2023-06-01" \
    --header "content-type: application/json" \
    --header "anthropic-beta: context-management-2025-06-27" \
    --data '{
        "model": "claude-sonnet-4-5",
        "max_tokens": 4096,
        "messages": [
            {
                "role": "user",
                "content": "Search for recent developments in AI"
            }
        ],
        "tools": [
            {
                "type": "web_search_20250305",
                "name": "web_search"
            }
        ],
        "context_management": {
            "edits": [
                {"type": "clear_tool_uses_20250919"}
            ]
        }
    }'
```

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=4096,
    messages=[
        {
            "role": "user",
            "content": "Search for recent developments in AI"
        }
    ],
    tools=[
        {
            "type": "web_search_20250305",
            "name": "web_search"
        }
    ],
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {"type": "clear_tool_uses_20250919"}
        ]
    }
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 4096,
  messages: [
    {
      role: "user",
      content: "Search for recent developments in AI"
    }
  ],
  tools: [
    {
      type: "web_search_20250305",
      name: "web_search"
    }
  ],
  context_management: {
    edits: [
      { type: "clear_tool_uses_20250919" }
    ]
  },
  betas: ["context-management-2025-06-27"]
});
```

</CodeGroup>

### 高级配置

您可以使用其他参数自定义工具结果清除行为：

<CodeGroup>

```bash cURL
curl https://api.anthropic.com/v1/messages \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "anthropic-version: 2023-06-01" \
    --header "content-type: application/json" \
    --header "anthropic-beta: context-management-2025-06-27" \
    --data '{
        "model": "claude-sonnet-4-5",
        "max_tokens": 4096,
        "messages": [
            {
                "role": "user",
                "content": "Create a simple command line calculator app using Python"
            }
        ],
        "tools": [
            {
                "type": "text_editor_20250728",
                "name": "str_replace_based_edit_tool",
                "max_characters": 10000
            },
            {
                "type": "web_search_20250305",
                "name": "web_search",
                "max_uses": 3
            }
        ],
        "context_management": {
            "edits": [
                {
                    "type": "clear_tool_uses_20250919",
                    "trigger": {
                        "type": "input_tokens",
                        "value": 30000
                    },
                    "keep": {
                        "type": "tool_uses",
                        "value": 3
                    },
                    "clear_at_least": {
                        "type": "input_tokens",
                        "value": 5000
                    },
                    "exclude_tools": ["web_search"]
                }
            ]
        }
    }'
```

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=4096,
    messages=[
        {
            "role": "user",
            "content": "Create a simple command line calculator app using Python"
        }
    ],
    tools=[
        {
            "type": "text_editor_20250728",
            "name": "str_replace_based_edit_tool",
            "max_characters": 10000
        },
        {
            "type": "web_search_20250305",
            "name": "web_search",
            "max_uses": 3
        }
    ],
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {
                "type": "clear_tool_uses_20250919",
                # Trigger clearing when threshold is exceeded
                "trigger": {
                    "type": "input_tokens",
                    "value": 30000
                },
                # Number of tool uses to keep after clearing
                "keep": {
                    "type": "tool_uses",
                    "value": 3
                },
                # Optional: Clear at least this many tokens
                "clear_at_least": {
                    "type": "input_tokens",
                    "value": 5000
                },
                # Exclude these tools from being cleared
                "exclude_tools": ["web_search"]
            }
        ]
    }
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 4096,
  messages: [
    {
      role: "user",
      content: "Create a simple command line calculator app using Python"
    }
  ],
  tools: [
    {
      type: "text_editor_20250728",
      name: "str_replace_based_edit_tool",
      max_characters: 10000
    },
    {
      type: "web_search_20250305",
      name: "web_search",
      max_uses: 3
    }
  ],
  betas: ["context-management-2025-06-27"],
  context_management: {
    edits: [
      {
        type: "clear_tool_uses_20250919",
        // Trigger clearing when threshold is exceeded
        trigger: {
          type: "input_tokens",
          value: 30000
        },
        // Number of tool uses to keep after clearing
        keep: {
          type: "tool_uses",
          value: 3
        },
        // Optional: Clear at least this many tokens
        clear_at_least: {
          type: "input_tokens",
          value: 5000
        },
        // Exclude these tools from being cleared
        exclude_tools: ["web_search"]
      }
    ]
  }
});
```

</CodeGroup>

## 思考块清除用法

启用思考块清除以在启用扩展思考时有效管理上下文和提示缓存：

<CodeGroup>

```bash cURL
curl https://api.anthropic.com/v1/messages \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "anthropic-version: 2023-06-01" \
    --header "content-type: application/json" \
    --header "anthropic-beta: context-management-2025-06-27" \
    --data '{
        "model": "claude-sonnet-4-5-20250929",
        "max_tokens": 1024,
        "messages": [...],
        "thinking": {
            "type": "enabled",
            "budget_tokens": 10000
        },
        "context_management": {
            "edits": [
                {
                    "type": "clear_thinking_20251015",
                    "keep": {
                        "type": "thinking_turns",
                        "value": 2
                    }
                }
            ]
        }
    }'
```

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    messages=[...],
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {
                "type": "clear_thinking_20251015",
                "keep": {
                    "type": "thinking_turns",
                    "value": 2
                }
            }
        ]
    }
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 1024,
  messages: [...],
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  betas: ["context-management-2025-06-27"],
  context_management: {
    edits: [
      {
        type: "clear_thinking_20251015",
        keep: {
          type: "thinking_turns",
          value: 2
        }
      }
    ]
  }
});
```

</CodeGroup>

### 思考块清除的配置选项

`clear_thinking_20251015` 策略支持以下配置：

| 配置选项 | 默认值 | 描述 |
|---------------------|---------|-------------|
| `keep` | `{type: "thinking_turns", value: 1}` | 定义要保留的最近助手轮次（包含思考块）的数量。使用 `{type: "thinking_turns", value: N}`，其中 N 必须 > 0 以保留最后 N 个轮次，或使用 `"all"` 保留所有思考块。 |

**示例配置：**

```json
// 保留最后 3 个助手轮次的思考块
{
  "type": "clear_thinking_20251015",
  "keep": {
    "type": "thinking_turns",
    "value": 3
  }
}

// 保留所有思考块（最大化缓存命中）
{
  "type": "clear_thinking_20251015",
  "keep": "all"
}
```

### 组合策略

您可以同时使用思考块清除和工具结果清除：

<Note>
使用多个策略时，`clear_thinking_20251015` 策略必须在 `edits` 数组中首先列出。
</Note>

<CodeGroup>

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    messages=[...],
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[...],
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {
                "type": "clear_thinking_20251015",
                "keep": {
                    "type": "thinking_turns",
                    "value": 2
                }
            },
            {
                "type": "clear_tool_uses_20250919",
                "trigger": {
                    "type": "input_tokens",
                    "value": 50000
                },
                "keep": {
                    "type": "tool_uses",
                    "value": 5
                }
            }
        ]
    }
)
```

```typescript TypeScript
const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 1024,
  messages: [...],
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [...],
  betas: ["context-management-2025-06-27"],
  context_management: {
    edits: [
      {
        type: "clear_thinking_20251015",
        keep: {
          type: "thinking_turns",
          value: 2
        }
      },
      {
        type: "clear_tool_uses_20250919",
        trigger: {
          type: "input_tokens",
          value: 50000
        },
        keep: {
          type: "tool_uses",
          value: 5
        }
      }
    ]
  }
});
```

</CodeGroup>

## 工具结果清除的配置选项

| 配置选项 | 默认值 | 描述 |
|---------------------|---------|-------------|
| `trigger` | 100,000 输入令牌 | 定义上下文编辑策略何时激活。一旦提示超过此阈值，清除将开始。您可以在 `input_tokens` 或 `tool_uses` 中指定此值。 |
| `keep` | 3 个工具使用 | 定义清除发生后要保留的最近工具使用/结果对的数量。API 首先移除最旧的工具交互，保留最新的工具交互。 |
| `clear_at_least` | 无 | 确保每次策略激活时至少清除最少数量的令牌。如果 API 无法清除至少指定的数量，则不会应用该策略。这有助于确定上下文清除是否值得破坏您的提示缓存。 |
| `exclude_tools` | 无 | 工具名称列表，其工具使用和结果永远不应被清除。用于保留重要上下文。 |
| `clear_tool_inputs` | `false` | 控制是否将工具调用参数与工具结果一起清除。默认情况下，仅清除工具结果，同时保持 Claude 的原始工具调用可见。 |

## 上下文编辑响应

您可以使用 `context_management` 响应字段查看哪些上下文编辑已应用于您的请求，以及有关清除的内容和输入令牌的有用统计信息。

```json 响应
{
    "id": "msg_013Zva2CMHLNnXjNJJKqJ2EF",
    "type": "message",
    "role": "assistant",
    "content": [...],
    "usage": {...},
    "context_management": {
        "applied_edits": [
            // 使用 `clear_thinking_20251015` 时
            {
                "type": "clear_thinking_20251015",
                "cleared_thinking_turns": 3,
                "cleared_input_tokens": 15000
            },
            // 使用 `clear_tool_uses_20250919` 时
            {
                "type": "clear_tool_uses_20250919",
                "cleared_tool_uses": 8,
                "cleared_input_tokens": 50000
            }
        ]
    }
}
```

对于流式响应，上下文编辑将包含在最终的 `message_delta` 事件中：

```json 流式响应
{
    "type": "message_delta",
    "delta": {
        "stop_reason": "end_turn",
        "stop_sequence": null
    },
    "usage": {
        "output_tokens": 1024
    },
    "context_management": {
        "applied_edits": [...]
    }
}
```

### 思考块清除的配置选项

`clear_thinking_20251015` 策略支持以下配置：

| 配置选项 | 默认值 | 描述 |
|---------------------|---------|-------------|
| `keep` | `{type: "thinking_turns", value: 1}` | 定义要保留多少个最近的带有思考块的助手轮次。使用 `{type: "thinking_turns", value: N}`，其中 N 必须 > 0 以保留最后 N 个轮次，或使用 `"all"` 保留所有思考块。 |

**配置示例：**

```json
// 保留最后 3 个助手轮次的思考块
{
  "type": "clear_thinking_20251015",
  "keep": {
    "type": "thinking_turns",
    "value": 3
  }
}

// 保留所有思考块（最大化缓存命中）
{
  "type": "clear_thinking_20251015",
  "keep": "all"
}
```

### 组合策略

您可以同时使用思考块清除和工具结果清除：

<Note>
使用多个策略时，`clear_thinking_20251015` 策略必须在 `edits` 数组中首先列出。
</Note>

<CodeGroup>

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    messages=[...],
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[...],
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {
                "type": "clear_thinking_20251015",
                "keep": {
                    "type": "thinking_turns",
                    "value": 2
                }
            },
            {
                "type": "clear_tool_uses_20250919",
                "trigger": {
                    "type": "input_tokens",
                    "value": 50000
                },
                "keep": {
                    "type": "tool_uses",
                    "value": 5
                }
            }
        ]
    }
)
```

```typescript TypeScript
const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 1024,
  messages: [...],
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [...],
  betas: ["context-management-2025-06-27"],
  context_management: {
    edits: [
      {
        type: "clear_thinking_20251015",
        keep: {
          type: "thinking_turns",
          value: 2
        }
      },
      {
        type: "clear_tool_uses_20250919",
        trigger: {
          type: "input_tokens",
          value: 50000
        },
        keep: {
          type: "tool_uses",
          value: 5
        }
      }
    ]
  }
});
```

</CodeGroup>

## 工具结果清除的配置选项

| 配置选项 | 默认值 | 描述 |
|---------------------|---------|-------------|
| `trigger` | 100,000 输入令牌 | 定义上下文编辑策略何时激活。一旦提示超过此阈值，清除将开始。您可以在 `input_tokens` 或 `tool_uses` 中指定此值。 |
| `keep` | 3 个工具使用 | 定义清除发生后要保留多少个最近的工具使用/结果对。API 首先删除最旧的工具交互，保留最新的交互。 |
| `clear_at_least` | 无 | 确保每次策略激活时至少清除一定数量的令牌。如果 API 无法清除至少指定的数量，则不会应用该策略。这有助于确定上下文清除是否值得破坏您的提示缓存。 |
| `exclude_tools` | 无 | 工具名称列表，其工具使用和结果永远不应被清除。对于保留重要上下文很有用。 |
| `clear_tool_inputs` | `false` | 控制是否将工具调用参数与工具结果一起清除。默认情况下，仅清除工具结果，同时保持 Claude 的原始工具调用可见。 |

## 上下文编辑响应

您可以使用 `context_management` 响应字段查看哪些上下文编辑已应用于您的请求，以及有关已清除内容和输入令牌的有用统计信息。

```json Response
{
    "id": "msg_013Zva2CMHLNnXjNJJKqJ2EF",
    "type": "message",
    "role": "assistant",
    "content": [...],
    "usage": {...},
    "context_management": {
        "applied_edits": [
            // 使用 `clear_thinking_20251015` 时
            {
                "type": "clear_thinking_20251015",
                "cleared_thinking_turns": 3,
                "cleared_input_tokens": 15000
            },
            // 使用 `clear_tool_uses_20250919` 时
            {
                "type": "clear_tool_uses_20250919",
                "cleared_tool_uses": 8,
                "cleared_input_tokens": 50000
            }
        ]
    }
}
```

对于流式响应，上下文编辑将包含在最终的 `message_delta` 事件中：

```json Streaming Response
{
    "type": "message_delta",
    "delta": {
        "stop_reason": "end_turn",
        "stop_sequence": null
    },
    "usage": {
        "output_tokens": 1024
    },
    "context_management": {
        "applied_edits": [...]
    }
}
```

## 令牌计数

[令牌计数](/docs/zh-CN/build-with-claude/token-counting)端点支持上下文管理，允许您预览应用上下文编辑后提示将使用多少令牌。

<CodeGroup>

```bash cURL
curl https://api.anthropic.com/v1/messages/count_tokens \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "anthropic-version: 2023-06-01" \
    --header "content-type: application/json" \
    --header "anthropic-beta: context-management-2025-06-27" \
    --data '{
        "model": "claude-sonnet-4-5",
        "messages": [
            {
                "role": "user",
                "content": "Continue our conversation..."
            }
        ],
        "tools": [...],
        "context_management": {
            "edits": [
                {
                    "type": "clear_tool_uses_20250919",
                    "trigger": {
                        "type": "input_tokens",
                        "value": 30000
                    },
                    "keep": {
                        "type": "tool_uses",
                        "value": 5
                    }
                }
            ]
        }
    }'
```

```python Python
response = client.beta.messages.count_tokens(
    model="claude-sonnet-4-5",
    messages=[
        {
            "role": "user",
            "content": "Continue our conversation..."
        }
    ],
    tools=[...],  # Your tool definitions
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {
                "type": "clear_tool_uses_20250919",
                "trigger": {
                    "type": "input_tokens",
                    "value": 30000
                },
                "keep": {
                    "type": "tool_uses",
                    "value": 5
                }
            }
        ]
    }
)

print(f"Original tokens: {response.context_management['original_input_tokens']}")
print(f"After clearing: {response.input_tokens}")
print(f"Savings: {response.context_management['original_input_tokens'] - response.input_tokens} tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const response = await anthropic.beta.messages.countTokens({
  model: "claude-sonnet-4-5",
  messages: [
    {
      role: "user",
      content: "Continue our conversation..."
    }
  ],
  tools: [...],  // Your tool definitions
  betas: ["context-management-2025-06-27"],
  context_management: {
    edits: [
      {
        type: "clear_tool_uses_20250919",
        trigger: {
          type: "input_tokens",
          value: 30000
        },
        keep: {
          type: "tool_uses",
          value: 5
        }
      }
    ]
  }
});

console.log(`Original tokens: ${response.context_management?.original_input_tokens}`);
console.log(`After clearing: ${response.input_tokens}`);
console.log(`Savings: ${(response.context_management?.original_input_tokens || 0) - response.input_tokens} tokens`);
```
</CodeGroup>

```json Response
{
    "input_tokens": 25000,
    "context_management": {
        "original_input_tokens": 70000
    }
}
```

响应显示应用上下文管理后的最终令牌计数（`input_tokens`）和任何清除发生前的原始令牌计数（`original_input_tokens`）。

## 与内存工具一起使用

上下文编辑可以与[内存工具](/docs/zh-CN/agents-and-tools/tool-use/memory-tool)结合使用。当您的对话上下文接近配置的清除阈值时，Claude 会收到自动警告以保留重要信息。这使 Claude 能够在工具结果从对话历史中清除之前将其保存到内存文件中。

这种组合允许您：

- **保留重要上下文**：Claude 可以在清除工具结果之前将工具结果中的重要信息写入内存文件
- **维护长期运行的工作流**：通过将信息卸载到持久存储来启用否则会超过上下文限制的代理工作流
- **按需访问信息**：Claude 可以从内存文件中查找之前清除的信息，而不是将所有内容保留在活动上下文窗口中

例如，在 Claude 执行许多操作的文件编辑工作流中，Claude 可以在上下文增长时将已完成的更改总结到内存文件中。当工具结果被清除时，Claude 通过其内存系统保留对该信息的访问权限，并可以继续有效地工作。

要同时使用这两个功能，请在您的 API 请求中启用它们：

<CodeGroup>

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=4096,
    messages=[...],
    tools=[
        {
            "type": "memory_20250818",
            "name": "memory"
        },
        # Your other tools
    ],
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {"type": "clear_tool_uses_20250919"}
        ]
    }
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 4096,
  messages: [...],
  tools: [
    {
      type: "memory_20250818",
      name: "memory"
    },
    // Your other tools
  ],
  betas: ["context-management-2025-06-27"],
  context_management: {
    edits: [
      { type: "clear_tool_uses_20250919" }
    ]
  }
});
```

</CodeGroup>

## 客户端压缩 (SDK)

<Note>
压缩在[Python 和 TypeScript SDK](/docs/zh-CN/api/client-sdks)中可用，当使用[`tool_runner` 方法](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#tool-runner-beta)时。
</Note>

压缩是一个 SDK 功能，当令牌使用量增长过大时，通过生成摘要来自动管理对话上下文。与清除内容的服务器端上下文编辑策略不同，压缩指示 Claude 总结对话历史，然后用该摘要替换完整历史。这允许 Claude 继续处理否则会超过[上下文窗口](/docs/zh-CN/build-with-claude/context-windows)的长期运行任务。

### 压缩如何工作

启用压缩时，SDK 在每个模型响应后监控令牌使用情况：

1. **阈值检查**：SDK 计算总令牌数为 `input_tokens + cache_creation_input_tokens + cache_read_input_tokens + output_tokens`
2. **摘要生成**：当超过阈值时，摘要提示被注入为用户轮次，Claude 生成包装在 `<summary></summary>` 标签中的结构化摘要
3. **上下文替换**：SDK 提取摘要并用其替换整个消息历史
4. **继续**：对话从摘要恢复，Claude 从中断处继续

### 使用压缩

将 `compaction_control` 添加到您的 `tool_runner` 调用中：

<CodeGroup>

```python Python
import anthropic

client = anthropic.Anthropic()

runner = client.beta.messages.tool_runner(
    model="claude-sonnet-4-5",
    max_tokens=4096,
    tools=[...],
    messages=[
        {
            "role": "user",
            "content": "Analyze all the files in this directory and write a summary report."
        }
    ],
    compaction_control={
        "enabled": True,
        "context_token_threshold": 100000
    }
)

for message in runner:
    print(f"Tokens used: {message.usage.input_tokens}")

final = runner.until_done()
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const runner = client.beta.messages.toolRunner({
    model: 'claude-sonnet-4-5',
    max_tokens: 4096,
    tools: [...],
    messages: [
        {
            role: 'user',
            content: 'Analyze all the files in this directory and write a summary report.'
        }
    ],
    compactionControl: {
        enabled: true,
        contextTokenThreshold: 100000
    }
});

for await (const message of runner) {
    console.log('Tokens used:', message.usage.input_tokens);
}

const finalMessage = await runner.runUntilDone();
```

</CodeGroup>

#### 压缩期间发生的情况

随着对话的增长，消息历史积累：

**压缩前（接近 100k 令牌）：**
```json
[
  { "role": "user", "content": "Analyze all files and write a report..." },
  { "role": "assistant", "content": "I'll help. Let me start by reading..." },
  { "role": "user", "content": [{ "type": "tool_result", "tool_use_id": "...", "content": "..." }] },
  { "role": "assistant", "content": "Based on file1.txt, I see..." },
  { "role": "user", "content": [{ "type": "tool_result", "tool_use_id": "...", "content": "..." }] },
  { "role": "assistant", "content": "After analyzing file2.txt..." },
  // ... 50 more exchanges like this ...
]
```

当令牌超过阈值时，SDK 注入摘要请求，Claude 生成摘要。然后整个历史被替换：

**压缩后（回到约 2-3k 令牌）：**
```json
[
  {
    "role": "assistant",
    "content": "# Task Overview\nThe user requested analysis of directory files to produce a summary report...\n\n# Current State\nAnalyzed 52 files across 3 subdirectories. Key findings documented in report.md...\n\n# Important Discoveries\n- Configuration files use YAML format\n- Found 3 deprecated dependencies\n- Test coverage at 67%\n\n# Next Steps\n1. Analyze remaining files in /src/legacy\n2. Complete final report sections...\n\n# Context to Preserve\nUser prefers markdown format with executive summary first..."
  }
]
```

Claude 从这个摘要继续工作，就像它是原始对话历史一样。

### 配置选项

| 参数 | 类型 | 必需 | 默认值 | 描述 |
|-----------|------|----------|---------|-------------|
| `enabled` | boolean | 是 | - | 是否启用自动压缩 |
| `context_token_threshold` | number | 否 | 100,000 | 触发压缩的令牌计数 |
| `model` | string | 否 | 与主模型相同 | 用于生成摘要的模型 |
| `summary_prompt` | string | 否 | 见下文 | 摘要生成的自定义提示 |

#### 选择令牌阈值

阈值确定何时发生压缩。较低的阈值意味着更频繁的压缩，上下文窗口更小。较高的阈值允许更多上下文，但存在达到限制的风险。

<CodeGroup>

```python Python
# 对于内存受限的场景进行更频繁的压缩
compaction_control={
    "enabled": True,
    "context_token_threshold": 50000
}

# 当您需要更多上下文时进行较少频繁的压缩
compaction_control={
    "enabled": True,
    "context_token_threshold": 150000
}
```

```typescript TypeScript
// 对于内存受限的场景进行更频繁的压缩
compactionControl: {
    enabled: true,
    contextTokenThreshold: 50000
}

// 当您需要更多上下文时进行较少频繁的压缩
compactionControl: {
    enabled: true,
    contextTokenThreshold: 150000
}
```

</CodeGroup>

#### 为摘要使用不同的模型

您可以使用更快或更便宜的模型来生成摘要：

<CodeGroup>

```python Python
compaction_control={
    "enabled": True,
    "context_token_threshold": 100000,
    "model": "claude-haiku-4-5"
}
```

```typescript TypeScript
compactionControl: {
    enabled: true,
    contextTokenThreshold: 100000,
    model: 'claude-haiku-4-5'
}
```

</CodeGroup>

#### 自定义摘要提示

您可以为特定领域的需求提供自定义提示。您的提示应指示 Claude 将其摘要包装在 `<summary></summary>` 标签中。

<CodeGroup>

```python Python
compaction_control={
    "enabled": True,
    "context_token_threshold": 100000,
    "summary_prompt": """Summarize the research conducted so far, including:
- Sources consulted and key findings
- Questions answered and remaining unknowns
- Recommended next steps

Wrap your summary in <summary></summary> tags."""
}
```

```typescript TypeScript
compactionControl: {
    enabled: true,
    contextTokenThreshold: 100000,
    summaryPrompt: `Summarize the research conducted so far, including:
- Sources consulted and key findings
- Questions answered and remaining unknowns
- Recommended next steps

Wrap your summary in <summary></summary> tags.`
}
```

</CodeGroup>

## 与内存工具一起使用

上下文编辑可以与[内存工具](/docs/zh-CN/agents-and-tools/tool-use/memory-tool)结合使用。当您的对话上下文接近配置的清除阈值时，Claude 会收到自动警告以保留重要信息。这使 Claude 能够在工具结果或上下文从对话历史中清除之前将其保存到内存文件中。

这种组合允许您：

- **保留重要上下文**：Claude 可以在工具结果被清除之前将其中的基本信息写入内存文件
- **维护长期运行的工作流**：通过将信息卸载到持久存储，启用可能会超过上下文限制的代理工作流
- **按需访问信息**：Claude 可以从内存文件中查找之前清除的信息，而不是将所有内容保留在活跃上下文窗口中

例如，在 Claude 执行许多操作的文件编辑工作流中，Claude 可以在上下文增长时将已完成的更改总结到内存文件中。当工具结果被清除时，Claude 通过其内存系统保留对该信息的访问权限，并可以继续有效地工作。

要同时使用这两个功能，请在您的 API 请求中启用它们：

<CodeGroup>

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=4096,
    messages=[...],
    tools=[
        {
            "type": "memory_20250818",
            "name": "memory"
        },
        # Your other tools
    ],
    betas=["context-management-2025-06-27"],
    context_management={
        "edits": [
            {"type": "clear_tool_uses_20250919"}
        ]
    }
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 4096,
  messages: [...],
  tools: [
    {
      type: "memory_20250818",
      name: "memory"
    },
    // Your other tools
  ],
  betas: ["context-management-2025-06-27"],
  context_management: {
    edits: [
      { type: "clear_tool_uses_20250919" }
    ]
  }
});
```

</CodeGroup>

## 客户端压缩 (SDK)

<Note>
压缩功能在使用 [`tool_runner` 方法](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#tool-runner-beta)时可在 [Python 和 TypeScript SDK](/docs/zh-CN/api/client-sdks) 中使用。
</Note>

压缩是一个 SDK 功能，当令牌使用量增长过大时，它通过生成摘要来自动管理对话上下文。与清除内容的服务器端上下文编辑策略不同，压缩指示 Claude 总结对话历史，然后用该摘要替换完整历史。这允许 Claude 继续处理长期运行的任务，否则会超过[上下文窗口](/docs/zh-CN/build-with-claude/context-windows)。

### 压缩如何工作

启用压缩后，SDK 在每个模型响应后监控令牌使用情况：

1. **阈值检查**：SDK 将总令牌计算为 `input_tokens + cache_creation_input_tokens + cache_read_input_tokens + output_tokens`
2. **摘要生成**：当超过阈值时，摘要提示被注入为用户回合，Claude 生成包装在 `<summary></summary>` 标签中的结构化摘要
3. **上下文替换**：SDK 提取摘要并用它替换整个消息历史
4. **继续**：对话从摘要恢复，Claude 从中断处继续

### 使用压缩

将 `compaction_control` 添加到您的 `tool_runner` 调用中：

<CodeGroup>

```python Python
import anthropic

client = anthropic.Anthropic()

runner = client.beta.messages.tool_runner(
    model="claude-sonnet-4-5",
    max_tokens=4096,
    tools=[...],
    messages=[
        {
            "role": "user",
            "content": "Analyze all the files in this directory and write a summary report."
        }
    ],
    compaction_control={
        "enabled": True,
        "context_token_threshold": 100000
    }
)

for message in runner:
    print(f"Tokens used: {message.usage.input_tokens}")

final = runner.until_done()
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const runner = client.beta.messages.toolRunner({
    model: 'claude-sonnet-4-5',
    max_tokens: 4096,
    tools: [...],
    messages: [
        {
            role: 'user',
            content: 'Analyze all the files in this directory and write a summary report.'
        }
    ],
    compactionControl: {
        enabled: true,
        contextTokenThreshold: 100000
    }
});

for await (const message of runner) {
    console.log('Tokens used:', message.usage.input_tokens);
}

const finalMessage = await runner.runUntilDone();
```

</CodeGroup>

#### 压缩期间发生的情况

随着对话的增长，消息历史会累积：

**压缩前（接近 100k 令牌）：**
```json
[
  { "role": "user", "content": "Analyze all files and write a report..." },
  { "role": "assistant", "content": "I'll help. Let me start by reading..." },
  { "role": "user", "content": [{ "type": "tool_result", "tool_use_id": "...", "content": "..." }] },
  { "role": "assistant", "content": "Based on file1.txt, I see..." },
  { "role": "user", "content": [{ "type": "tool_result", "tool_use_id": "...", "content": "..." }] },
  { "role": "assistant", "content": "After analyzing file2.txt..." },
  // ... 50 more exchanges like this ...
]
```

当令牌超过阈值时，SDK 注入摘要请求，Claude 生成摘要。然后整个历史被替换：

**压缩后（回到约 2-3k 令牌）：**
```json
[
  {
    "role": "assistant",
    "content": "# Task Overview\nThe user requested analysis of directory files to produce a summary report...\n\n# Current State\nAnalyzed 52 files across 3 subdirectories. Key findings documented in report.md...\n\n# Important Discoveries\n- Configuration files use YAML format\n- Found 3 deprecated dependencies\n- Test coverage at 67%\n\n# Next Steps\n1. Analyze remaining files in /src/legacy\n2. Complete final report sections...\n\n# Context to Preserve\nUser prefers markdown format with executive summary first..."
  }
]
```

Claude 从这个摘要继续工作，就像它是原始对话历史一样。

### 配置选项

| 参数 | 类型 | 必需 | 默认值 | 描述 |
|-----------|------|----------|---------|-------------|
| `enabled` | boolean | 是 | - | 是否启用自动压缩 |
| `context_token_threshold` | number | 否 | 100,000 | 触发压缩的令牌计数 |
| `model` | string | 否 | 与主模型相同 | 用于生成摘要的模型 |
| `summary_prompt` | string | 否 | 见下文 | 摘要生成的自定义提示 |

#### 选择令牌阈值

阈值决定了压缩何时发生。较低的阈值意味着更频繁的压缩，上下文窗口更小。较高的阈值允许更多上下文，但有触及限制的风险。

<CodeGroup>

```python Python
# More frequent compaction for memory-constrained scenarios
compaction_control={
    "enabled": True,
    "context_token_threshold": 50000
}

# Less frequent compaction when you need more context
compaction_control={
    "enabled": True,
    "context_token_threshold": 150000
}
```

```typescript TypeScript
// More frequent compaction for memory-constrained scenarios
compactionControl: {
    enabled: true,
    contextTokenThreshold: 50000
}

// Less frequent compaction when you need more context
compactionControl: {
    enabled: true,
    contextTokenThreshold: 150000
}
```

</CodeGroup>

#### 使用不同的模型生成摘要

您可以使用更快或更便宜的模型来生成摘要：

<CodeGroup>

```python Python
compaction_control={
    "enabled": True,
    "context_token_threshold": 100000,
    "model": "claude-haiku-4-5"
}
```

```typescript TypeScript
compactionControl: {
    enabled: true,
    contextTokenThreshold: 100000,
    model: 'claude-haiku-4-5'
}
```

</CodeGroup>

#### 自定义摘要提示

您可以为特定领域的需求提供自定义提示。您的提示应指示 Claude 将其摘要包装在 `<summary></summary>` 标签中。

<CodeGroup>

```python Python
compaction_control={
    "enabled": True,
    "context_token_threshold": 100000,
    "summary_prompt": """Summarize the research conducted so far, including:
- Sources consulted and key findings
- Questions answered and remaining unknowns
- Recommended next steps

Wrap your summary in <summary></summary> tags."""
}
```

```typescript TypeScript
compactionControl: {
    enabled: true,
    contextTokenThreshold: 100000,
    summaryPrompt: `Summarize the research conducted so far, including:
- Sources consulted and key findings
- Questions answered and remaining unknowns
- Recommended next steps

Wrap your summary in <summary></summary> tags.`
}
```

</CodeGroup>

### 默认摘要提示

内置摘要提示指示 Claude 创建包含以下内容的结构化继续摘要：

1. **任务概述**：用户的核心请求、成功标准和约束
2. **当前状态**：已完成的内容、修改的文件和生成的工件
3. **重要发现**：技术约束、做出的决定、解决的错误和失败的方法
4. **后续步骤**：需要的具体行动、阻碍因素和优先顺序
5. **保留的上下文**：用户偏好、特定领域的详细信息和做出的承诺

这种结构使 Claude 能够高效地恢复工作，而不会丢失重要上下文或重复错误。

<section title="查看完整默认提示">

```
You have been working on the task described above but have not yet completed it. Write a continuation summary that will allow you (or another instance of yourself) to resume work efficiently in a future context window where the conversation history will be replaced with this summary. Your summary should be structured, concise, and actionable. Include:

1. Task Overview
The user's core request and success criteria
Any clarifications or constraints they specified

2. Current State
What has been completed so far
Files created, modified, or analyzed (with paths if relevant)
Key outputs or artifacts produced

3. Important Discoveries
Technical constraints or requirements uncovered
Decisions made and their rationale
Errors encountered and how they were resolved
What approaches were tried that didn't work (and why)

4. Next Steps
Specific actions needed to complete the task
Any blockers or open questions to resolve
Priority order if multiple steps remain

5. Context to Preserve
User preferences or style requirements
Domain-specific details that aren't obvious
Any promises made to the user

Be concise but complete—err on the side of including information that would prevent duplicate work or repeated mistakes. Write in a way that enables immediate resumption of the task.

Wrap your summary in <summary></summary> tags.
```

</section>

### 限制

#### 服务器端工具

<Warning>
使用服务器端工具（如[网络搜索](/docs/zh-CN/agents-and-tools/tool-use/web-search-tool)或[网络获取](/docs/zh-CN/agents-and-tools/tool-use/web-fetch-tool)）时，压缩需要特殊考虑。
</Warning>

使用服务器端工具时，SDK 可能会错误地计算令牌使用情况，导致压缩在错误的时间触发。

例如，在网络搜索操作后，API 响应可能显示：

```json
{
  "usage": {
    "input_tokens": 63000,
    "cache_read_input_tokens": 270000,
    "output_tokens": 1400
  }
}
```

SDK 将总使用情况计算为 63,000 + 270,000 = 333,000 令牌。但是，`cache_read_input_tokens` 值包括服务器端工具进行的多个内部 API 调用的累积读取，而不是您的实际对话上下文。您的真实上下文长度可能只有 63,000 个 `input_tokens`，但 SDK 看到 333k 并过早触发压缩。

**解决方案：**

- 使用[令牌计数](/docs/zh-CN/build-with-claude/token-counting)端点获取准确的上下文长度
- 在广泛使用服务器端工具时避免压缩

#### 工具使用边界情况

当在工具使用响应待处理时触发压缩时，SDK 在生成摘要之前从消息历史中删除工具使用块。如果仍需要，Claude 将在从摘要恢复后重新发出工具调用。

### 限制

#### 服务器端工具

<Warning>
使用服务器端工具（如[网络搜索](/docs/zh-CN/agents-and-tools/tool-use/web-search-tool)或[网络获取](/docs/zh-CN/agents-and-tools/tool-use/web-fetch-tool)）时，压缩需要特殊考虑。
</Warning>

使用服务器端工具时，SDK 可能会错误地计算令牌使用情况，导致压缩在错误的时间触发。

例如，在网络搜索操作后，API 响应可能显示：

```json
{
  "usage": {
    "input_tokens": 63000,
    "cache_read_input_tokens": 270000,
    "output_tokens": 1400
  }
}
```

SDK 将总使用情况计算为 63,000 + 270,000 = 333,000 令牌。但是，`cache_read_input_tokens` 值包括服务器端工具进行的多个内部 API 调用的累积读取，而不是您的实际对话上下文。您的真实上下文长度可能只有 63,000 个 `input_tokens`，但 SDK 看到 333k 并过早触发压缩。

**解决方案：**

- 使用[令牌计数](/docs/zh-CN/build-with-claude/token-counting)端点获取准确的上下文长度
- 在广泛使用服务器端工具时避免压缩

#### 工具使用边界情况

当在工具使用响应待处理时触发压缩时，SDK 在生成摘要之前从消息历史中删除工具使用块。如果仍需要，Claude 将在从摘要恢复后重新发出工具调用。

### 监控压缩

启用日志记录以跟踪压缩何时发生：

<CodeGroup>

```python Python
import logging

logging.basicConfig(level=logging.INFO)
logging.getLogger("anthropic.lib.tools").setLevel(logging.INFO)

# Logs will show:
# INFO: Token usage 105000 has exceeded the threshold of 100000. Performing compaction.
# INFO: Compaction complete. New token usage: 2500
```

```typescript TypeScript
// The SDK logs compaction events to the console
// You'll see messages like:
// Token usage 105000 has exceeded the threshold of 100000. Performing compaction.
// Compaction complete. New token usage: 2500
```

</CodeGroup>

### 限制

#### 服务器端工具

<Warning>
使用服务器端工具（如[网络搜索](/docs/zh-CN/agents-and-tools/tool-use/web-search-tool)或[网络获取](/docs/zh-CN/agents-and-tools/tool-use/web-fetch-tool)）时，压缩需要特殊考虑。
</Warning>

使用服务器端工具时，SDK 可能会错误地计算令牌使用情况，导致压缩在错误的时间触发。

例如，在网络搜索操作后，API 响应可能显示：

```json
{
  "usage": {
    "input_tokens": 63000,
    "cache_read_input_tokens": 270000,
    "output_tokens": 1400
  }
}
```

SDK 将总使用情况计算为 63,000 + 270,000 = 333,000 令牌。但是，`cache_read_input_tokens` 值包括服务器端工具进行的多个内部 API 调用的累积读取，而不是您的实际对话上下文。您的真实上下文长度可能只有 63,000 个 `input_tokens`，但 SDK 看到 333k 并过早触发压缩。

**解决方案：**

- 使用[令牌计数](/docs/zh-CN/build-with-claude/token-counting)端点获取准确的上下文长度
- 在广泛使用服务器端工具时避免压缩

#### 工具使用边界情况

当在工具使用响应待处理时触发压缩时，SDK 在生成摘要之前从消息历史中删除工具使用块。如果仍需要，Claude 将在从摘要恢复后重新发出工具调用。

### 监控压缩

启用日志记录以跟踪压缩何时发生：

<CodeGroup>

```python Python
import logging

logging.basicConfig(level=logging.INFO)
logging.getLogger("anthropic.lib.tools").setLevel(logging.INFO)

# Logs will show:
# INFO: Token usage 105000 has exceeded the threshold of 100000. Performing compaction.
# INFO: Compaction complete. New token usage: 2500
```

```typescript TypeScript
// The SDK logs compaction events to the console
// You'll see messages like:
// Token usage 105000 has exceeded the threshold of 100000. Performing compaction.
// Compaction complete. New token usage: 2500
```

</CodeGroup>

### 何时使用压缩

**良好的用例：**

- 处理许多文件或数据源的长期运行代理任务
- 积累大量信息的研究工作流
- 具有明确、可测量进度的多步骤任务
- 生成在对话外持久存在的工件（文件、报告）的任务

**不太理想的用例：**

- 需要精确回忆早期对话细节的任务
- 广泛使用服务器端工具的工作流
- 需要在许多变量中维护精确状态的任务

# 使用扩展思考进行构建

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

---

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

### 总结的思考

启用扩展思考后，Claude 4 模型的消息 API 会返回 Claude 完整思考过程的摘要。总结的思考提供了扩展思考的全部智能优势，同时防止滥用。

以下是关于总结思考的一些重要注意事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了特别有助于提示工程的详细推理。
- 随着 Anthropic 寻求改进扩展思考功能，总结行为可能会发生变化。
- 总结保留了 Claude 思考过程的关键思想，延迟最少，实现了可流式传输的用户体验，并便于从 Claude Sonnet 3.7 迁移到 Claude 4 模型。
- 总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

### 总结的思考

启用扩展思考后，Claude 4 模型的消息 API 会返回 Claude 完整思考过程的摘要。总结的思考提供了扩展思考的全部智能优势，同时防止滥用。

以下是关于总结思考的一些重要注意事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了特别有助于提示工程的详细推理。
- 随着 Anthropic 寻求改进扩展思考功能，总结行为可能会发生变化。
- 总结保留了 Claude 思考过程的关键思想，延迟最少，实现了可流式传输的用户体验，并便于从 Claude Sonnet 3.7 迁移到 Claude 4 模型。
- 总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

### 流式传输思考

您可以使用[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) 流式传输扩展思考响应。

启用扩展思考的流式传输后，您会通过 `thinking_delta` 事件接收思考内容。

有关通过消息 API 进行流式传输的更多文档，请参阅[流式传输消息](/docs/zh-CN/build-with-claude/streaming)。

以下是如何处理思考流式传输的方法：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "stream": true,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "What is 27 * 453?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={"type": "enabled", "budget_tokens": 10000},
    messages=[{"role": "user", "content": "What is 27 * 453?"}],
) as stream:
    thinking_started = False
    response_started = False

    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
            # 为每个新块重置标志
            thinking_started = False
            response_started = False
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                if not thinking_started:
                    print("Thinking: ", end="", flush=True)
                    thinking_started = True
                print(event.delta.thinking, end="", flush=True)
            elif event.delta.type == "text_delta":
                if not response_started:
                    print("Response: ", end="", flush=True)
                    response_started = True
                print(event.delta.text, end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const stream = await client.messages.stream({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "What is 27 * 453?"
  }]
});

let thinkingStarted = false;
let responseStarted = false;

for await (const event of stream) {
  if (event.type === 'content_block_start') {
    console.log(`\nStarting ${event.content_block.type} block...`);
    // 为每个新块重置标志
    thinkingStarted = false;
    responseStarted = false;
  } else if (event.type === 'content_block_delta') {
    if (event.delta.type === 'thinking_delta') {
      if (!thinkingStarted) {
        process.stdout.write('Thinking: ');
        thinkingStarted = true;
      }
      process.stdout.write(event.delta.thinking);
    } else if (event.delta.type === 'text_delta') {
      if (!responseStarted) {
        process.stdout.write('Response: ');
        responseStarted = true;
      }
      process.stdout.write(event.delta.text);
    }
  } else if (event.type === 'content_block_stop') {
    console.log('\nBlock complete.');
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.http.StreamResponse;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class SimpleThinkingStreamingExample {
    private static boolean thinkingStarted = false;
    private static boolean responseStarted = false;
    
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams createParams = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_0)
                .maxTokens(16000)
                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                .addUserMessage("What is 27 * 453?")
                .build();

        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                     client.beta().messages().createStreaming(createParams)) {
            streamResponse.stream()
                    .forEach(event -> {
                        if (event.isContentBlockStart()) {
                            System.out.printf("\nStarting %s block...%n",
                                    event.asContentBlockStart()._type());
                            // 为每个新块重置标志
                            thinkingStarted = false;
                            responseStarted = false;
                        } else if (event.isContentBlockDelta()) {
                            var delta = event.asContentBlockDelta().delta();
                            if (delta.isBetaThinking()) {
                                if (!thinkingStarted) {
                                    System.out.print("Thinking: ");
                                    thinkingStarted = true;
                                }
                                System.out.print(delta.asBetaThinking().thinking());
                                System.out.flush();
                            } else if (delta.isBetaText()) {
                                if (!responseStarted) {
                                    System.out.print("Response: ");
                                    responseStarted = true;
                                }
                                System.out.print(delta.asBetaText().text());
                                System.out.flush();
                            }
                        } else if (event.isContentBlockStop()) {
                            System.out.println("\nBlock complete.");
                        }
                    });
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  在控制台中尝试
</TryInConsoleButton>

示例流式传输输出：
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "让我逐步解决这个问题：\n\n1. 首先分解 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// 其他思考增量...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// 其他文本增量...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

<Note>
使用启用思考的流式传输时，您可能会注意到文本有时以较大的块到达，交替出现较小的逐令牌传递。这是预期的行为，特别是对于思考内容。

流式传输系统需要分批处理内容以获得最佳性能，这可能导致这种"分块"传递模式，流式传输事件之间可能出现延迟。我们正在不断努力改进这种体验，未来的更新将专注于使思考内容流式传输更加平顺。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于处理复杂任务，同时在提供最终答案之前提供了对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制作最终响应之前会结合来自此推理的见解。

API 响应将包括 `thinking` 内容块，然后是 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅 [Messages API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在 Messages API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# The response will contain summarized thinking blocks and text blocks
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// The response will contain summarized thinking blocks and text blocks
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不是[摘要输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变为您的整个上下文窗口（200k 令牌）。

### 摘要思考

启用扩展思考后，Claude 4 模型的 Messages API 返回 Claude 完整思考过程的摘要。摘要思考提供了扩展思考的全部智能优势，同时防止了滥用。

以下是摘要思考的一些重要考虑事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了详细的推理，这对提示工程目的特别有帮助。
- 随着 Anthropic 寻求改进扩展思考功能，摘要行为可能会发生变化。
- 摘要保留了 Claude 思考过程的关键思想，同时增加了最少的延迟，实现了可流式传输的用户体验和从 Claude Sonnet 3.7 到 Claude 4 模型的轻松迁移。
- 摘要由与您在请求中指定的模型不同的模型处理。思考模型看不到摘要输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

### 流式思考

您可以使用[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) 流式传输扩展思考响应。

启用扩展思考的流式传输后，您会通过 `thinking_delta` 事件接收思考内容。

有关通过 Messages API 进行流式传输的更多文档，请参阅[流式传输消息](/docs/zh-CN/build-with-claude/streaming)。

以下是如何处理思考流式传输的方法：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "stream": true,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "What is 27 * 453?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={"type": "enabled", "budget_tokens": 10000},
    messages=[{"role": "user", "content": "What is 27 * 453?"}],
) as stream:
    thinking_started = False
    response_started = False

    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
            # Reset flags for each new block
            thinking_started = False
            response_started = False
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                if not thinking_started:
                    print("Thinking: ", end="", flush=True)
                    thinking_started = True
                print(event.delta.thinking, end="", flush=True)
            elif event.delta.type == "text_delta":
                if not response_started:
                    print("Response: ", end="", flush=True)
                    response_started = True
                print(event.delta.text, end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const stream = await client.messages.stream({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "What is 27 * 453?"
  }]
});

let thinkingStarted = false;
let responseStarted = false;

for await (const event of stream) {
  if (event.type === 'content_block_start') {
    console.log(`\nStarting ${event.content_block.type} block...`);
    // Reset flags for each new block
    thinkingStarted = false;
    responseStarted = false;
  } else if (event.type === 'content_block_delta') {
    if (event.delta.type === 'thinking_delta') {
      if (!thinkingStarted) {
        process.stdout.write('Thinking: ');
        thinkingStarted = true;
      }
      process.stdout.write(event.delta.thinking);
    } else if (event.delta.type === 'text_delta') {
      if (!responseStarted) {
        process.stdout.write('Response: ');
        responseStarted = true;
      }
      process.stdout.write(event.delta.text);
    }
  } else if (event.type === 'content_block_stop') {
    console.log('\nBlock complete.');
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.http.StreamResponse;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class SimpleThinkingStreamingExample {
    private static boolean thinkingStarted = false;
    private static boolean responseStarted = false;
    
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams createParams = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_0)
                .maxTokens(16000)
                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                .addUserMessage("What is 27 * 453?")
                .build();

        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                     client.beta().messages().createStreaming(createParams)) {
            streamResponse.stream()
                    .forEach(event -> {
                        if (event.isContentBlockStart()) {
                            System.out.printf("\nStarting %s block...%n",
                                    event.asContentBlockStart()._type());
                            // Reset flags for each new block
                            thinkingStarted = false;
                            responseStarted = false;
                        } else if (event.isContentBlockDelta()) {
                            var delta = event.asContentBlockDelta().delta();
                            if (delta.isBetaThinking()) {
                                if (!thinkingStarted) {
                                    System.out.print("Thinking: ");
                                    thinkingStarted = true;
                                }
                                System.out.print(delta.asBetaThinking().thinking());
                                System.out.flush();
                            } else if (delta.isBetaText()) {
                                if (!responseStarted) {
                                    System.out.print("Response: ");
                                    responseStarted = true;
                                }
                                System.out.print(delta.asBetaText().text());
                                System.out.flush();
                            }
                        } else if (event.isContentBlockStop()) {
                            System.out.println("\nBlock complete.");
                        }
                    });
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  在控制台中尝试
</TryInConsoleButton>

示例流式输出：
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "Let me solve this step by step:\n\n1. First break down 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// Additional thinking deltas...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// Additional text deltas...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

<Note>
使用启用思考的流式传输时，您可能会注意到文本有时会以较大的块到达，交替出现较小的逐令牌传输。这是预期的行为，特别是对于思考内容。

流式传输系统需要分批处理内容以获得最佳性能，这可能导致这种"块状"传输模式，流式传输事件之间可能出现延迟。我们正在不断努力改进这种体验，未来的更新将专注于使思考内容流式传输更加平稳。
</Note>

## 扩展思考与工具使用

扩展思考可以与[工具使用](/docs/zh-CN/agents-and-tools/tool-use/overview)一起使用，允许 Claude 通过工具选择和结果处理进行推理。

使用扩展思考和工具使用时，请注意以下限制：

1. **工具选择限制**：工具使用与思考仅支持 `tool_choice: {"type": "auto"}` (默认) 或 `tool_choice: {"type": "none"}`。使用 `tool_choice: {"type": "any"}` 或 `tool_choice: {"type": "tool", "name": "..."}` 将导致错误，因为这些选项强制工具使用，这与扩展思考不兼容。

2. **保留思考块**：在工具使用期间，您必须将 `thinking` 块传回 API 以获取最后的助手消息。将完整的未修改块传回 API 以保持推理连续性。

### 在对话中切换思考模式

您不能在助手转向的中间切换思考，包括在工具使用循环期间。整个助手转向必须在单一思考模式下运行：

- **如果启用了思考**，最后的助手转向必须以思考块开始。
- **如果禁用了思考**，最后的助手转向不能包含任何思考块

从模型的角度来看，**工具使用循环是助手转向的一部分**。助手转向直到 Claude 完成其完整响应后才完成，这可能包括多个工具调用和结果。

例如，此序列都是**单个助手转向**的一部分：
```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```

尽管有多个 API 消息，但工具使用循环在概念上是一个连续助手响应的一部分。

#### 常见错误场景

您可能会遇到此错误：
```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```

这通常发生在以下情况：
1. 您在工具使用序列期间**禁用了**思考
2. 您想再次启用思考
3. 您的最后一条助手消息包含工具使用块但没有思考块

#### 实用指南

**✗ 无效：在工具使用后立即切换思考**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```

**✓ 有效：首先完成助手转向**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```

**最佳实践**：在每个转向开始时规划您的思考策略，而不是尝试在中途切换。

<Note>
切换思考模式也会使消息历史的提示缓存失效。有关更多详情，请参阅[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分。
</Note>

<section title="示例：使用工具结果传递思考块">

以下是一个实际示例，展示了在提供工具结果时如何保留思考块：

<CodeGroup>
```python Python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

```typescript TypeScript
const weatherTool = {
  name: "get_weather",
  description: "Get current weather for a location",
  input_schema: {
    type: "object",
    properties: {
      location: { type: "string" }
    },
    required: ["location"]
  }
};

// First request - Claude responds with thinking and tool request
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" }
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.beta.messages.BetaTool;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingWithToolsExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        System.out.println(response);
    }
}
```
</CodeGroup>

API 响应将包括思考、文本和 tool_use 块：

```json
{
    "content": [
        {
            "type": "thinking",
            "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
            "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
        },
        {
            "type": "text",
            "text": "I can help you get the current weather information for Paris. Let me check that for you"
        },
        {
            "type": "tool_use",
            "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
            "name": "get_weather",
            "input": {
                "location": "Paris"
            }
        }
    ]
}
```

现在让我们继续对话并使用该工具

<CodeGroup>
```python Python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

```typescript TypeScript
// Extract thinking block and tool use block
const thinkingBlock = response.content.find(block =>
  block.type === 'thinking');
const toolUseBlock = response.content.find(block =>
  block.type === 'tool_use');

// Call your actual weather API, here is where your actual API call would go
// let's pretend this is what we get back
const weatherData = { temperature: 88 };

// Second request - Include thinking block and tool result
// No new thinking blocks will be generated in the response
const continuation = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" },
    // notice that the thinkingBlock is passed in as well as the toolUseBlock
    // if this is not passed in, an error is raised
    { role: "assistant", content: [thinkingBlock, toolUseBlock] },
    { role: "user", content: [{
      type: "tool_result",
      tool_use_id: toolUseBlock.id,
      content: `Current temperature: ${weatherData.temperature}°F`
    }]}
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;
import java.util.Optional;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingToolsResultExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        // Extract thinking block and tool use block
        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isThinking)
                .map(BetaContentBlock::asThinking)
                .findFirst();

        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isToolUse)
                .map(BetaContentBlock::asToolUse)
                .findFirst();

        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

            // Call your actual weather API, here is where your actual API call would go
            // let's pretend this is what we get back
            Map<String, Object> weatherData = Map.of("temperature", 88);

            // Second request - Include thinking block and tool result
            // No new thinking blocks will be generated in the response
            BetaMessage continuation = client.beta().messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_OPUS_4_0)
                            .maxTokens(16000)
                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                            .addTool(weatherTool)
                            .addUserMessage("What's the weather in Paris?")
                            .addAssistantMessageOfBetaContentBlockParams(
                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                    // if this is not passed in, an error is raised
                                    List.of(
                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                    )
                            )
                            .addUserMessageOfBetaContentBlockParams(List.of(
                                    BetaContentBlockParam.ofToolResult(
                                            BetaToolResultBlockParam.builder()
                                                    .toolUseId(toolUseBlock.id())
                                                    .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                    .build()
                                    )
                            ))
                            .build()
            );

            System.out.println(continuation);
        }
    }
}
```
</CodeGroup>

API 响应现在将**仅**包括文本

```json
{
    "content": [
        {
            "type": "text",
            "text": "Currently in Paris, the temperature is 88°F (31°C)"
        }
    ]
}
```

</section>

### 在对话中切换思考模式

您不能在助手回合的中间切换思考，包括在工具使用循环期间。整个助手回合必须在单一思考模式下运行：

- **如果启用了思考**，最终的助手回合必须以思考块开始。
- **如果禁用了思考**，最终的助手回合不能包含任何思考块

从模型的角度来看，**工具使用循环是助手回合的一部分**。助手回合直到 Claude 完成其完整响应（可能包括多个工具调用和结果）才完成。

例如，这个序列都是**单个助手回合**的一部分：
```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```

尽管有多个 API 消息，但工具使用循环在概念上是一个连续的助手响应的一部分。

#### 常见错误场景

您可能会遇到此错误：
```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```

这通常发生在以下情况：
1. 您在工具使用序列期间**禁用了**思考
2. 您想再次启用思考
3. 您的最后一条助手消息包含工具使用块但没有思考块

#### 实用指导

**✗ 无效：在工具使用后立即切换思考**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```

**✓ 有效：首先完成助手回合**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```

**最佳实践**：在每个回合开始时规划您的思考策略，而不是尝试在中间切换。

<Note>
切换思考模式也会使消息历史的提示缓存失效。有关更多详细信息，请参阅 [使用提示缓存的扩展思考](#extended-thinking-with-prompt-caching) 部分。
</Note>

<section title="示例：使用工具结果传递思考块">

这是一个实际示例，展示了在提供工具结果时如何保留思考块：

<CodeGroup>
```python Python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

```typescript TypeScript
const weatherTool = {
  name: "get_weather",
  description: "Get current weather for a location",
  input_schema: {
    type: "object",
    properties: {
      location: { type: "string" }
    },
    required: ["location"]
  }
};

// First request - Claude responds with thinking and tool request
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" }
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.beta.messages.BetaTool;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingWithToolsExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        System.out.println(response);
    }
}
```
</CodeGroup>

API 响应将包括思考、文本和 tool_use 块：

```json
{
    "content": [
        {
            "type": "thinking",
            "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
            "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
        },
        {
            "type": "text",
            "text": "I can help you get the current weather information for Paris. Let me check that for you"
        },
        {
            "type": "tool_use",
            "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
            "name": "get_weather",
            "input": {
                "location": "Paris"
            }
        }
    ]
}
```

现在让我们继续对话并使用该工具

<CodeGroup>
```python Python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

```typescript TypeScript
// Extract thinking block and tool use block
const thinkingBlock = response.content.find(block =>
  block.type === 'thinking');
const toolUseBlock = response.content.find(block =>
  block.type === 'tool_use');

// Call your actual weather API, here is where your actual API call would go
// let's pretend this is what we get back
const weatherData = { temperature: 88 };

// Second request - Include thinking block and tool result
// No new thinking blocks will be generated in the response
const continuation = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" },
    // notice that the thinkingBlock is passed in as well as the toolUseBlock
    // if this is not passed in, an error is raised
    { role: "assistant", content: [thinkingBlock, toolUseBlock] },
    { role: "user", content: [{
      type: "tool_result",
      tool_use_id: toolUseBlock.id,
      content: `Current temperature: ${weatherData.temperature}°F`
    }]}
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;
import java.util.Optional;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingToolsResultExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        // Extract thinking block and tool use block
        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isThinking)
                .map(BetaContentBlock::asThinking)
                .findFirst();

        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isToolUse)
                .map(BetaContentBlock::asToolUse)
                .findFirst();

        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

            // Call your actual weather API, here is where your actual API call would go
            // let's pretend this is what we get back
            Map<String, Object> weatherData = Map.of("temperature", 88);

            // Second request - Include thinking block and tool result
            // No new thinking blocks will be generated in the response
            BetaMessage continuation = client.beta().messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_OPUS_4_0)
                            .maxTokens(16000)
                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                            .addTool(weatherTool)
                            .addUserMessage("What's the weather in Paris?")
                            .addAssistantMessageOfBetaContentBlockParams(
                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                    // if this is not passed in, an error is raised
                                    List.of(
                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                    )
                            )
                            .addUserMessageOfBetaContentBlockParams(List.of(
                                    BetaContentBlockParam.ofToolResult(
                                            BetaToolResultBlockParam.builder()
                                                    .toolUseId(toolUseBlock.id())
                                                    .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                    .build()
                                    )
                            ))
                            .build()
            );

            System.out.println(continuation);
        }
    }
}
```
</CodeGroup>

API 响应现在**仅**包括文本

```json
{
    "content": [
        {
            "type": "text",
            "text": "Currently in Paris, the temperature is 88°F (31°C)"
        }
    ]
}
```

</section>

### 保留思考块

在工具使用期间，您必须将 `thinking` 块传递回 API，并且必须将完整的未修改块包含回 API。这对于维持模型的推理流和对话完整性至关重要。

<Tip>
虽然您可以从先前的 `assistant` 角色回合中省略 `thinking` 块，但我们建议对于任何多轮对话始终将所有思考块传递回 API。API 将：
- 自动过滤提供的思考块
- 使用必要的相关思考块来保留模型的推理
- 仅对显示给 Claude 的块的输入令牌进行计费
</Tip>

<Note>
在对话期间切换思考模式时，请记住整个助手回合（包括工具使用循环）必须在单一思考模式下运行。有关更多详细信息，请参阅 [在对话中切换思考模式](#toggling-thinking-modes-in-conversations)。
</Note>

当 Claude 调用工具时，它暂停了其响应的构建以等待外部信息。当工具结果返回时，Claude 将继续构建该现有响应。这需要在工具使用期间保留思考块，原因有几个：

1. **推理连续性**：思考块捕获了导致工具请求的 Claude 的逐步推理。当您发布工具结果时，包括原始思考可以确保 Claude 可以从中断的地方继续其推理。

2. **上下文维护**：虽然工具结果在 API 结构中显示为用户消息，但它们是连续推理流的一部分。保留思考块在多个 API 调用中维持这个概念流。有关上下文管理的更多信息，请参阅我们的 [上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)。

**重要**：提供 `thinking` 块时，连续 `thinking` 块的整个序列必须与模型在原始请求期间生成的输出相匹配；您不能重新排列或修改这些块的序列。

### 交错思考

Claude 4 模型中的扩展思考与工具使用支持交错思考，这使 Claude 能够在工具调用之间进行思考，并在接收工具结果后进行更复杂的推理。

通过交错思考，Claude 可以：
- 在工具调用结果后进行推理，然后决定接下来做什么
- 在推理步骤之间链接多个工具调用
- 根据中间结果做出更细致的决策

要启用交错思考，请将[测试版标头](/docs/zh-CN/api/beta-headers) `interleaved-thinking-2025-05-14` 添加到您的 API 请求中。

以下是交错思考的一些重要注意事项：
- 使用交错思考时，`budget_tokens` 可以超过 `max_tokens` 参数，因为它代表一个助手轮次内所有思考块的总预算。
- 交错思考仅支持[通过消息 API 使用的工具](/docs/zh-CN/agents-and-tools/tool-use/overview)。
- 交错思考仅支持 Claude 4 模型，使用测试版标头 `interleaved-thinking-2025-05-14`。
- 直接调用 Claude API 允许您在对任何模型的请求中传递 `interleaved-thinking-2025-05-14`，但不会产生任何效果。
- 在第三方平台上（例如，[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock) 和 [Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)），如果您将 `interleaved-thinking-2025-05-14` 传递给除 Claude Opus 4.5、Claude Opus 4.1、Opus 4 或 Sonnet 4 之外的任何模型，您的请求将失败。

<section title="不使用交错思考的工具使用">

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 定义工具
calculator_tool = {
    "name": "calculator",
    "description": "Perform mathematical calculations",
    "input_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
            }
        },
        "required": ["expression"]
    }
}

database_tool = {
    "name": "database_query",
    "description": "Query product database",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "SQL query to execute"
            }
        },
        "required": ["query"]
    }
}

# 第一个请求 - Claude 在所有工具调用之前思考一次
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    messages=[{
        "role": "user",
        "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    }]
)

# 响应包括思考，然后是工具使用
# 注意：Claude 在开始时思考一次，然后做出所有工具决策
print("First response:")
for block in response.content:
    if block.type == "thinking":
        print(f"Thinking (summarized): {block.thinking}")
    elif block.type == "tool_use":
        print(f"Tool use: {block.name} with input {block.input}")
    elif block.type == "text":
        print(f"Text: {block.text}")

# 您将执行工具并返回结果...
# 获得两个工具结果后，Claude 直接响应而不进行额外思考
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 定义工具
const calculatorTool = {
  name: "calculator",
  description: "Perform mathematical calculations",
  input_schema: {
    type: "object",
    properties: {
      expression: {
        type: "string",
        description: "Mathematical expression to evaluate"
      }
    },
    required: ["expression"]
  }
};

const databaseTool = {
  name: "database_query",
  description: "Query product database",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "SQL query to execute"
      }
    },
    required: ["query"]
  }
};

// 第一个请求 - Claude 在所有工具调用之前思考一次
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [{
    role: "user",
    content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
  }]
});

// 响应包括思考，然后是工具使用
// 注意：Claude 在开始时思考一次，然后做出所有工具决策
console.log("First response:");
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`Thinking (summarized): ${block.thinking}`);
  } else if (block.type === "tool_use") {
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  } else if (block.type === "text") {
    console.log(`Text: ${block.text}`);
  }
}

// 您将执行工具并返回结果...
// 获得两个工具结果后，Claude 直接响应而不进行额外思考
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.messages.Model;
import java.util.List;
import java.util.Map;

public class NonInterleavedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 定义计算器工具
        BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "expression", Map.of(
                                "type", "string",
                                "description", "Mathematical expression to evaluate"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                .build();

        BetaTool calculatorTool = BetaTool.builder()
                .name("calculator")
                .description("Perform mathematical calculations")
                .inputSchema(calculatorSchema)
                .build();

        // 定义数据库工具
        BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "SQL query to execute"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        BetaTool databaseTool = BetaTool.builder()
                .name("database_query")
                .description("Query product database")
                .inputSchema(databaseSchema)
                .build();

        // 第一个请求 - Claude 在所有工具调用之前思考一次
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .build()
        );

        // 响应包括思考，然后是工具使用
        // 注意：Claude 在开始时思考一次，然后做出所有工具决策
        System.out.println("First response:");
        for (BetaContentBlock block : response.content()) {
            if (block.isThinking()) {
                System.out.println("Thinking (summarized): " + block.asThinking().thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            } else if (block.isText()) {
                System.out.println("Text: " + block.asText().text());
            }
        }

        // 您将执行工具并返回结果...
        // 获得两个工具结果后，Claude 直接响应而不进行额外思考
    }
}
```
</CodeGroup>

在这个不使用交错思考的示例中：
1. Claude 在开始时思考一次以理解任务
2. 提前做出所有工具使用决策
3. 当工具结果返回时，Claude 立即提供响应而不进行额外思考

</section>

<section title="使用交错思考的工具使用">

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 与之前相同的工具定义
calculator_tool = {
    "name": "calculator",
    "description": "Perform mathematical calculations",
    "input_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
            }
        },
        "required": ["expression"]
    }
}

database_tool = {
    "name": "database_query",
    "description": "Query product database",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "SQL query to execute"
            }
        },
        "required": ["query"]
    }
}

# 第一个请求，启用交错思考
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[{
        "role": "user",
        "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    }]
)

print("Initial response:")
thinking_blocks = []
tool_use_blocks = []

for block in response.content:
    if block.type == "thinking":
        thinking_blocks.append(block)
        print(f"Thinking: {block.thinking}")
    elif block.type == "tool_use":
        tool_use_blocks.append(block)
        print(f"Tool use: {block.name} with input {block.input}")
    elif block.type == "text":
        print(f"Text: {block.text}")

# 第一个工具结果（计算器）
calculator_result = "7500"  # 150 * 50

# 继续第一个工具结果
response2 = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[
        {
            "role": "user",
            "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        },
        {
            "role": "assistant",
            "content": [thinking_blocks[0], tool_use_blocks[0]]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[0].id,
                "content": calculator_result
            }]
        }
    ]
)

print("\nAfter calculator result:")
# 使用交错思考，Claude 可以思考计算器结果
# 然后决定是否查询数据库
for block in response2.content:
    if block.type == "thinking":
        thinking_blocks.append(block)
        print(f"Interleaved thinking: {block.thinking}")
    elif block.type == "tool_use":
        tool_use_blocks.append(block)
        print(f"Tool use: {block.name} with input {block.input}")

# 第二个工具结果（数据库）
database_result = "5200"  # 示例平均月收入

# 继续第二个工具结果
response3 = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[
        {
            "role": "user",
            "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        },
        {
            "role": "assistant",
            "content": [thinking_blocks[0], tool_use_blocks[0]]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[0].id,
                "content": calculator_result
            }]
        },
        {
            "role": "assistant",
            "content": thinking_blocks[1:] + tool_use_blocks[1:]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[1].id,
                "content": database_result
            }]
        }
    ]
)

print("\nAfter database result:")
# 使用交错思考，Claude 可以思考两个结果
# 然后制定最终响应
for block in response3.content:
    if block.type == "thinking":
        print(f"Final thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Final response: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 与之前相同的工具定义
const calculatorTool = {
  name: "calculator",
  description: "Perform mathematical calculations",
  input_schema: {
    type: "object",
    properties: {
      expression: {
        type: "string",
        description: "Mathematical expression to evaluate"
      }
    },
    required: ["expression"]
  }
};

const databaseTool = {
  name: "database_query",
  description: "Query product database",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "SQL query to execute"
      }
    },
    required: ["query"]
  }
};

// 第一个请求，启用交错思考
const response = await client.beta.messages.create({
  // 启用交错思考
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [{
    role: "user",
    content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
  }]
});

console.log("Initial response:");
const thinkingBlocks = [];
const toolUseBlocks = [];

for (const block of response.content) {
  if (block.type === "thinking") {
    thinkingBlocks.push(block);
    console.log(`Thinking: ${block.thinking}`);
  } else if (block.type === "tool_use") {
    toolUseBlocks.push(block);
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  } else if (block.type === "text") {
    console.log(`Text: ${block.text}`);
  }
}

// 第一个工具结果（计算器）
const calculatorResult = "7500"; // 150 * 50

// 继续第一个工具结果
const response2 = await client.beta.messages.create({
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [
    {
      role: "user",
      content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    },
    {
      role: "assistant",
      content: [thinkingBlocks[0], toolUseBlocks[0]]
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[0].id,
        content: calculatorResult
      }]
    }
  ]
});

console.log("\nAfter calculator result:");
// 使用交错思考，Claude 可以思考计算器结果
// 然后决定是否查询数据库
for (const block of response2.content) {
  if (block.type === "thinking") {
    thinkingBlocks.push(block);
    console.log(`Interleaved thinking: ${block.thinking}`);
  } else if (block.type === "tool_use") {
    toolUseBlocks.push(block);
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  }
}

// 第二个工具结果（数据库）
const databaseResult = "5200"; // 示例平均月收入

// 继续第二个工具结果
const response3 = await client.beta.messages.create({
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [
    {
      role: "user",
      content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    },
    {
      role: "assistant",
      content: [thinkingBlocks[0], toolUseBlocks[0]]
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[0].id,
        content: calculatorResult
      }]
    },
    {
      role: "assistant",
      content: thinkingBlocks.slice(1).concat(toolUseBlocks.slice(1))
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[1].id,
        content: databaseResult
      }]
    }
  ]
});

console.log("\nAfter database result:");
// 使用交错思考，Claude 可以思考两个结果
// 然后制定最终响应
for (const block of response3.content) {
  if (block.type === "thinking") {
    console.log(`Final thinking: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`Final response: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.messages.Model;
import java.util.*;
import static java.util.stream.Collectors.toList;

public class InterleavedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 定义计算器工具
        BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "expression", Map.of(
                                "type", "string",
                                "description", "Mathematical expression to evaluate"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                .build();

        BetaTool calculatorTool = BetaTool.builder()
                .name("calculator")
                .description("Perform mathematical calculations")
                .inputSchema(calculatorSchema)
                .build();

        // 定义数据库工具
        BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "SQL query to execute"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        BetaTool databaseTool = BetaTool.builder()
                .name("database_query")
                .description("Query product database")
                .inputSchema(databaseSchema)
                .build();

        // 第一个请求，启用交错思考
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        // 使用测试版标头启用交错思考
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .build()
        );

        System.out.println("Initial response:");
        List<BetaThinkingBlock> thinkingBlocks = new ArrayList<>();
        List<BetaToolUseBlock> toolUseBlocks = new ArrayList<>();

        for (BetaContentBlock block : response.content()) {
            if (block.isThinking()) {
                BetaThinkingBlock thinking = block.asThinking();
                thinkingBlocks.add(thinking);
                System.out.println("Thinking: " + thinking.thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                toolUseBlocks.add(toolUse);
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            } else if (block.isText()) {
                System.out.println("Text: " + block.asText().text());
            }
        }

        // 第一个工具结果（计算器）
        String calculatorResult = "7500"; // 150 * 50

        // 继续第一个工具结果
        BetaMessage response2 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .addAssistantMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                        ))
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(0).id())
                                                .content(calculatorResult)
                                                .build()
                                )
                        ))
                        .build()
        );

        System.out.println("\nAfter calculator result:");
        // 使用交错思考，Claude 可以思考计算器结果
        // 然后决定是否查询数据库
        for (BetaContentBlock block : response2.content()) {
            if (block.isThinking()) {
                BetaThinkingBlock thinking = block.asThinking();
                thinkingBlocks.add(thinking);
                System.out.println("Interleaved thinking: " + thinking.thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                toolUseBlocks.add(toolUse);
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            }
        }

        // 第二个工具结果（数据库）
        String databaseResult = "5200"; // 示例平均月收入

        // 准备助手消息的组合内容
        List<BetaContentBlockParam> combinedContent = new ArrayList<>();
        for (int i = 1; i < thinkingBlocks.size(); i++) {
            combinedContent.add(BetaContentBlockParam.ofThinking(thinkingBlocks.get(i).toParam()));
        }
        for (int i = 1; i < toolUseBlocks.size(); i++) {
            combinedContent.add(BetaContentBlockParam.ofToolUse(toolUseBlocks.get(i).toParam()));
        }

        // 继续第二个工具结果
        BetaMessage response3 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .addAssistantMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                        ))
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(0).id())
                                                .content(calculatorResult)
                                                .build()
                                )
                        ))
                        .addAssistantMessageOfBetaContentBlockParams(combinedContent)
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(1).id())
                                                .content(databaseResult)
                                                .build()
                                )
                        ))
                        .build()
        );

        System.out.println("\nAfter database result:");
        // 使用交错思考，Claude 可以思考两个结果
        // 然后制定最终响应
        for (BetaContentBlock block : response3.content()) {
            if (block.isThinking()) {
                System.out.println("Final thinking: " + block.asThinking().thinking());
            } else if (block.isText()) {
                System.out.println("Final response: " + block.asText().text());
            }
        }
    }
}
```
</CodeGroup>

在这个使用交错思考的示例中：
1. Claude 最初思考任务
2. 收到计算器结果后，Claude 可以再次思考该结果的含义
3. Claude 然后根据第一个结果决定如何查询数据库
4. 收到数据库结果后，Claude 再思考一次两个结果，然后制定最终响应
5. 思考预算分布在该轮次内的所有思考块中

这种模式允许更复杂的推理链，其中每个工具的输出为下一个决策提供信息。

</section>

## 扩展思考与提示缓存

[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)与思考有几个重要的注意事项：

<Tip>
扩展思考任务通常需要超过 5 分钟才能完成。考虑使用[1 小时缓存持续时间](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration)来维护跨越较长思考会话和多步骤工作流的缓存命中。
</Tip>

**思考块上下文移除**
- 来自先前轮次的思考块从上下文中移除，这可能会影响缓存断点
- 继续使用工具的对话时，思考块被缓存并在从缓存读取时计为输入令牌
- 这产生了一个权衡：虽然思考块在视觉上不消耗上下文窗口空间，但在缓存时仍然计入您的输入令牌使用
- 如果思考被禁用，如果您在当前工具使用轮次中传递思考内容，请求将失败。在其他情况下，传递给 API 的思考内容将被忽略

**缓存失效模式**
- 思考参数的更改（启用/禁用或预算分配）会使消息缓存断点失效
- [交错思考](#interleaved-thinking)放大了缓存失效，因为思考块可能发生在多个[工具调用](#extended-thinking-with-tool-use)之间
- 系统提示和工具保持缓存，尽管思考参数更改或块移除

<Note>
虽然思考块被移除用于缓存和上下文计算，但在继续使用[工具使用](#extended-thinking-with-tool-use)的对话时必须保留它们，特别是使用[交错思考](#interleaved-thinking)时。
</Note>

## 扩展思考与提示缓存

[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)与思考有几个重要的考虑事项：

<Tip>
扩展思考任务通常需要超过5分钟才能完成。考虑使用[1小时缓存时长](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration)来维持跨越更长思考会话和多步骤工作流的缓存命中。
</Tip>

**思考块上下文移除**
- 来自前面轮次的思考块会从上下文中移除，这可能会影响缓存断点
- 当继续使用工具的对话时，思考块会被缓存，并在从缓存读取时计为输入令牌
- 这产生了一个权衡：虽然思考块在视觉上不消耗上下文窗口空间，但在缓存时仍然计入您的输入令牌使用量
- 如果思考被禁用，如果您在当前工具使用轮次中传递思考内容，请求将失败。在其他上下文中，传递给API的思考内容会被简单地忽略

**缓存失效模式**
- 对思考参数的更改（启用/禁用或预算分配）会使消息缓存断点失效
- [交错思考](#interleaved-thinking)会放大缓存失效，因为思考块可能出现在多个[工具调用](#extended-thinking-with-tool-use)之间
- 系统提示和工具尽管思考参数更改或块移除，仍保持缓存

<Note>
虽然思考块会因缓存和上下文计算而被移除，但在继续使用[工具使用](#extended-thinking-with-tool-use)的对话时，特别是使用[交错思考](#interleaved-thinking)时，必须保留它们。
</Note>

### 理解思考块缓存行为

当使用扩展思考与工具使用时，思考块表现出特定的缓存行为，这会影响令牌计数：

**工作原理：**

1. 仅当您发出包含工具结果的后续请求时，才会发生缓存
2. 当发出后续请求时，之前的对话历史（包括思考块）可以被缓存
3. 这些缓存的思考块在从缓存读取时计为您使用指标中的输入令牌
4. 当包含非工具结果用户块时，所有之前的思考块都会被忽略并从上下文中移除

**详细示例流程：**

**请求1：**
```
User: "What's the weather in Paris?"
```
**响应1：**
```
[thinking_block_1] + [tool_use block 1]
```

**请求2：**
```
User: ["What's the weather in Paris?"], 
Assistant: [thinking_block_1] + [tool_use block 1], 
User: [tool_result_1, cache=True]
```
**响应2：**
```
[thinking_block_2] + [text block 2]
```
请求2写入请求内容的缓存（不是响应）。缓存包括原始用户消息、第一个思考块、工具使用块和工具结果。

**请求3：**
```
User: ["What's the weather in Paris?"],
Assistant: [thinking_block_1] + [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [thinking_block_2] + [text block 2],
User: [Text response, cache=True]
```
对于Claude Opus 4.5及更高版本，默认情况下保留所有之前的思考块。对于较旧的模型，由于包含了非工具结果用户块，所有之前的思考块都会被忽略。此请求将按以下方式处理：
```
User: ["What's the weather in Paris?"],
Assistant: [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [text block 2],
User: [Text response, cache=True]
```

**关键点：**
- 此缓存行为会自动发生，即使没有显式的`cache_control`标记
- 此行为在使用常规思考或交错思考时是一致的

<section title="系统提示缓存（思考更改时保留）">

<CodeGroup>
```python Python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

SYSTEM_PROMPT=[
    {
        "type": "text",
        "text": "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
    },
    {
        "type": "text",
        "text": LARGE_TEXT,
        "cache_control": {"type": "ephemeral"}
    }
]

MESSAGES = [
    {
        "role": "user",
        "content": "Analyze the tone of this passage."
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

# Third request - different thinking parameters (cache miss for messages)
print("\nThird request - different thinking parameters (cache miss for messages)")
response3 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Changed thinking budget
    },
    system=SYSTEM_PROMPT,  # System prompt remains cached
    messages=MESSAGES  # Messages cache is invalidated
)

print(f"Third response usage: {response3.usage}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import axios from 'axios';
import * as cheerio from 'cheerio';

const client = new Anthropic();

async function fetchArticleContent(url: string): Promise<string> {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);
  
  // Remove script and style elements
  $('script, style').remove();
  
  // Get text
  let text = $.text();
  
  // Break into lines and remove leading and trailing space on each
  const lines = text.split('\n').map(line => line.trim());
  // Drop blank lines
  text = lines.filter(line => line.length > 0).join('\n');
  
  return text;
}

// Fetch the content of the article
const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
const bookContent = await fetchArticleContent(bookUrl);
// Use just enough text for caching (first few chapters)
const LARGE_TEXT = bookContent.slice(0, 5000);

const SYSTEM_PROMPT = [
  {
    type: "text",
    text: "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
  },
  {
    type: "text",
    text: LARGE_TEXT,
    cache_control: { type: "ephemeral" }
  }
];

const MESSAGES = [
  {
    role: "user",
    content: "Analyze the tone of this passage."
  }
];

// First request - establish cache
console.log("First request - establishing cache");
const response1 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 4000
  },
  system: SYSTEM_PROMPT,
  messages: MESSAGES
});

console.log(`First response usage: ${response1.usage}`);

MESSAGES.push({
  role: "assistant",
  content: response1.content
});
MESSAGES.push({
  role: "user",
  content: "Analyze the characters in this passage."
});

// Second request - same thinking parameters (cache hit expected)
console.log("\nSecond request - same thinking parameters (cache hit expected)");
const response2 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 4000
  },
  system: SYSTEM_PROMPT,
  messages: MESSAGES
});

console.log(`Second response usage: ${response2.usage}`);

// Third request - different thinking parameters (cache miss for messages)
console.log("\nThird request - different thinking parameters (cache miss for messages)");
const response3 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 8000  // Changed thinking budget
  },
  system: SYSTEM_PROMPT,  // System prompt remains cached
  messages: MESSAGES  // Messages cache is invalidated
});

console.log(`Third response usage: ${response3.usage}`);
```
</CodeGroup>

</section>
<section title="消息缓存（思考更改时失效）">

<CodeGroup>
```python Python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

# No system prompt - caching in messages instead
MESSAGES = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": LARGE_TEXT,
                "cache_control": {"type": "ephemeral"},
            },
            {
                "type": "text",
                "text": "Analyze the tone of this passage."
            }
        ]
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000  # Same thinking budget
    },
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response2.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the setting in this passage."
})

# Third request - different thinking budget (cache miss expected)
print("\nThird request - different thinking budget (cache miss expected)")
response3 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Different thinking budget breaks cache
    },
    messages=MESSAGES
)

print(f"Third response usage: {response3.usage}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import axios from 'axios';
import * as cheerio from 'cheerio';

const client = new Anthropic();

async function fetchArticleContent(url: string): Promise<string> {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);

  // Remove script and style elements
  $('script, style').remove();

  // Get text
  let text = $.text();

  // Clean up text (break into lines, remove whitespace)
  const lines = text.split('\n').map(line => line.trim());
  const chunks = lines.flatMap(line => line.split('  ').map(phrase => phrase.trim()));
  text = chunks.filter(chunk => chunk).join('\n');

  return text;
}

async function main() {
  // Fetch the content of the article
  const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
  const bookContent = await fetchArticleContent(bookUrl);
  // Use just enough text for caching (first few chapters)
  const LARGE_TEXT = bookContent.substring(0, 5000);

  // No system prompt - caching in messages instead
  let MESSAGES = [
    {
      role: "user",
      content: [
        {
          type: "text",
          text: LARGE_TEXT,
          cache_control: {type: "ephemeral"},
        },
        {
          type: "text",
          text: "Analyze the tone of this passage."
        }
      ]
    }
  ];

  // First request - establish cache
  console.log("First request - establishing cache");
  const response1 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 4000
    },
    messages: MESSAGES
  });

  console.log(`First response usage: `, response1.usage);

  MESSAGES = [
    ...MESSAGES,
    {
      role: "assistant",
      content: response1.content
    },
    {
      role: "user",
      content: "Analyze the characters in this passage."
    }
  ];

  // Second request - same thinking parameters (cache hit expected)
  console.log("\nSecond request - same thinking parameters (cache hit expected)");
  const response2 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 4000  // Same thinking budget
    },
    messages: MESSAGES
  });

  console.log(`Second response usage: `, response2.usage);

  MESSAGES = [
    ...MESSAGES,
    {
      role: "assistant",
      content: response2.content
    },
    {
      role: "user",
      content: "Analyze the setting in this passage."
    }
  ];

  // Third request - different thinking budget (cache miss expected)
  console.log("\nThird request - different thinking budget (cache miss expected)");
  const response3 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 8000  // Different thinking budget breaks cache
    },
    messages: MESSAGES
  });

  console.log(`Third response usage: `, response3.usage);
}

main().catch(console.error);
```

```java Java
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.net.URL;
import java.util.Arrays;
import java.util.regex.Pattern;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;

import static java.util.stream.Collectors.joining;
import static java.util.stream.Collectors.toList;

public class ThinkingCacheExample {
    public static void main(String[] args) throws IOException {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // Fetch the content of the article
        String bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
        String bookContent = fetchArticleContent(bookUrl);
        // Use just enough text for caching (first few chapters)
        String largeText = bookContent.substring(0, 5000);

        List<BetaTextBlockParam> systemPrompt = List.of(
                BetaTextBlockParam.builder()
                        .text("You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.")
                        .build(),
                BetaTextBlockParam.builder()
                        .text(largeText)
                        .cacheControl(BetaCacheControlEphemeral.builder().build())
                        .build()
        );

        List<BetaMessageParam> messages = new ArrayList<>();
        messages.add(BetaMessageParam.builder()
                .role(BetaMessageParam.Role.USER)
                .content("Analyze the tone of this passage.")
                .build());

        // First request - establish cache
        System.out.println("First request - establishing cache");
        BetaMessage response1 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .messages(messages)
                        .build()
        );

        System.out.println("First response usage: " + response1.usage());

        // Second request - same thinking parameters (cache hit expected)
        System.out.println("\nSecond request - same thinking parameters (cache hit expected)");
        BetaMessage response2 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .addMessage(response1)
                        .addUserMessage("Analyze the characters in this passage.")
                        .messages(messages)
                        .build()
        );

        System.out.println("Second response usage: " + response2.usage());

        // Third request - different thinking budget (cache hit expected because system prompt caching)
        System.out.println("\nThird request - different thinking budget (cache hit expected)");
        BetaMessage response3 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(8000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .addMessage(response1)
                        .addUserMessage("Analyze the characters in this passage.")
                        .addMessage(response2)
                        .addUserMessage("Analyze the setting in this passage.")
                        .build()
        );

        System.out.println("Third response usage: " + response3.usage());
    }

    private static String fetchArticleContent(String url) throws IOException {
        // Fetch HTML content
        String htmlContent = fetchHtml(url);

        // Remove script and style elements
        String noScriptStyle = removeElements(htmlContent, "script", "style");

        // Extract text (simple approach - remove HTML tags)
        String text = removeHtmlTags(noScriptStyle);

        // Clean up text (break into lines, remove whitespace)
        List<String> lines = Arrays.asList(text.split("\n"));
        List<String> trimmedLines = lines.stream()
                .map(String::trim)
                .collect(toList());

        // Split on double spaces and flatten
        List<String> chunks = trimmedLines.stream()
                .flatMap(line -> Arrays.stream(line.split("  "))
                        .map(String::trim))
                .collect(toList());

        // Filter empty chunks and join with newlines
        return chunks.stream()
                .filter(chunk -> !chunk.isEmpty())
                .collect(joining("\n"));
    }

    /**
     * Fetches HTML content from a URL
     */
    private static String fetchHtml(String urlString) throws IOException {
        try (InputStream inputStream = new URL(urlString).openStream()) {
            StringBuilder content = new StringBuilder();
            try (BufferedReader reader = new BufferedReader(
                    new InputStreamReader(inputStream))) {
                String line;
                while ((line = reader.readLine()) != null) {
                    content.append(line).append("\n");
                }
            }
            return content.toString();
        }
    }

    /**
     * Removes specified HTML elements and their content
     */
    private static String removeElements(String html, String... elementNames) {
        String result = html;
        for (String element : elementNames) {
            // Pattern to match <element>...</element> and self-closing tags
            String pattern = "<" + element + "\\s*[^>]*>.*?</" + element + ">|<" + element + "\\s*[^>]*/?>";
            result = Pattern.compile(pattern, Pattern.DOTALL).matcher(result).replaceAll("");
        }
        return result;
    }

    /**
     * Removes all HTML tags from content
     */
    private static String removeHtmlTags(String html) {
        // Replace <br> and <p> tags with newlines for better text formatting
        String withLineBreaks = html.replaceAll("<br\\s*/?\\s*>|</?p\\s*[^>]*>", "\n");

        // Remove remaining HTML tags
        String noTags = withLineBreaks.replaceAll("<[^>]*>", "");

        // Decode HTML entities (simplified for common entities)
        return decodeHtmlEntities(noTags);
    }

    /**
     * Simple HTML entity decoder for common entities
     */
    private static String decodeHtmlEntities(String text) {
        return text
                .replaceAll("&nbsp;", " ")
                .replaceAll("&amp;", "&")
                .replaceAll("&lt;", "<")
                .replaceAll("&gt;", ">")
                .replaceAll("&quot;", "\"")
                .replaceAll("&#39;", "'")
                .replaceAll("&hellip;", "...")
                .replaceAll("&mdash;", "—");
    }

}
```
</CodeGroup>

脚本的输出如下（您可能会看到略有不同的数字）

```
First request - establishing cache
First response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }

Second request - same thinking parameters (cache hit expected)

Second response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }

Third request - different thinking budget (cache miss expected)
Third response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }
```

此示例演示了当缓存在消息数组中设置时，更改思考参数（预算令牌从4000增加到8000）**使缓存失效**。第三个请求显示没有缓存命中，`cache_creation_input_tokens=1370`和`cache_read_input_tokens=0`，证明当思考参数更改时，基于消息的缓存会失效。

</section>

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 扩展思考与工具使用中的上下文窗口

当使用扩展思考与工具使用时，思考块必须显式保留并与工具结果一起返回。

扩展思考与工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下图说明了扩展思考与工具使用的令牌管理：

![扩展思考和工具使用的上下文窗口图](/docs/images/context-window-thinking-tools.svg)

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 扩展思考与工具使用中的上下文窗口

当使用扩展思考与工具使用时，思考块必须显式保留并与工具结果一起返回。

扩展思考与工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下图说明了扩展思考与工具使用的令牌管理：

![扩展思考和工具使用的上下文窗口图](/docs/images/context-window-thinking-tools.svg)

### 使用扩展思考管理令牌

鉴于Claude 3.7和4模型的上下文窗口和`max_tokens`行为与扩展思考，您可能需要：

- 更积极地监控和管理您的令牌使用
- 随着提示长度的变化调整`max_tokens`值
- 可能更频繁地使用[令牌计数端点](/docs/zh-CN/build-with-claude/token-counting)
- 意识到之前的思考块不会在您的上下文窗口中累积

这一变化是为了提供更可预测和透明的行为，特别是随着最大令牌限制显著增加。

## 最大令牌数和扩展思考的上下文窗口大小

在较早的 Claude 模型（Claude Sonnet 3.7 之前），如果提示令牌和 `max_tokens` 的总和超过了模型的上下文窗口，系统会自动调整 `max_tokens` 以适应上下文限制。这意味着您可以设置一个较大的 `max_tokens` 值，系统会根据需要静默地减少它。

使用 Claude 3.7 和 4 模型时，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌 + `max_tokens` 超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 启用扩展思考的上下文窗口

在计算启用思考的上下文窗口使用情况时，需要注意以下几点：

- 来自先前轮次的思考块被剥离，不计入您的上下文窗口
- 当前轮次的思考计入该轮次的 `max_tokens` 限制

下面的图表演示了启用扩展思考时的专门令牌管理：

![启用扩展思考的上下文窗口图表](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数 API](/docs/zh-CN/build-with-claude/token-counting)来获得您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 启用扩展思考和工具使用的上下文窗口

使用扩展思考和工具使用时，思考块必须被显式保留并与工具结果一起返回。

启用扩展思考和工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下面的图表说明了扩展思考和工具使用的令牌管理：

![启用扩展思考和工具使用的上下文窗口图表](/docs/images/context-window-thinking-tools.svg)

### 使用扩展思考管理令牌

考虑到 Claude 3.7 和 4 模型的上下文窗口和 `max_tokens` 行为与扩展思考的关系，您可能需要：

- 更主动地监控和管理您的令牌使用
- 随着提示长度的变化调整 `max_tokens` 值
- 可能需要更频繁地使用[令牌计数端点](/docs/zh-CN/build-with-claude/token-counting)
- 意识到先前的思考块不会在您的上下文窗口中累积

做出这一改变是为了提供更可预测和透明的行为，特别是因为最大令牌限制已大幅增加。

## 思考加密

完整的思考内容被加密并在 `signature` 字段中返回。该字段用于验证思考块是由 Claude 生成的，当传回 API 时使用。

<Note>
只有在使用[带扩展思考的工具](#extended-thinking-with-tool-use)时才严格需要发送回思考块。否则，您可以省略先前轮次的思考块，或者如果您传回它们，让 API 为您剥离它们。

如果发送回思考块，我们建议按照您接收的方式传回所有内容以保持一致性并避免潜在问题。
</Note>

以下是关于思考加密的一些重要注意事项：
- 当[流式传输响应](#streaming-thinking)时，签名通过 `content_block_delta` 事件中的 `signature_delta` 添加，位于 `content_block_stop` 事件之前。
- `signature` 值在 Claude 4 模型中的长度明显长于之前的模型。
- `signature` 字段是一个不透明字段，不应被解释或解析 - 它仅用于验证目的。
- `signature` 值在各平台上兼容（Claude API、[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock) 和 [Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)）。在一个平台上生成的值将与另一个平台兼容。

### 思考编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密 `thinking` 块的部分或全部内容，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 意识到编辑的思考块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的一些内部推理已自动加密以确保安全。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉编辑的块，同时保留正常的思考块
- 透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理以优雅地管理编辑的思考，而不会破坏您的 UI

以下是显示正常和编辑思考块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到编辑的思考块是预期的行为。该模型仍然可以使用这个编辑的推理来为其响应提供信息，同时保持安全护栏。

如果您需要在应用程序中测试编辑的思考处理，您可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手轮次的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用编辑的思考块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发编辑的思考（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别编辑的思考块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块仍然可用于后续请求

    # 提取所有块（编辑和非编辑）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 在传递到后续请求时，包括所有块而不进行修改
    # 这保留了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发编辑的思考（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别编辑的思考块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块仍然可用于后续请求

  // 提取所有块（编辑和非编辑）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 在传递到后续请求时，包括所有块而不进行修改
  // 这保留了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发编辑的思考（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别编辑的思考块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块仍然可用于后续请求
            // 提取所有块（编辑和非编辑）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 在传递到后续请求时，包括所有块而不进行修改
            // 这保留了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

### 思考编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密 `thinking` 块的部分或全部内容，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 意识到编辑的思考块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的一些内部推理已自动加密以确保安全。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉编辑的块，同时保留正常的思考块
- 透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理以优雅地管理编辑的思考，而不会破坏您的 UI

以下是显示正常和编辑思考块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到编辑的思考块是预期的行为。该模型仍然可以使用这个编辑的推理来为其响应提供信息，同时保持安全护栏。

如果您需要在应用程序中测试编辑的思考处理，您可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手轮次的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用编辑的思考块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发编辑的思考（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别编辑的思考块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块仍然可用于后续请求

    # 提取所有块（编辑和非编辑）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 在传递到后续请求时，包括所有块而不进行修改
    # 这保留了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发编辑的思考（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别编辑的思考块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块仍然可用于后续请求

  // 提取所有块（编辑和非编辑）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 在传递到后续请求时，包括所有块而不进行修改
  // 这保留了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发编辑的思考（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别编辑的思考块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块仍然可用于后续请求
            // 提取所有块（编辑和非编辑）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 在传递到后续请求时，包括所有块而不进行修改
            // 这保留了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

## 不同模型版本之间的思考差异

Messages API 在 Claude Sonnet 3.7 和 Claude 4 模型之间处理思考的方式不同，主要在编辑和总结行为方面。

请参阅下表以获得简明比较：

| 功能 | Claude Sonnet 3.7 | Claude 4 模型（Opus 4.5 之前） | Claude Opus 4.5 及更高版本 |
|---------|------------------|-------------------------------|--------------------------|
| **思考输出** | 返回完整思考输出 | 返回总结的思考 | 返回总结的思考 |
| **交错思考** | 不支持 | 支持 `interleaved-thinking-2025-05-14` beta 标头 | 支持 `interleaved-thinking-2025-05-14` beta 标头 |
| **思考块保留** | 不跨轮保留 | 不跨轮保留 | **默认保留**（启用缓存优化、令牌节省） |

### Claude Opus 4.5 中的思考块保留

Claude Opus 4.5 引入了一个新的默认行为：**来自先前助手轮次的思考块默认在模型上下文中保留**。这与较早的模型不同，较早的模型会删除先前轮次的思考块。

**思考块保留的好处：**

- **缓存优化**：使用工具使用时，保留的思考块启用缓存命中，因为它们与工具结果一起传回并在助手轮次中增量缓存，在多步工作流中产生令牌节省
- **无智能影响**：保留思考块对模型性能没有负面影响

**重要注意事项：**

- **上下文使用**：长对话将消耗更多上下文空间，因为思考块保留在上下文中
- **自动行为**：这是 Claude Opus 4.5 的默认行为 - 不需要代码更改或 beta 标头
- **向后兼容性**：要利用此功能，继续将完整的、未修改的思考块传回 API，就像您对工具使用所做的那样

<Note>
对于较早的模型（Claude Sonnet 4.5、Opus 4.1 等），来自先前轮次的思考块继续从上下文中删除。[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分中描述的现有行为适用于这些模型。
</Note>

### 思考内容编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密部分或全部 `thinking` 块，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时会被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 请注意 redacted thinking 块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的某些内部推理已因安全原因自动加密。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉 redacted 块，同时保留正常的思考块
- 要透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理，以优雅地管理 redacted thinking，而不会破坏您的 UI

以下是显示正常和 redacted thinking 块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到 redacted thinking 块是预期的行为。该模型仍然可以使用这个 redacted 推理来为其响应提供信息，同时维护安全护栏。

如果您需要在应用程序中测试 redacted thinking 处理，可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手转向的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用 redacted thinking 块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发 redacted thinking（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别 redacted thinking 块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块在后续请求中仍然可用

    # 提取所有块（redacted 和非 redacted）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 传递到后续请求时，包括所有块而不进行修改
    # 这保持了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发 redacted thinking（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别 redacted thinking 块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块在后续请求中仍然可用

  // 提取所有块（redacted 和非 redacted）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 传递到后续请求时，包括所有块而不进行修改
  // 这保持了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发 redacted thinking（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别 redacted thinking 块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块在后续请求中仍然可用
            // 提取所有块（redacted 和非 redacted）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 传递到后续请求时，包括所有块而不进行修改
            // 这保持了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

## 不同模型版本中的思考差异

Messages API 在 Claude Sonnet 3.7 和 Claude 4 模型中处理思考的方式不同，主要在编辑和总结行为方面。

请参阅下表了解简明比较：

| 功能 | Claude Sonnet 3.7 | Claude 4 模型（Opus 4.5 之前） | Claude Opus 4.5 及更高版本 |
|---------|------------------|-------------------------------|--------------------------|
| **思考输出** | 返回完整思考输出 | 返回总结的思考 | 返回总结的思考 |
| **交错思考** | 不支持 | 支持 `interleaved-thinking-2025-05-14` beta 标头 | 支持 `interleaved-thinking-2025-05-14` beta 标头 |
| **思考块保留** | 不跨转向保留 | 不跨转向保留 | **默认保留**（启用缓存优化、节省令牌） |

### Claude Opus 4.5 中的思考块保留

Claude Opus 4.5 引入了一个新的默认行为：**来自先前助手转向的思考块默认在模型上下文中保留**。这与早期模型不同，早期模型会删除先前转向的思考块。

**思考块保留的好处：**

- **缓存优化**：使用工具使用时，保留的思考块启用缓存命中，因为它们与工具结果一起传回并在助手转向中增量缓存，在多步工作流中节省令牌
- **无智能影响**：保留思考块对模型性能没有负面影响

**重要考虑事项：**

- **上下文使用**：长对话将消耗更多上下文空间，因为思考块保留在上下文中
- **自动行为**：这是 Claude Opus 4.5 的默认行为——不需要代码更改或 beta 标头
- **向后兼容性**：要利用此功能，继续将完整的、未修改的思考块传回 API，就像您对工具使用所做的那样

<Note>
对于早期模型（Claude Sonnet 4.5、Opus 4.1 等），来自先前转向的思考块继续从上下文中删除。[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分中描述的现有行为适用于这些模型。
</Note>

## 定价

有关完整的定价信息，包括基础费率、缓存写入、缓存命中和输出令牌，请参阅[定价页面](/docs/zh-CN/about-claude/pricing)。

思考过程产生的费用包括：
- 思考期间使用的令牌（输出令牌）
- 后续请求中包含的最后一个助手转向的思考块（输入令牌）
- 标准文本输出令牌

<Note>
启用扩展思考时，会自动包含专门的系统提示以支持此功能。
</Note>

使用总结思考时：
- **输入令牌**：您原始请求中的令牌（不包括先前转向的思考令牌）
- **输出令牌（计费）**：Claude 内部生成的原始思考令牌
- **输出令牌（可见）**：您在响应中看到的总结思考令牌
- **无费用**：用于生成摘要的令牌

<Warning>
计费的输出令牌计数将**不**与响应中的可见令牌计数匹配。您需要为完整的思考过程付费，而不是您看到的摘要。
</Warning>

## 扩展思考的最佳实践和注意事项

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

### 使用指南

- **任务选择**：对于特别复杂的任务使用扩展思考，这些任务受益于逐步推理，如数学、编码和分析。
- **上下文处理**：您不需要自己删除以前的思考块。Claude API 会自动忽略先前转向的思考块，在计算上下文使用时不包括它们。
- **提示工程**：如果您想最大化 Claude 的思考能力，请查看我们的[扩展思考提示技巧](/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips)。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

### 使用指南

- **任务选择**：对于特别复杂的任务使用扩展思考，这些任务受益于逐步推理，如数学、编码和分析。
- **上下文处理**：您不需要自己删除以前的思考块。Claude API 会自动忽略先前转向的思考块，在计算上下文使用时不包括它们。
- **提示工程**：如果您想最大化 Claude 的思考能力，请查看我们的[扩展思考提示技巧](/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips)。

## 后续步骤

<CardGroup>
  <Card title="尝试扩展思考食谱" icon="book" href="https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking">
    在我们的食谱中探索思考的实际示例。
  </Card>
  <Card title="扩展思考提示技巧" icon="code" href="/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips">
    学习扩展思考的提示工程最佳实践。
  </Card>
</CardGroup>


# 使用扩展思考进行构建

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

---

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

### 总结的思考

启用扩展思考后，Claude 4 模型的消息 API 会返回 Claude 完整思考过程的摘要。总结的思考提供了扩展思考的全部智能优势，同时防止滥用。

以下是关于总结思考的一些重要注意事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了特别有助于提示工程的详细推理。
- 随着 Anthropic 寻求改进扩展思考功能，总结行为可能会发生变化。
- 总结保留了 Claude 思考过程的关键思想，延迟最少，实现了可流式传输的用户体验，并便于从 Claude Sonnet 3.7 迁移到 Claude 4 模型。
- 总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

### 总结的思考

启用扩展思考后，Claude 4 模型的消息 API 会返回 Claude 完整思考过程的摘要。总结的思考提供了扩展思考的全部智能优势，同时防止滥用。

以下是关于总结思考的一些重要注意事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了特别有助于提示工程的详细推理。
- 随着 Anthropic 寻求改进扩展思考功能，总结行为可能会发生变化。
- 总结保留了 Claude 思考过程的关键思想，延迟最少，实现了可流式传输的用户体验，并便于从 Claude Sonnet 3.7 迁移到 Claude 4 模型。
- 总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

### 流式传输思考

您可以使用[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) 流式传输扩展思考响应。

启用扩展思考的流式传输后，您会通过 `thinking_delta` 事件接收思考内容。

有关通过消息 API 进行流式传输的更多文档，请参阅[流式传输消息](/docs/zh-CN/build-with-claude/streaming)。

以下是如何处理思考流式传输的方法：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "stream": true,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "What is 27 * 453?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={"type": "enabled", "budget_tokens": 10000},
    messages=[{"role": "user", "content": "What is 27 * 453?"}],
) as stream:
    thinking_started = False
    response_started = False

    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
            # 为每个新块重置标志
            thinking_started = False
            response_started = False
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                if not thinking_started:
                    print("Thinking: ", end="", flush=True)
                    thinking_started = True
                print(event.delta.thinking, end="", flush=True)
            elif event.delta.type == "text_delta":
                if not response_started:
                    print("Response: ", end="", flush=True)
                    response_started = True
                print(event.delta.text, end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const stream = await client.messages.stream({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "What is 27 * 453?"
  }]
});

let thinkingStarted = false;
let responseStarted = false;

for await (const event of stream) {
  if (event.type === 'content_block_start') {
    console.log(`\nStarting ${event.content_block.type} block...`);
    // 为每个新块重置标志
    thinkingStarted = false;
    responseStarted = false;
  } else if (event.type === 'content_block_delta') {
    if (event.delta.type === 'thinking_delta') {
      if (!thinkingStarted) {
        process.stdout.write('Thinking: ');
        thinkingStarted = true;
      }
      process.stdout.write(event.delta.thinking);
    } else if (event.delta.type === 'text_delta') {
      if (!responseStarted) {
        process.stdout.write('Response: ');
        responseStarted = true;
      }
      process.stdout.write(event.delta.text);
    }
  } else if (event.type === 'content_block_stop') {
    console.log('\nBlock complete.');
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.http.StreamResponse;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class SimpleThinkingStreamingExample {
    private static boolean thinkingStarted = false;
    private static boolean responseStarted = false;
    
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams createParams = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_0)
                .maxTokens(16000)
                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                .addUserMessage("What is 27 * 453?")
                .build();

        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                     client.beta().messages().createStreaming(createParams)) {
            streamResponse.stream()
                    .forEach(event -> {
                        if (event.isContentBlockStart()) {
                            System.out.printf("\nStarting %s block...%n",
                                    event.asContentBlockStart()._type());
                            // 为每个新块重置标志
                            thinkingStarted = false;
                            responseStarted = false;
                        } else if (event.isContentBlockDelta()) {
                            var delta = event.asContentBlockDelta().delta();
                            if (delta.isBetaThinking()) {
                                if (!thinkingStarted) {
                                    System.out.print("Thinking: ");
                                    thinkingStarted = true;
                                }
                                System.out.print(delta.asBetaThinking().thinking());
                                System.out.flush();
                            } else if (delta.isBetaText()) {
                                if (!responseStarted) {
                                    System.out.print("Response: ");
                                    responseStarted = true;
                                }
                                System.out.print(delta.asBetaText().text());
                                System.out.flush();
                            }
                        } else if (event.isContentBlockStop()) {
                            System.out.println("\nBlock complete.");
                        }
                    });
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  在控制台中尝试
</TryInConsoleButton>

示例流式传输输出：
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "让我逐步解决这个问题：\n\n1. 首先分解 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// 其他思考增量...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// 其他文本增量...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

<Note>
使用启用思考的流式传输时，您可能会注意到文本有时以较大的块到达，交替出现较小的逐令牌传递。这是预期的行为，特别是对于思考内容。

流式传输系统需要分批处理内容以获得最佳性能，这可能导致这种"分块"传递模式，流式传输事件之间可能出现延迟。我们正在不断努力改进这种体验，未来的更新将专注于使思考内容流式传输更加平顺。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于处理复杂任务，同时在提供最终答案之前提供了对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制作最终响应之前会结合来自此推理的见解。

API 响应将包括 `thinking` 内容块，然后是 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅 [Messages API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在 Messages API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# The response will contain summarized thinking blocks and text blocks
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// The response will contain summarized thinking blocks and text blocks
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不是[摘要输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变为您的整个上下文窗口（200k 令牌）。

### 摘要思考

启用扩展思考后，Claude 4 模型的 Messages API 返回 Claude 完整思考过程的摘要。摘要思考提供了扩展思考的全部智能优势，同时防止了滥用。

以下是摘要思考的一些重要考虑事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了详细的推理，这对提示工程目的特别有帮助。
- 随着 Anthropic 寻求改进扩展思考功能，摘要行为可能会发生变化。
- 摘要保留了 Claude 思考过程的关键思想，同时增加了最少的延迟，实现了可流式传输的用户体验和从 Claude Sonnet 3.7 到 Claude 4 模型的轻松迁移。
- 摘要由与您在请求中指定的模型不同的模型处理。思考模型看不到摘要输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

### 流式思考

您可以使用[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) 流式传输扩展思考响应。

启用扩展思考的流式传输后，您会通过 `thinking_delta` 事件接收思考内容。

有关通过 Messages API 进行流式传输的更多文档，请参阅[流式传输消息](/docs/zh-CN/build-with-claude/streaming)。

以下是如何处理思考流式传输的方法：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "stream": true,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "What is 27 * 453?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={"type": "enabled", "budget_tokens": 10000},
    messages=[{"role": "user", "content": "What is 27 * 453?"}],
) as stream:
    thinking_started = False
    response_started = False

    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
            # Reset flags for each new block
            thinking_started = False
            response_started = False
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                if not thinking_started:
                    print("Thinking: ", end="", flush=True)
                    thinking_started = True
                print(event.delta.thinking, end="", flush=True)
            elif event.delta.type == "text_delta":
                if not response_started:
                    print("Response: ", end="", flush=True)
                    response_started = True
                print(event.delta.text, end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const stream = await client.messages.stream({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "What is 27 * 453?"
  }]
});

let thinkingStarted = false;
let responseStarted = false;

for await (const event of stream) {
  if (event.type === 'content_block_start') {
    console.log(`\nStarting ${event.content_block.type} block...`);
    // Reset flags for each new block
    thinkingStarted = false;
    responseStarted = false;
  } else if (event.type === 'content_block_delta') {
    if (event.delta.type === 'thinking_delta') {
      if (!thinkingStarted) {
        process.stdout.write('Thinking: ');
        thinkingStarted = true;
      }
      process.stdout.write(event.delta.thinking);
    } else if (event.delta.type === 'text_delta') {
      if (!responseStarted) {
        process.stdout.write('Response: ');
        responseStarted = true;
      }
      process.stdout.write(event.delta.text);
    }
  } else if (event.type === 'content_block_stop') {
    console.log('\nBlock complete.');
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.http.StreamResponse;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class SimpleThinkingStreamingExample {
    private static boolean thinkingStarted = false;
    private static boolean responseStarted = false;
    
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams createParams = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_0)
                .maxTokens(16000)
                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                .addUserMessage("What is 27 * 453?")
                .build();

        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                     client.beta().messages().createStreaming(createParams)) {
            streamResponse.stream()
                    .forEach(event -> {
                        if (event.isContentBlockStart()) {
                            System.out.printf("\nStarting %s block...%n",
                                    event.asContentBlockStart()._type());
                            // Reset flags for each new block
                            thinkingStarted = false;
                            responseStarted = false;
                        } else if (event.isContentBlockDelta()) {
                            var delta = event.asContentBlockDelta().delta();
                            if (delta.isBetaThinking()) {
                                if (!thinkingStarted) {
                                    System.out.print("Thinking: ");
                                    thinkingStarted = true;
                                }
                                System.out.print(delta.asBetaThinking().thinking());
                                System.out.flush();
                            } else if (delta.isBetaText()) {
                                if (!responseStarted) {
                                    System.out.print("Response: ");
                                    responseStarted = true;
                                }
                                System.out.print(delta.asBetaText().text());
                                System.out.flush();
                            }
                        } else if (event.isContentBlockStop()) {
                            System.out.println("\nBlock complete.");
                        }
                    });
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  在控制台中尝试
</TryInConsoleButton>

示例流式输出：
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "Let me solve this step by step:\n\n1. First break down 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// Additional thinking deltas...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// Additional text deltas...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

<Note>
使用启用思考的流式传输时，您可能会注意到文本有时会以较大的块到达，交替出现较小的逐令牌传输。这是预期的行为，特别是对于思考内容。

流式传输系统需要分批处理内容以获得最佳性能，这可能导致这种"块状"传输模式，流式传输事件之间可能出现延迟。我们正在不断努力改进这种体验，未来的更新将专注于使思考内容流式传输更加平稳。
</Note>

## 扩展思考与工具使用

扩展思考可以与[工具使用](/docs/zh-CN/agents-and-tools/tool-use/overview)一起使用，允许 Claude 通过工具选择和结果处理进行推理。

使用扩展思考和工具使用时，请注意以下限制：

1. **工具选择限制**：工具使用与思考仅支持 `tool_choice: {"type": "auto"}` (默认) 或 `tool_choice: {"type": "none"}`。使用 `tool_choice: {"type": "any"}` 或 `tool_choice: {"type": "tool", "name": "..."}` 将导致错误，因为这些选项强制工具使用，这与扩展思考不兼容。

2. **保留思考块**：在工具使用期间，您必须将 `thinking` 块传回 API 以获取最后的助手消息。将完整的未修改块传回 API 以保持推理连续性。

### 在对话中切换思考模式

您不能在助手转向的中间切换思考，包括在工具使用循环期间。整个助手转向必须在单一思考模式下运行：

- **如果启用了思考**，最后的助手转向必须以思考块开始。
- **如果禁用了思考**，最后的助手转向不能包含任何思考块

从模型的角度来看，**工具使用循环是助手转向的一部分**。助手转向直到 Claude 完成其完整响应后才完成，这可能包括多个工具调用和结果。

例如，此序列都是**单个助手转向**的一部分：
```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```

尽管有多个 API 消息，但工具使用循环在概念上是一个连续助手响应的一部分。

#### 常见错误场景

您可能会遇到此错误：
```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```

这通常发生在以下情况：
1. 您在工具使用序列期间**禁用了**思考
2. 您想再次启用思考
3. 您的最后一条助手消息包含工具使用块但没有思考块

#### 实用指南

**✗ 无效：在工具使用后立即切换思考**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```

**✓ 有效：首先完成助手转向**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```

**最佳实践**：在每个转向开始时规划您的思考策略，而不是尝试在中途切换。

<Note>
切换思考模式也会使消息历史的提示缓存失效。有关更多详情，请参阅[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分。
</Note>

<section title="示例：使用工具结果传递思考块">

以下是一个实际示例，展示了在提供工具结果时如何保留思考块：

<CodeGroup>
```python Python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

```typescript TypeScript
const weatherTool = {
  name: "get_weather",
  description: "Get current weather for a location",
  input_schema: {
    type: "object",
    properties: {
      location: { type: "string" }
    },
    required: ["location"]
  }
};

// First request - Claude responds with thinking and tool request
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" }
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.beta.messages.BetaTool;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingWithToolsExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        System.out.println(response);
    }
}
```
</CodeGroup>

API 响应将包括思考、文本和 tool_use 块：

```json
{
    "content": [
        {
            "type": "thinking",
            "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
            "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
        },
        {
            "type": "text",
            "text": "I can help you get the current weather information for Paris. Let me check that for you"
        },
        {
            "type": "tool_use",
            "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
            "name": "get_weather",
            "input": {
                "location": "Paris"
            }
        }
    ]
}
```

现在让我们继续对话并使用该工具

<CodeGroup>
```python Python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

```typescript TypeScript
// Extract thinking block and tool use block
const thinkingBlock = response.content.find(block =>
  block.type === 'thinking');
const toolUseBlock = response.content.find(block =>
  block.type === 'tool_use');

// Call your actual weather API, here is where your actual API call would go
// let's pretend this is what we get back
const weatherData = { temperature: 88 };

// Second request - Include thinking block and tool result
// No new thinking blocks will be generated in the response
const continuation = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" },
    // notice that the thinkingBlock is passed in as well as the toolUseBlock
    // if this is not passed in, an error is raised
    { role: "assistant", content: [thinkingBlock, toolUseBlock] },
    { role: "user", content: [{
      type: "tool_result",
      tool_use_id: toolUseBlock.id,
      content: `Current temperature: ${weatherData.temperature}°F`
    }]}
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;
import java.util.Optional;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingToolsResultExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        // Extract thinking block and tool use block
        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isThinking)
                .map(BetaContentBlock::asThinking)
                .findFirst();

        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isToolUse)
                .map(BetaContentBlock::asToolUse)
                .findFirst();

        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

            // Call your actual weather API, here is where your actual API call would go
            // let's pretend this is what we get back
            Map<String, Object> weatherData = Map.of("temperature", 88);

            // Second request - Include thinking block and tool result
            // No new thinking blocks will be generated in the response
            BetaMessage continuation = client.beta().messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_OPUS_4_0)
                            .maxTokens(16000)
                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                            .addTool(weatherTool)
                            .addUserMessage("What's the weather in Paris?")
                            .addAssistantMessageOfBetaContentBlockParams(
                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                    // if this is not passed in, an error is raised
                                    List.of(
                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                    )
                            )
                            .addUserMessageOfBetaContentBlockParams(List.of(
                                    BetaContentBlockParam.ofToolResult(
                                            BetaToolResultBlockParam.builder()
                                                    .toolUseId(toolUseBlock.id())
                                                    .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                    .build()
                                    )
                            ))
                            .build()
            );

            System.out.println(continuation);
        }
    }
}
```
</CodeGroup>

API 响应现在将**仅**包括文本

```json
{
    "content": [
        {
            "type": "text",
            "text": "Currently in Paris, the temperature is 88°F (31°C)"
        }
    ]
}
```

</section>

### 在对话中切换思考模式

您不能在助手回合的中间切换思考，包括在工具使用循环期间。整个助手回合必须在单一思考模式下运行：

- **如果启用了思考**，最终的助手回合必须以思考块开始。
- **如果禁用了思考**，最终的助手回合不能包含任何思考块

从模型的角度来看，**工具使用循环是助手回合的一部分**。助手回合直到 Claude 完成其完整响应（可能包括多个工具调用和结果）才完成。

例如，这个序列都是**单个助手回合**的一部分：
```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```

尽管有多个 API 消息，但工具使用循环在概念上是一个连续的助手响应的一部分。

#### 常见错误场景

您可能会遇到此错误：
```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```

这通常发生在以下情况：
1. 您在工具使用序列期间**禁用了**思考
2. 您想再次启用思考
3. 您的最后一条助手消息包含工具使用块但没有思考块

#### 实用指导

**✗ 无效：在工具使用后立即切换思考**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```

**✓ 有效：首先完成助手回合**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```

**最佳实践**：在每个回合开始时规划您的思考策略，而不是尝试在中间切换。

<Note>
切换思考模式也会使消息历史的提示缓存失效。有关更多详细信息，请参阅 [使用提示缓存的扩展思考](#extended-thinking-with-prompt-caching) 部分。
</Note>

<section title="示例：使用工具结果传递思考块">

这是一个实际示例，展示了在提供工具结果时如何保留思考块：

<CodeGroup>
```python Python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

```typescript TypeScript
const weatherTool = {
  name: "get_weather",
  description: "Get current weather for a location",
  input_schema: {
    type: "object",
    properties: {
      location: { type: "string" }
    },
    required: ["location"]
  }
};

// First request - Claude responds with thinking and tool request
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" }
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.beta.messages.BetaTool;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingWithToolsExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        System.out.println(response);
    }
}
```
</CodeGroup>

API 响应将包括思考、文本和 tool_use 块：

```json
{
    "content": [
        {
            "type": "thinking",
            "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
            "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
        },
        {
            "type": "text",
            "text": "I can help you get the current weather information for Paris. Let me check that for you"
        },
        {
            "type": "tool_use",
            "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
            "name": "get_weather",
            "input": {
                "location": "Paris"
            }
        }
    ]
}
```

现在让我们继续对话并使用该工具

<CodeGroup>
```python Python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

```typescript TypeScript
// Extract thinking block and tool use block
const thinkingBlock = response.content.find(block =>
  block.type === 'thinking');
const toolUseBlock = response.content.find(block =>
  block.type === 'tool_use');

// Call your actual weather API, here is where your actual API call would go
// let's pretend this is what we get back
const weatherData = { temperature: 88 };

// Second request - Include thinking block and tool result
// No new thinking blocks will be generated in the response
const continuation = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" },
    // notice that the thinkingBlock is passed in as well as the toolUseBlock
    // if this is not passed in, an error is raised
    { role: "assistant", content: [thinkingBlock, toolUseBlock] },
    { role: "user", content: [{
      type: "tool_result",
      tool_use_id: toolUseBlock.id,
      content: `Current temperature: ${weatherData.temperature}°F`
    }]}
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;
import java.util.Optional;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingToolsResultExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        // Extract thinking block and tool use block
        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isThinking)
                .map(BetaContentBlock::asThinking)
                .findFirst();

        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isToolUse)
                .map(BetaContentBlock::asToolUse)
                .findFirst();

        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

            // Call your actual weather API, here is where your actual API call would go
            // let's pretend this is what we get back
            Map<String, Object> weatherData = Map.of("temperature", 88);

            // Second request - Include thinking block and tool result
            // No new thinking blocks will be generated in the response
            BetaMessage continuation = client.beta().messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_OPUS_4_0)
                            .maxTokens(16000)
                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                            .addTool(weatherTool)
                            .addUserMessage("What's the weather in Paris?")
                            .addAssistantMessageOfBetaContentBlockParams(
                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                    // if this is not passed in, an error is raised
                                    List.of(
                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                    )
                            )
                            .addUserMessageOfBetaContentBlockParams(List.of(
                                    BetaContentBlockParam.ofToolResult(
                                            BetaToolResultBlockParam.builder()
                                                    .toolUseId(toolUseBlock.id())
                                                    .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                    .build()
                                    )
                            ))
                            .build()
            );

            System.out.println(continuation);
        }
    }
}
```
</CodeGroup>

API 响应现在**仅**包括文本

```json
{
    "content": [
        {
            "type": "text",
            "text": "Currently in Paris, the temperature is 88°F (31°C)"
        }
    ]
}
```

</section>

### 保留思考块

在工具使用期间，您必须将 `thinking` 块传递回 API，并且必须将完整的未修改块包含回 API。这对于维持模型的推理流和对话完整性至关重要。

<Tip>
虽然您可以从先前的 `assistant` 角色回合中省略 `thinking` 块，但我们建议对于任何多轮对话始终将所有思考块传递回 API。API 将：
- 自动过滤提供的思考块
- 使用必要的相关思考块来保留模型的推理
- 仅对显示给 Claude 的块的输入令牌进行计费
</Tip>

<Note>
在对话期间切换思考模式时，请记住整个助手回合（包括工具使用循环）必须在单一思考模式下运行。有关更多详细信息，请参阅 [在对话中切换思考模式](#toggling-thinking-modes-in-conversations)。
</Note>

当 Claude 调用工具时，它暂停了其响应的构建以等待外部信息。当工具结果返回时，Claude 将继续构建该现有响应。这需要在工具使用期间保留思考块，原因有几个：

1. **推理连续性**：思考块捕获了导致工具请求的 Claude 的逐步推理。当您发布工具结果时，包括原始思考可以确保 Claude 可以从中断的地方继续其推理。

2. **上下文维护**：虽然工具结果在 API 结构中显示为用户消息，但它们是连续推理流的一部分。保留思考块在多个 API 调用中维持这个概念流。有关上下文管理的更多信息，请参阅我们的 [上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)。

**重要**：提供 `thinking` 块时，连续 `thinking` 块的整个序列必须与模型在原始请求期间生成的输出相匹配；您不能重新排列或修改这些块的序列。

### 交错思考

Claude 4 模型中的扩展思考与工具使用支持交错思考，这使 Claude 能够在工具调用之间进行思考，并在接收工具结果后进行更复杂的推理。

通过交错思考，Claude 可以：
- 在工具调用结果后进行推理，然后决定接下来做什么
- 在推理步骤之间链接多个工具调用
- 根据中间结果做出更细致的决策

要启用交错思考，请将[测试版标头](/docs/zh-CN/api/beta-headers) `interleaved-thinking-2025-05-14` 添加到您的 API 请求中。

以下是交错思考的一些重要注意事项：
- 使用交错思考时，`budget_tokens` 可以超过 `max_tokens` 参数，因为它代表一个助手轮次内所有思考块的总预算。
- 交错思考仅支持[通过消息 API 使用的工具](/docs/zh-CN/agents-and-tools/tool-use/overview)。
- 交错思考仅支持 Claude 4 模型，使用测试版标头 `interleaved-thinking-2025-05-14`。
- 直接调用 Claude API 允许您在对任何模型的请求中传递 `interleaved-thinking-2025-05-14`，但不会产生任何效果。
- 在第三方平台上（例如，[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock) 和 [Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)），如果您将 `interleaved-thinking-2025-05-14` 传递给除 Claude Opus 4.5、Claude Opus 4.1、Opus 4 或 Sonnet 4 之外的任何模型，您的请求将失败。

<section title="不使用交错思考的工具使用">

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 定义工具
calculator_tool = {
    "name": "calculator",
    "description": "Perform mathematical calculations",
    "input_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
            }
        },
        "required": ["expression"]
    }
}

database_tool = {
    "name": "database_query",
    "description": "Query product database",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "SQL query to execute"
            }
        },
        "required": ["query"]
    }
}

# 第一个请求 - Claude 在所有工具调用之前思考一次
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    messages=[{
        "role": "user",
        "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    }]
)

# 响应包括思考，然后是工具使用
# 注意：Claude 在开始时思考一次，然后做出所有工具决策
print("First response:")
for block in response.content:
    if block.type == "thinking":
        print(f"Thinking (summarized): {block.thinking}")
    elif block.type == "tool_use":
        print(f"Tool use: {block.name} with input {block.input}")
    elif block.type == "text":
        print(f"Text: {block.text}")

# 您将执行工具并返回结果...
# 获得两个工具结果后，Claude 直接响应而不进行额外思考
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 定义工具
const calculatorTool = {
  name: "calculator",
  description: "Perform mathematical calculations",
  input_schema: {
    type: "object",
    properties: {
      expression: {
        type: "string",
        description: "Mathematical expression to evaluate"
      }
    },
    required: ["expression"]
  }
};

const databaseTool = {
  name: "database_query",
  description: "Query product database",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "SQL query to execute"
      }
    },
    required: ["query"]
  }
};

// 第一个请求 - Claude 在所有工具调用之前思考一次
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [{
    role: "user",
    content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
  }]
});

// 响应包括思考，然后是工具使用
// 注意：Claude 在开始时思考一次，然后做出所有工具决策
console.log("First response:");
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`Thinking (summarized): ${block.thinking}`);
  } else if (block.type === "tool_use") {
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  } else if (block.type === "text") {
    console.log(`Text: ${block.text}`);
  }
}

// 您将执行工具并返回结果...
// 获得两个工具结果后，Claude 直接响应而不进行额外思考
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.messages.Model;
import java.util.List;
import java.util.Map;

public class NonInterleavedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 定义计算器工具
        BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "expression", Map.of(
                                "type", "string",
                                "description", "Mathematical expression to evaluate"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                .build();

        BetaTool calculatorTool = BetaTool.builder()
                .name("calculator")
                .description("Perform mathematical calculations")
                .inputSchema(calculatorSchema)
                .build();

        // 定义数据库工具
        BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "SQL query to execute"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        BetaTool databaseTool = BetaTool.builder()
                .name("database_query")
                .description("Query product database")
                .inputSchema(databaseSchema)
                .build();

        // 第一个请求 - Claude 在所有工具调用之前思考一次
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .build()
        );

        // 响应包括思考，然后是工具使用
        // 注意：Claude 在开始时思考一次，然后做出所有工具决策
        System.out.println("First response:");
        for (BetaContentBlock block : response.content()) {
            if (block.isThinking()) {
                System.out.println("Thinking (summarized): " + block.asThinking().thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            } else if (block.isText()) {
                System.out.println("Text: " + block.asText().text());
            }
        }

        // 您将执行工具并返回结果...
        // 获得两个工具结果后，Claude 直接响应而不进行额外思考
    }
}
```
</CodeGroup>

在这个不使用交错思考的示例中：
1. Claude 在开始时思考一次以理解任务
2. 提前做出所有工具使用决策
3. 当工具结果返回时，Claude 立即提供响应而不进行额外思考

</section>

<section title="使用交错思考的工具使用">

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 与之前相同的工具定义
calculator_tool = {
    "name": "calculator",
    "description": "Perform mathematical calculations",
    "input_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
            }
        },
        "required": ["expression"]
    }
}

database_tool = {
    "name": "database_query",
    "description": "Query product database",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "SQL query to execute"
            }
        },
        "required": ["query"]
    }
}

# 第一个请求，启用交错思考
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[{
        "role": "user",
        "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    }]
)

print("Initial response:")
thinking_blocks = []
tool_use_blocks = []

for block in response.content:
    if block.type == "thinking":
        thinking_blocks.append(block)
        print(f"Thinking: {block.thinking}")
    elif block.type == "tool_use":
        tool_use_blocks.append(block)
        print(f"Tool use: {block.name} with input {block.input}")
    elif block.type == "text":
        print(f"Text: {block.text}")

# 第一个工具结果（计算器）
calculator_result = "7500"  # 150 * 50

# 继续第一个工具结果
response2 = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[
        {
            "role": "user",
            "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        },
        {
            "role": "assistant",
            "content": [thinking_blocks[0], tool_use_blocks[0]]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[0].id,
                "content": calculator_result
            }]
        }
    ]
)

print("\nAfter calculator result:")
# 使用交错思考，Claude 可以思考计算器结果
# 然后决定是否查询数据库
for block in response2.content:
    if block.type == "thinking":
        thinking_blocks.append(block)
        print(f"Interleaved thinking: {block.thinking}")
    elif block.type == "tool_use":
        tool_use_blocks.append(block)
        print(f"Tool use: {block.name} with input {block.input}")

# 第二个工具结果（数据库）
database_result = "5200"  # 示例平均月收入

# 继续第二个工具结果
response3 = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[
        {
            "role": "user",
            "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        },
        {
            "role": "assistant",
            "content": [thinking_blocks[0], tool_use_blocks[0]]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[0].id,
                "content": calculator_result
            }]
        },
        {
            "role": "assistant",
            "content": thinking_blocks[1:] + tool_use_blocks[1:]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[1].id,
                "content": database_result
            }]
        }
    ]
)

print("\nAfter database result:")
# 使用交错思考，Claude 可以思考两个结果
# 然后制定最终响应
for block in response3.content:
    if block.type == "thinking":
        print(f"Final thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Final response: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 与之前相同的工具定义
const calculatorTool = {
  name: "calculator",
  description: "Perform mathematical calculations",
  input_schema: {
    type: "object",
    properties: {
      expression: {
        type: "string",
        description: "Mathematical expression to evaluate"
      }
    },
    required: ["expression"]
  }
};

const databaseTool = {
  name: "database_query",
  description: "Query product database",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "SQL query to execute"
      }
    },
    required: ["query"]
  }
};

// 第一个请求，启用交错思考
const response = await client.beta.messages.create({
  // 启用交错思考
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [{
    role: "user",
    content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
  }]
});

console.log("Initial response:");
const thinkingBlocks = [];
const toolUseBlocks = [];

for (const block of response.content) {
  if (block.type === "thinking") {
    thinkingBlocks.push(block);
    console.log(`Thinking: ${block.thinking}`);
  } else if (block.type === "tool_use") {
    toolUseBlocks.push(block);
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  } else if (block.type === "text") {
    console.log(`Text: ${block.text}`);
  }
}

// 第一个工具结果（计算器）
const calculatorResult = "7500"; // 150 * 50

// 继续第一个工具结果
const response2 = await client.beta.messages.create({
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [
    {
      role: "user",
      content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    },
    {
      role: "assistant",
      content: [thinkingBlocks[0], toolUseBlocks[0]]
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[0].id,
        content: calculatorResult
      }]
    }
  ]
});

console.log("\nAfter calculator result:");
// 使用交错思考，Claude 可以思考计算器结果
// 然后决定是否查询数据库
for (const block of response2.content) {
  if (block.type === "thinking") {
    thinkingBlocks.push(block);
    console.log(`Interleaved thinking: ${block.thinking}`);
  } else if (block.type === "tool_use") {
    toolUseBlocks.push(block);
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  }
}

// 第二个工具结果（数据库）
const databaseResult = "5200"; // 示例平均月收入

// 继续第二个工具结果
const response3 = await client.beta.messages.create({
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [
    {
      role: "user",
      content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    },
    {
      role: "assistant",
      content: [thinkingBlocks[0], toolUseBlocks[0]]
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[0].id,
        content: calculatorResult
      }]
    },
    {
      role: "assistant",
      content: thinkingBlocks.slice(1).concat(toolUseBlocks.slice(1))
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[1].id,
        content: databaseResult
      }]
    }
  ]
});

console.log("\nAfter database result:");
// 使用交错思考，Claude 可以思考两个结果
// 然后制定最终响应
for (const block of response3.content) {
  if (block.type === "thinking") {
    console.log(`Final thinking: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`Final response: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.messages.Model;
import java.util.*;
import static java.util.stream.Collectors.toList;

public class InterleavedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 定义计算器工具
        BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "expression", Map.of(
                                "type", "string",
                                "description", "Mathematical expression to evaluate"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                .build();

        BetaTool calculatorTool = BetaTool.builder()
                .name("calculator")
                .description("Perform mathematical calculations")
                .inputSchema(calculatorSchema)
                .build();

        // 定义数据库工具
        BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "SQL query to execute"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        BetaTool databaseTool = BetaTool.builder()
                .name("database_query")
                .description("Query product database")
                .inputSchema(databaseSchema)
                .build();

        // 第一个请求，启用交错思考
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        // 使用测试版标头启用交错思考
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .build()
        );

        System.out.println("Initial response:");
        List<BetaThinkingBlock> thinkingBlocks = new ArrayList<>();
        List<BetaToolUseBlock> toolUseBlocks = new ArrayList<>();

        for (BetaContentBlock block : response.content()) {
            if (block.isThinking()) {
                BetaThinkingBlock thinking = block.asThinking();
                thinkingBlocks.add(thinking);
                System.out.println("Thinking: " + thinking.thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                toolUseBlocks.add(toolUse);
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            } else if (block.isText()) {
                System.out.println("Text: " + block.asText().text());
            }
        }

        // 第一个工具结果（计算器）
        String calculatorResult = "7500"; // 150 * 50

        // 继续第一个工具结果
        BetaMessage response2 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .addAssistantMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                        ))
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(0).id())
                                                .content(calculatorResult)
                                                .build()
                                )
                        ))
                        .build()
        );

        System.out.println("\nAfter calculator result:");
        // 使用交错思考，Claude 可以思考计算器结果
        // 然后决定是否查询数据库
        for (BetaContentBlock block : response2.content()) {
            if (block.isThinking()) {
                BetaThinkingBlock thinking = block.asThinking();
                thinkingBlocks.add(thinking);
                System.out.println("Interleaved thinking: " + thinking.thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                toolUseBlocks.add(toolUse);
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            }
        }

        // 第二个工具结果（数据库）
        String databaseResult = "5200"; // 示例平均月收入

        // 准备助手消息的组合内容
        List<BetaContentBlockParam> combinedContent = new ArrayList<>();
        for (int i = 1; i < thinkingBlocks.size(); i++) {
            combinedContent.add(BetaContentBlockParam.ofThinking(thinkingBlocks.get(i).toParam()));
        }
        for (int i = 1; i < toolUseBlocks.size(); i++) {
            combinedContent.add(BetaContentBlockParam.ofToolUse(toolUseBlocks.get(i).toParam()));
        }

        // 继续第二个工具结果
        BetaMessage response3 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .addAssistantMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                        ))
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(0).id())
                                                .content(calculatorResult)
                                                .build()
                                )
                        ))
                        .addAssistantMessageOfBetaContentBlockParams(combinedContent)
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(1).id())
                                                .content(databaseResult)
                                                .build()
                                )
                        ))
                        .build()
        );

        System.out.println("\nAfter database result:");
        // 使用交错思考，Claude 可以思考两个结果
        // 然后制定最终响应
        for (BetaContentBlock block : response3.content()) {
            if (block.isThinking()) {
                System.out.println("Final thinking: " + block.asThinking().thinking());
            } else if (block.isText()) {
                System.out.println("Final response: " + block.asText().text());
            }
        }
    }
}
```
</CodeGroup>

在这个使用交错思考的示例中：
1. Claude 最初思考任务
2. 收到计算器结果后，Claude 可以再次思考该结果的含义
3. Claude 然后根据第一个结果决定如何查询数据库
4. 收到数据库结果后，Claude 再思考一次两个结果，然后制定最终响应
5. 思考预算分布在该轮次内的所有思考块中

这种模式允许更复杂的推理链，其中每个工具的输出为下一个决策提供信息。

</section>

## 扩展思考与提示缓存

[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)与思考有几个重要的注意事项：

<Tip>
扩展思考任务通常需要超过 5 分钟才能完成。考虑使用[1 小时缓存持续时间](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration)来维护跨越较长思考会话和多步骤工作流的缓存命中。
</Tip>

**思考块上下文移除**
- 来自先前轮次的思考块从上下文中移除，这可能会影响缓存断点
- 继续使用工具的对话时，思考块被缓存并在从缓存读取时计为输入令牌
- 这产生了一个权衡：虽然思考块在视觉上不消耗上下文窗口空间，但在缓存时仍然计入您的输入令牌使用
- 如果思考被禁用，如果您在当前工具使用轮次中传递思考内容，请求将失败。在其他情况下，传递给 API 的思考内容将被忽略

**缓存失效模式**
- 思考参数的更改（启用/禁用或预算分配）会使消息缓存断点失效
- [交错思考](#interleaved-thinking)放大了缓存失效，因为思考块可能发生在多个[工具调用](#extended-thinking-with-tool-use)之间
- 系统提示和工具保持缓存，尽管思考参数更改或块移除

<Note>
虽然思考块被移除用于缓存和上下文计算，但在继续使用[工具使用](#extended-thinking-with-tool-use)的对话时必须保留它们，特别是使用[交错思考](#interleaved-thinking)时。
</Note>

## 扩展思考与提示缓存

[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)与思考有几个重要的考虑事项：

<Tip>
扩展思考任务通常需要超过5分钟才能完成。考虑使用[1小时缓存时长](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration)来维持跨越更长思考会话和多步骤工作流的缓存命中。
</Tip>

**思考块上下文移除**
- 来自前面轮次的思考块会从上下文中移除，这可能会影响缓存断点
- 当继续使用工具的对话时，思考块会被缓存，并在从缓存读取时计为输入令牌
- 这产生了一个权衡：虽然思考块在视觉上不消耗上下文窗口空间，但在缓存时仍然计入您的输入令牌使用量
- 如果思考被禁用，如果您在当前工具使用轮次中传递思考内容，请求将失败。在其他上下文中，传递给API的思考内容会被简单地忽略

**缓存失效模式**
- 对思考参数的更改（启用/禁用或预算分配）会使消息缓存断点失效
- [交错思考](#interleaved-thinking)会放大缓存失效，因为思考块可能出现在多个[工具调用](#extended-thinking-with-tool-use)之间
- 系统提示和工具尽管思考参数更改或块移除，仍保持缓存

<Note>
虽然思考块会因缓存和上下文计算而被移除，但在继续使用[工具使用](#extended-thinking-with-tool-use)的对话时，特别是使用[交错思考](#interleaved-thinking)时，必须保留它们。
</Note>

### 理解思考块缓存行为

当使用扩展思考与工具使用时，思考块表现出特定的缓存行为，这会影响令牌计数：

**工作原理：**

1. 仅当您发出包含工具结果的后续请求时，才会发生缓存
2. 当发出后续请求时，之前的对话历史（包括思考块）可以被缓存
3. 这些缓存的思考块在从缓存读取时计为您使用指标中的输入令牌
4. 当包含非工具结果用户块时，所有之前的思考块都会被忽略并从上下文中移除

**详细示例流程：**

**请求1：**
```
User: "What's the weather in Paris?"
```
**响应1：**
```
[thinking_block_1] + [tool_use block 1]
```

**请求2：**
```
User: ["What's the weather in Paris?"], 
Assistant: [thinking_block_1] + [tool_use block 1], 
User: [tool_result_1, cache=True]
```
**响应2：**
```
[thinking_block_2] + [text block 2]
```
请求2写入请求内容的缓存（不是响应）。缓存包括原始用户消息、第一个思考块、工具使用块和工具结果。

**请求3：**
```
User: ["What's the weather in Paris?"],
Assistant: [thinking_block_1] + [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [thinking_block_2] + [text block 2],
User: [Text response, cache=True]
```
对于Claude Opus 4.5及更高版本，默认情况下保留所有之前的思考块。对于较旧的模型，由于包含了非工具结果用户块，所有之前的思考块都会被忽略。此请求将按以下方式处理：
```
User: ["What's the weather in Paris?"],
Assistant: [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [text block 2],
User: [Text response, cache=True]
```

**关键点：**
- 此缓存行为会自动发生，即使没有显式的`cache_control`标记
- 此行为在使用常规思考或交错思考时是一致的

<section title="系统提示缓存（思考更改时保留）">

<CodeGroup>
```python Python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

SYSTEM_PROMPT=[
    {
        "type": "text",
        "text": "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
    },
    {
        "type": "text",
        "text": LARGE_TEXT,
        "cache_control": {"type": "ephemeral"}
    }
]

MESSAGES = [
    {
        "role": "user",
        "content": "Analyze the tone of this passage."
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

# Third request - different thinking parameters (cache miss for messages)
print("\nThird request - different thinking parameters (cache miss for messages)")
response3 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Changed thinking budget
    },
    system=SYSTEM_PROMPT,  # System prompt remains cached
    messages=MESSAGES  # Messages cache is invalidated
)

print(f"Third response usage: {response3.usage}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import axios from 'axios';
import * as cheerio from 'cheerio';

const client = new Anthropic();

async function fetchArticleContent(url: string): Promise<string> {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);
  
  // Remove script and style elements
  $('script, style').remove();
  
  // Get text
  let text = $.text();
  
  // Break into lines and remove leading and trailing space on each
  const lines = text.split('\n').map(line => line.trim());
  // Drop blank lines
  text = lines.filter(line => line.length > 0).join('\n');
  
  return text;
}

// Fetch the content of the article
const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
const bookContent = await fetchArticleContent(bookUrl);
// Use just enough text for caching (first few chapters)
const LARGE_TEXT = bookContent.slice(0, 5000);

const SYSTEM_PROMPT = [
  {
    type: "text",
    text: "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
  },
  {
    type: "text",
    text: LARGE_TEXT,
    cache_control: { type: "ephemeral" }
  }
];

const MESSAGES = [
  {
    role: "user",
    content: "Analyze the tone of this passage."
  }
];

// First request - establish cache
console.log("First request - establishing cache");
const response1 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 4000
  },
  system: SYSTEM_PROMPT,
  messages: MESSAGES
});

console.log(`First response usage: ${response1.usage}`);

MESSAGES.push({
  role: "assistant",
  content: response1.content
});
MESSAGES.push({
  role: "user",
  content: "Analyze the characters in this passage."
});

// Second request - same thinking parameters (cache hit expected)
console.log("\nSecond request - same thinking parameters (cache hit expected)");
const response2 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 4000
  },
  system: SYSTEM_PROMPT,
  messages: MESSAGES
});

console.log(`Second response usage: ${response2.usage}`);

// Third request - different thinking parameters (cache miss for messages)
console.log("\nThird request - different thinking parameters (cache miss for messages)");
const response3 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 8000  // Changed thinking budget
  },
  system: SYSTEM_PROMPT,  // System prompt remains cached
  messages: MESSAGES  // Messages cache is invalidated
});

console.log(`Third response usage: ${response3.usage}`);
```
</CodeGroup>

</section>
<section title="消息缓存（思考更改时失效）">

<CodeGroup>
```python Python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

# No system prompt - caching in messages instead
MESSAGES = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": LARGE_TEXT,
                "cache_control": {"type": "ephemeral"},
            },
            {
                "type": "text",
                "text": "Analyze the tone of this passage."
            }
        ]
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000  # Same thinking budget
    },
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response2.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the setting in this passage."
})

# Third request - different thinking budget (cache miss expected)
print("\nThird request - different thinking budget (cache miss expected)")
response3 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Different thinking budget breaks cache
    },
    messages=MESSAGES
)

print(f"Third response usage: {response3.usage}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import axios from 'axios';
import * as cheerio from 'cheerio';

const client = new Anthropic();

async function fetchArticleContent(url: string): Promise<string> {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);

  // Remove script and style elements
  $('script, style').remove();

  // Get text
  let text = $.text();

  // Clean up text (break into lines, remove whitespace)
  const lines = text.split('\n').map(line => line.trim());
  const chunks = lines.flatMap(line => line.split('  ').map(phrase => phrase.trim()));
  text = chunks.filter(chunk => chunk).join('\n');

  return text;
}

async function main() {
  // Fetch the content of the article
  const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
  const bookContent = await fetchArticleContent(bookUrl);
  // Use just enough text for caching (first few chapters)
  const LARGE_TEXT = bookContent.substring(0, 5000);

  // No system prompt - caching in messages instead
  let MESSAGES = [
    {
      role: "user",
      content: [
        {
          type: "text",
          text: LARGE_TEXT,
          cache_control: {type: "ephemeral"},
        },
        {
          type: "text",
          text: "Analyze the tone of this passage."
        }
      ]
    }
  ];

  // First request - establish cache
  console.log("First request - establishing cache");
  const response1 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 4000
    },
    messages: MESSAGES
  });

  console.log(`First response usage: `, response1.usage);

  MESSAGES = [
    ...MESSAGES,
    {
      role: "assistant",
      content: response1.content
    },
    {
      role: "user",
      content: "Analyze the characters in this passage."
    }
  ];

  // Second request - same thinking parameters (cache hit expected)
  console.log("\nSecond request - same thinking parameters (cache hit expected)");
  const response2 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 4000  // Same thinking budget
    },
    messages: MESSAGES
  });

  console.log(`Second response usage: `, response2.usage);

  MESSAGES = [
    ...MESSAGES,
    {
      role: "assistant",
      content: response2.content
    },
    {
      role: "user",
      content: "Analyze the setting in this passage."
    }
  ];

  // Third request - different thinking budget (cache miss expected)
  console.log("\nThird request - different thinking budget (cache miss expected)");
  const response3 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 8000  // Different thinking budget breaks cache
    },
    messages: MESSAGES
  });

  console.log(`Third response usage: `, response3.usage);
}

main().catch(console.error);
```

```java Java
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.net.URL;
import java.util.Arrays;
import java.util.regex.Pattern;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;

import static java.util.stream.Collectors.joining;
import static java.util.stream.Collectors.toList;

public class ThinkingCacheExample {
    public static void main(String[] args) throws IOException {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // Fetch the content of the article
        String bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
        String bookContent = fetchArticleContent(bookUrl);
        // Use just enough text for caching (first few chapters)
        String largeText = bookContent.substring(0, 5000);

        List<BetaTextBlockParam> systemPrompt = List.of(
                BetaTextBlockParam.builder()
                        .text("You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.")
                        .build(),
                BetaTextBlockParam.builder()
                        .text(largeText)
                        .cacheControl(BetaCacheControlEphemeral.builder().build())
                        .build()
        );

        List<BetaMessageParam> messages = new ArrayList<>();
        messages.add(BetaMessageParam.builder()
                .role(BetaMessageParam.Role.USER)
                .content("Analyze the tone of this passage.")
                .build());

        // First request - establish cache
        System.out.println("First request - establishing cache");
        BetaMessage response1 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .messages(messages)
                        .build()
        );

        System.out.println("First response usage: " + response1.usage());

        // Second request - same thinking parameters (cache hit expected)
        System.out.println("\nSecond request - same thinking parameters (cache hit expected)");
        BetaMessage response2 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .addMessage(response1)
                        .addUserMessage("Analyze the characters in this passage.")
                        .messages(messages)
                        .build()
        );

        System.out.println("Second response usage: " + response2.usage());

        // Third request - different thinking budget (cache hit expected because system prompt caching)
        System.out.println("\nThird request - different thinking budget (cache hit expected)");
        BetaMessage response3 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(8000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .addMessage(response1)
                        .addUserMessage("Analyze the characters in this passage.")
                        .addMessage(response2)
                        .addUserMessage("Analyze the setting in this passage.")
                        .build()
        );

        System.out.println("Third response usage: " + response3.usage());
    }

    private static String fetchArticleContent(String url) throws IOException {
        // Fetch HTML content
        String htmlContent = fetchHtml(url);

        // Remove script and style elements
        String noScriptStyle = removeElements(htmlContent, "script", "style");

        // Extract text (simple approach - remove HTML tags)
        String text = removeHtmlTags(noScriptStyle);

        // Clean up text (break into lines, remove whitespace)
        List<String> lines = Arrays.asList(text.split("\n"));
        List<String> trimmedLines = lines.stream()
                .map(String::trim)
                .collect(toList());

        // Split on double spaces and flatten
        List<String> chunks = trimmedLines.stream()
                .flatMap(line -> Arrays.stream(line.split("  "))
                        .map(String::trim))
                .collect(toList());

        // Filter empty chunks and join with newlines
        return chunks.stream()
                .filter(chunk -> !chunk.isEmpty())
                .collect(joining("\n"));
    }

    /**
     * Fetches HTML content from a URL
     */
    private static String fetchHtml(String urlString) throws IOException {
        try (InputStream inputStream = new URL(urlString).openStream()) {
            StringBuilder content = new StringBuilder();
            try (BufferedReader reader = new BufferedReader(
                    new InputStreamReader(inputStream))) {
                String line;
                while ((line = reader.readLine()) != null) {
                    content.append(line).append("\n");
                }
            }
            return content.toString();
        }
    }

    /**
     * Removes specified HTML elements and their content
     */
    private static String removeElements(String html, String... elementNames) {
        String result = html;
        for (String element : elementNames) {
            // Pattern to match <element>...</element> and self-closing tags
            String pattern = "<" + element + "\\s*[^>]*>.*?</" + element + ">|<" + element + "\\s*[^>]*/?>";
            result = Pattern.compile(pattern, Pattern.DOTALL).matcher(result).replaceAll("");
        }
        return result;
    }

    /**
     * Removes all HTML tags from content
     */
    private static String removeHtmlTags(String html) {
        // Replace <br> and <p> tags with newlines for better text formatting
        String withLineBreaks = html.replaceAll("<br\\s*/?\\s*>|</?p\\s*[^>]*>", "\n");

        // Remove remaining HTML tags
        String noTags = withLineBreaks.replaceAll("<[^>]*>", "");

        // Decode HTML entities (simplified for common entities)
        return decodeHtmlEntities(noTags);
    }

    /**
     * Simple HTML entity decoder for common entities
     */
    private static String decodeHtmlEntities(String text) {
        return text
                .replaceAll("&nbsp;", " ")
                .replaceAll("&amp;", "&")
                .replaceAll("&lt;", "<")
                .replaceAll("&gt;", ">")
                .replaceAll("&quot;", "\"")
                .replaceAll("&#39;", "'")
                .replaceAll("&hellip;", "...")
                .replaceAll("&mdash;", "—");
    }

}
```
</CodeGroup>

脚本的输出如下（您可能会看到略有不同的数字）

```
First request - establishing cache
First response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }

Second request - same thinking parameters (cache hit expected)

Second response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }

Third request - different thinking budget (cache miss expected)
Third response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }
```

此示例演示了当缓存在消息数组中设置时，更改思考参数（预算令牌从4000增加到8000）**使缓存失效**。第三个请求显示没有缓存命中，`cache_creation_input_tokens=1370`和`cache_read_input_tokens=0`，证明当思考参数更改时，基于消息的缓存会失效。

</section>

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 扩展思考与工具使用中的上下文窗口

当使用扩展思考与工具使用时，思考块必须显式保留并与工具结果一起返回。

扩展思考与工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下图说明了扩展思考与工具使用的令牌管理：

![扩展思考和工具使用的上下文窗口图](/docs/images/context-window-thinking-tools.svg)

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 扩展思考与工具使用中的上下文窗口

当使用扩展思考与工具使用时，思考块必须显式保留并与工具结果一起返回。

扩展思考与工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下图说明了扩展思考与工具使用的令牌管理：

![扩展思考和工具使用的上下文窗口图](/docs/images/context-window-thinking-tools.svg)

### 使用扩展思考管理令牌

鉴于Claude 3.7和4模型的上下文窗口和`max_tokens`行为与扩展思考，您可能需要：

- 更积极地监控和管理您的令牌使用
- 随着提示长度的变化调整`max_tokens`值
- 可能更频繁地使用[令牌计数端点](/docs/zh-CN/build-with-claude/token-counting)
- 意识到之前的思考块不会在您的上下文窗口中累积

这一变化是为了提供更可预测和透明的行为，特别是随着最大令牌限制显著增加。

## 最大令牌数和扩展思考的上下文窗口大小

在较早的 Claude 模型（Claude Sonnet 3.7 之前），如果提示令牌和 `max_tokens` 的总和超过了模型的上下文窗口，系统会自动调整 `max_tokens` 以适应上下文限制。这意味着您可以设置一个较大的 `max_tokens` 值，系统会根据需要静默地减少它。

使用 Claude 3.7 和 4 模型时，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌 + `max_tokens` 超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 启用扩展思考的上下文窗口

在计算启用思考的上下文窗口使用情况时，需要注意以下几点：

- 来自先前轮次的思考块被剥离，不计入您的上下文窗口
- 当前轮次的思考计入该轮次的 `max_tokens` 限制

下面的图表演示了启用扩展思考时的专门令牌管理：

![启用扩展思考的上下文窗口图表](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数 API](/docs/zh-CN/build-with-claude/token-counting)来获得您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 启用扩展思考和工具使用的上下文窗口

使用扩展思考和工具使用时，思考块必须被显式保留并与工具结果一起返回。

启用扩展思考和工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下面的图表说明了扩展思考和工具使用的令牌管理：

![启用扩展思考和工具使用的上下文窗口图表](/docs/images/context-window-thinking-tools.svg)

### 使用扩展思考管理令牌

考虑到 Claude 3.7 和 4 模型的上下文窗口和 `max_tokens` 行为与扩展思考的关系，您可能需要：

- 更主动地监控和管理您的令牌使用
- 随着提示长度的变化调整 `max_tokens` 值
- 可能需要更频繁地使用[令牌计数端点](/docs/zh-CN/build-with-claude/token-counting)
- 意识到先前的思考块不会在您的上下文窗口中累积

做出这一改变是为了提供更可预测和透明的行为，特别是因为最大令牌限制已大幅增加。

## 思考加密

完整的思考内容被加密并在 `signature` 字段中返回。该字段用于验证思考块是由 Claude 生成的，当传回 API 时使用。

<Note>
只有在使用[带扩展思考的工具](#extended-thinking-with-tool-use)时才严格需要发送回思考块。否则，您可以省略先前轮次的思考块，或者如果您传回它们，让 API 为您剥离它们。

如果发送回思考块，我们建议按照您接收的方式传回所有内容以保持一致性并避免潜在问题。
</Note>

以下是关于思考加密的一些重要注意事项：
- 当[流式传输响应](#streaming-thinking)时，签名通过 `content_block_delta` 事件中的 `signature_delta` 添加，位于 `content_block_stop` 事件之前。
- `signature` 值在 Claude 4 模型中的长度明显长于之前的模型。
- `signature` 字段是一个不透明字段，不应被解释或解析 - 它仅用于验证目的。
- `signature` 值在各平台上兼容（Claude API、[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock) 和 [Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)）。在一个平台上生成的值将与另一个平台兼容。

### 思考编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密 `thinking` 块的部分或全部内容，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 意识到编辑的思考块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的一些内部推理已自动加密以确保安全。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉编辑的块，同时保留正常的思考块
- 透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理以优雅地管理编辑的思考，而不会破坏您的 UI

以下是显示正常和编辑思考块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到编辑的思考块是预期的行为。该模型仍然可以使用这个编辑的推理来为其响应提供信息，同时保持安全护栏。

如果您需要在应用程序中测试编辑的思考处理，您可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手轮次的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用编辑的思考块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发编辑的思考（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别编辑的思考块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块仍然可用于后续请求

    # 提取所有块（编辑和非编辑）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 在传递到后续请求时，包括所有块而不进行修改
    # 这保留了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发编辑的思考（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别编辑的思考块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块仍然可用于后续请求

  // 提取所有块（编辑和非编辑）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 在传递到后续请求时，包括所有块而不进行修改
  // 这保留了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发编辑的思考（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别编辑的思考块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块仍然可用于后续请求
            // 提取所有块（编辑和非编辑）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 在传递到后续请求时，包括所有块而不进行修改
            // 这保留了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

### 思考编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密 `thinking` 块的部分或全部内容，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 意识到编辑的思考块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的一些内部推理已自动加密以确保安全。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉编辑的块，同时保留正常的思考块
- 透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理以优雅地管理编辑的思考，而不会破坏您的 UI

以下是显示正常和编辑思考块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到编辑的思考块是预期的行为。该模型仍然可以使用这个编辑的推理来为其响应提供信息，同时保持安全护栏。

如果您需要在应用程序中测试编辑的思考处理，您可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手轮次的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用编辑的思考块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发编辑的思考（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别编辑的思考块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块仍然可用于后续请求

    # 提取所有块（编辑和非编辑）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 在传递到后续请求时，包括所有块而不进行修改
    # 这保留了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发编辑的思考（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别编辑的思考块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块仍然可用于后续请求

  // 提取所有块（编辑和非编辑）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 在传递到后续请求时，包括所有块而不进行修改
  // 这保留了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发编辑的思考（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别编辑的思考块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块仍然可用于后续请求
            // 提取所有块（编辑和非编辑）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 在传递到后续请求时，包括所有块而不进行修改
            // 这保留了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

## 不同模型版本之间的思考差异

Messages API 在 Claude Sonnet 3.7 和 Claude 4 模型之间处理思考的方式不同，主要在编辑和总结行为方面。

请参阅下表以获得简明比较：

| 功能 | Claude Sonnet 3.7 | Claude 4 模型（Opus 4.5 之前） | Claude Opus 4.5 及更高版本 |
|---------|------------------|-------------------------------|--------------------------|
| **思考输出** | 返回完整思考输出 | 返回总结的思考 | 返回总结的思考 |
| **交错思考** | 不支持 | 支持 `interleaved-thinking-2025-05-14` beta 标头 | 支持 `interleaved-thinking-2025-05-14` beta 标头 |
| **思考块保留** | 不跨轮保留 | 不跨轮保留 | **默认保留**（启用缓存优化、令牌节省） |

### Claude Opus 4.5 中的思考块保留

Claude Opus 4.5 引入了一个新的默认行为：**来自先前助手轮次的思考块默认在模型上下文中保留**。这与较早的模型不同，较早的模型会删除先前轮次的思考块。

**思考块保留的好处：**

- **缓存优化**：使用工具使用时，保留的思考块启用缓存命中，因为它们与工具结果一起传回并在助手轮次中增量缓存，在多步工作流中产生令牌节省
- **无智能影响**：保留思考块对模型性能没有负面影响

**重要注意事项：**

- **上下文使用**：长对话将消耗更多上下文空间，因为思考块保留在上下文中
- **自动行为**：这是 Claude Opus 4.5 的默认行为 - 不需要代码更改或 beta 标头
- **向后兼容性**：要利用此功能，继续将完整的、未修改的思考块传回 API，就像您对工具使用所做的那样

<Note>
对于较早的模型（Claude Sonnet 4.5、Opus 4.1 等），来自先前轮次的思考块继续从上下文中删除。[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分中描述的现有行为适用于这些模型。
</Note>

### 思考内容编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密部分或全部 `thinking` 块，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时会被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 请注意 redacted thinking 块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的某些内部推理已因安全原因自动加密。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉 redacted 块，同时保留正常的思考块
- 要透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理，以优雅地管理 redacted thinking，而不会破坏您的 UI

以下是显示正常和 redacted thinking 块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到 redacted thinking 块是预期的行为。该模型仍然可以使用这个 redacted 推理来为其响应提供信息，同时维护安全护栏。

如果您需要在应用程序中测试 redacted thinking 处理，可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手转向的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用 redacted thinking 块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发 redacted thinking（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别 redacted thinking 块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块在后续请求中仍然可用

    # 提取所有块（redacted 和非 redacted）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 传递到后续请求时，包括所有块而不进行修改
    # 这保持了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发 redacted thinking（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别 redacted thinking 块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块在后续请求中仍然可用

  // 提取所有块（redacted 和非 redacted）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 传递到后续请求时，包括所有块而不进行修改
  // 这保持了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发 redacted thinking（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别 redacted thinking 块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块在后续请求中仍然可用
            // 提取所有块（redacted 和非 redacted）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 传递到后续请求时，包括所有块而不进行修改
            // 这保持了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

## 不同模型版本中的思考差异

Messages API 在 Claude Sonnet 3.7 和 Claude 4 模型中处理思考的方式不同，主要在编辑和总结行为方面。

请参阅下表了解简明比较：

| 功能 | Claude Sonnet 3.7 | Claude 4 模型（Opus 4.5 之前） | Claude Opus 4.5 及更高版本 |
|---------|------------------|-------------------------------|--------------------------|
| **思考输出** | 返回完整思考输出 | 返回总结的思考 | 返回总结的思考 |
| **交错思考** | 不支持 | 支持 `interleaved-thinking-2025-05-14` beta 标头 | 支持 `interleaved-thinking-2025-05-14` beta 标头 |
| **思考块保留** | 不跨转向保留 | 不跨转向保留 | **默认保留**（启用缓存优化、节省令牌） |

### Claude Opus 4.5 中的思考块保留

Claude Opus 4.5 引入了一个新的默认行为：**来自先前助手转向的思考块默认在模型上下文中保留**。这与早期模型不同，早期模型会删除先前转向的思考块。

**思考块保留的好处：**

- **缓存优化**：使用工具使用时，保留的思考块启用缓存命中，因为它们与工具结果一起传回并在助手转向中增量缓存，在多步工作流中节省令牌
- **无智能影响**：保留思考块对模型性能没有负面影响

**重要考虑事项：**

- **上下文使用**：长对话将消耗更多上下文空间，因为思考块保留在上下文中
- **自动行为**：这是 Claude Opus 4.5 的默认行为——不需要代码更改或 beta 标头
- **向后兼容性**：要利用此功能，继续将完整的、未修改的思考块传回 API，就像您对工具使用所做的那样

<Note>
对于早期模型（Claude Sonnet 4.5、Opus 4.1 等），来自先前转向的思考块继续从上下文中删除。[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分中描述的现有行为适用于这些模型。
</Note>

## 定价

有关完整的定价信息，包括基础费率、缓存写入、缓存命中和输出令牌，请参阅[定价页面](/docs/zh-CN/about-claude/pricing)。

思考过程产生的费用包括：
- 思考期间使用的令牌（输出令牌）
- 后续请求中包含的最后一个助手转向的思考块（输入令牌）
- 标准文本输出令牌

<Note>
启用扩展思考时，会自动包含专门的系统提示以支持此功能。
</Note>

使用总结思考时：
- **输入令牌**：您原始请求中的令牌（不包括先前转向的思考令牌）
- **输出令牌（计费）**：Claude 内部生成的原始思考令牌
- **输出令牌（可见）**：您在响应中看到的总结思考令牌
- **无费用**：用于生成摘要的令牌

<Warning>
计费的输出令牌计数将**不**与响应中的可见令牌计数匹配。您需要为完整的思考过程付费，而不是您看到的摘要。
</Warning>

## 扩展思考的最佳实践和注意事项

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

### 使用指南

- **任务选择**：对于特别复杂的任务使用扩展思考，这些任务受益于逐步推理，如数学、编码和分析。
- **上下文处理**：您不需要自己删除以前的思考块。Claude API 会自动忽略先前转向的思考块，在计算上下文使用时不包括它们。
- **提示工程**：如果您想最大化 Claude 的思考能力，请查看我们的[扩展思考提示技巧](/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips)。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

### 使用指南

- **任务选择**：对于特别复杂的任务使用扩展思考，这些任务受益于逐步推理，如数学、编码和分析。
- **上下文处理**：您不需要自己删除以前的思考块。Claude API 会自动忽略先前转向的思考块，在计算上下文使用时不包括它们。
- **提示工程**：如果您想最大化 Claude 的思考能力，请查看我们的[扩展思考提示技巧](/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips)。

## 后续步骤

<CardGroup>
  <Card title="尝试扩展思考食谱" icon="book" href="https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking">
    在我们的食谱中探索思考的实际示例。
  </Card>
  <Card title="扩展思考提示技巧" icon="code" href="/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips">
    学习扩展思考的提示工程最佳实践。
  </Card>
</CardGroup>

# 使用扩展思考进行构建

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

---

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

### 总结的思考

启用扩展思考后，Claude 4 模型的消息 API 会返回 Claude 完整思考过程的摘要。总结的思考提供了扩展思考的全部智能优势，同时防止滥用。

以下是关于总结思考的一些重要注意事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了特别有助于提示工程的详细推理。
- 随着 Anthropic 寻求改进扩展思考功能，总结行为可能会发生变化。
- 总结保留了 Claude 思考过程的关键思想，延迟最少，实现了可流式传输的用户体验，并便于从 Claude Sonnet 3.7 迁移到 Claude 4 模型。
- 总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

### 总结的思考

启用扩展思考后，Claude 4 模型的消息 API 会返回 Claude 完整思考过程的摘要。总结的思考提供了扩展思考的全部智能优势，同时防止滥用。

以下是关于总结思考的一些重要注意事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了特别有助于提示工程的详细推理。
- 随着 Anthropic 寻求改进扩展思考功能，总结行为可能会发生变化。
- 总结保留了 Claude 思考过程的关键思想，延迟最少，实现了可流式传输的用户体验，并便于从 Claude Sonnet 3.7 迁移到 Claude 4 模型。
- 总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

### 流式传输思考

您可以使用[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) 流式传输扩展思考响应。

启用扩展思考的流式传输后，您会通过 `thinking_delta` 事件接收思考内容。

有关通过消息 API 进行流式传输的更多文档，请参阅[流式传输消息](/docs/zh-CN/build-with-claude/streaming)。

以下是如何处理思考流式传输的方法：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "stream": true,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "What is 27 * 453?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={"type": "enabled", "budget_tokens": 10000},
    messages=[{"role": "user", "content": "What is 27 * 453?"}],
) as stream:
    thinking_started = False
    response_started = False

    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
            # 为每个新块重置标志
            thinking_started = False
            response_started = False
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                if not thinking_started:
                    print("Thinking: ", end="", flush=True)
                    thinking_started = True
                print(event.delta.thinking, end="", flush=True)
            elif event.delta.type == "text_delta":
                if not response_started:
                    print("Response: ", end="", flush=True)
                    response_started = True
                print(event.delta.text, end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const stream = await client.messages.stream({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "What is 27 * 453?"
  }]
});

let thinkingStarted = false;
let responseStarted = false;

for await (const event of stream) {
  if (event.type === 'content_block_start') {
    console.log(`\nStarting ${event.content_block.type} block...`);
    // 为每个新块重置标志
    thinkingStarted = false;
    responseStarted = false;
  } else if (event.type === 'content_block_delta') {
    if (event.delta.type === 'thinking_delta') {
      if (!thinkingStarted) {
        process.stdout.write('Thinking: ');
        thinkingStarted = true;
      }
      process.stdout.write(event.delta.thinking);
    } else if (event.delta.type === 'text_delta') {
      if (!responseStarted) {
        process.stdout.write('Response: ');
        responseStarted = true;
      }
      process.stdout.write(event.delta.text);
    }
  } else if (event.type === 'content_block_stop') {
    console.log('\nBlock complete.');
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.http.StreamResponse;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class SimpleThinkingStreamingExample {
    private static boolean thinkingStarted = false;
    private static boolean responseStarted = false;
    
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams createParams = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_0)
                .maxTokens(16000)
                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                .addUserMessage("What is 27 * 453?")
                .build();

        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                     client.beta().messages().createStreaming(createParams)) {
            streamResponse.stream()
                    .forEach(event -> {
                        if (event.isContentBlockStart()) {
                            System.out.printf("\nStarting %s block...%n",
                                    event.asContentBlockStart()._type());
                            // 为每个新块重置标志
                            thinkingStarted = false;
                            responseStarted = false;
                        } else if (event.isContentBlockDelta()) {
                            var delta = event.asContentBlockDelta().delta();
                            if (delta.isBetaThinking()) {
                                if (!thinkingStarted) {
                                    System.out.print("Thinking: ");
                                    thinkingStarted = true;
                                }
                                System.out.print(delta.asBetaThinking().thinking());
                                System.out.flush();
                            } else if (delta.isBetaText()) {
                                if (!responseStarted) {
                                    System.out.print("Response: ");
                                    responseStarted = true;
                                }
                                System.out.print(delta.asBetaText().text());
                                System.out.flush();
                            }
                        } else if (event.isContentBlockStop()) {
                            System.out.println("\nBlock complete.");
                        }
                    });
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  在控制台中尝试
</TryInConsoleButton>

示例流式传输输出：
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "让我逐步解决这个问题：\n\n1. 首先分解 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// 其他思考增量...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// 其他文本增量...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

<Note>
使用启用思考的流式传输时，您可能会注意到文本有时以较大的块到达，交替出现较小的逐令牌传递。这是预期的行为，特别是对于思考内容。

流式传输系统需要分批处理内容以获得最佳性能，这可能导致这种"分块"传递模式，流式传输事件之间可能出现延迟。我们正在不断努力改进这种体验，未来的更新将专注于使思考内容流式传输更加平顺。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于处理复杂任务，同时在提供最终答案之前提供了对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制作最终响应之前会结合来自此推理的见解。

API 响应将包括 `thinking` 内容块，然后是 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅 [Messages API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在 Messages API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# The response will contain summarized thinking blocks and text blocks
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// The response will contain summarized thinking blocks and text blocks
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不是[摘要输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变为您的整个上下文窗口（200k 令牌）。

### 摘要思考

启用扩展思考后，Claude 4 模型的 Messages API 返回 Claude 完整思考过程的摘要。摘要思考提供了扩展思考的全部智能优势，同时防止了滥用。

以下是摘要思考的一些重要考虑事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了详细的推理，这对提示工程目的特别有帮助。
- 随着 Anthropic 寻求改进扩展思考功能，摘要行为可能会发生变化。
- 摘要保留了 Claude 思考过程的关键思想，同时增加了最少的延迟，实现了可流式传输的用户体验和从 Claude Sonnet 3.7 到 Claude 4 模型的轻松迁移。
- 摘要由与您在请求中指定的模型不同的模型处理。思考模型看不到摘要输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

### 流式思考

您可以使用[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) 流式传输扩展思考响应。

启用扩展思考的流式传输后，您会通过 `thinking_delta` 事件接收思考内容。

有关通过 Messages API 进行流式传输的更多文档，请参阅[流式传输消息](/docs/zh-CN/build-with-claude/streaming)。

以下是如何处理思考流式传输的方法：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "stream": true,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "What is 27 * 453?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={"type": "enabled", "budget_tokens": 10000},
    messages=[{"role": "user", "content": "What is 27 * 453?"}],
) as stream:
    thinking_started = False
    response_started = False

    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
            # Reset flags for each new block
            thinking_started = False
            response_started = False
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                if not thinking_started:
                    print("Thinking: ", end="", flush=True)
                    thinking_started = True
                print(event.delta.thinking, end="", flush=True)
            elif event.delta.type == "text_delta":
                if not response_started:
                    print("Response: ", end="", flush=True)
                    response_started = True
                print(event.delta.text, end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const stream = await client.messages.stream({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "What is 27 * 453?"
  }]
});

let thinkingStarted = false;
let responseStarted = false;

for await (const event of stream) {
  if (event.type === 'content_block_start') {
    console.log(`\nStarting ${event.content_block.type} block...`);
    // Reset flags for each new block
    thinkingStarted = false;
    responseStarted = false;
  } else if (event.type === 'content_block_delta') {
    if (event.delta.type === 'thinking_delta') {
      if (!thinkingStarted) {
        process.stdout.write('Thinking: ');
        thinkingStarted = true;
      }
      process.stdout.write(event.delta.thinking);
    } else if (event.delta.type === 'text_delta') {
      if (!responseStarted) {
        process.stdout.write('Response: ');
        responseStarted = true;
      }
      process.stdout.write(event.delta.text);
    }
  } else if (event.type === 'content_block_stop') {
    console.log('\nBlock complete.');
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.http.StreamResponse;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class SimpleThinkingStreamingExample {
    private static boolean thinkingStarted = false;
    private static boolean responseStarted = false;
    
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams createParams = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_0)
                .maxTokens(16000)
                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                .addUserMessage("What is 27 * 453?")
                .build();

        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                     client.beta().messages().createStreaming(createParams)) {
            streamResponse.stream()
                    .forEach(event -> {
                        if (event.isContentBlockStart()) {
                            System.out.printf("\nStarting %s block...%n",
                                    event.asContentBlockStart()._type());
                            // Reset flags for each new block
                            thinkingStarted = false;
                            responseStarted = false;
                        } else if (event.isContentBlockDelta()) {
                            var delta = event.asContentBlockDelta().delta();
                            if (delta.isBetaThinking()) {
                                if (!thinkingStarted) {
                                    System.out.print("Thinking: ");
                                    thinkingStarted = true;
                                }
                                System.out.print(delta.asBetaThinking().thinking());
                                System.out.flush();
                            } else if (delta.isBetaText()) {
                                if (!responseStarted) {
                                    System.out.print("Response: ");
                                    responseStarted = true;
                                }
                                System.out.print(delta.asBetaText().text());
                                System.out.flush();
                            }
                        } else if (event.isContentBlockStop()) {
                            System.out.println("\nBlock complete.");
                        }
                    });
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  在控制台中尝试
</TryInConsoleButton>

示例流式输出：
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "Let me solve this step by step:\n\n1. First break down 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// Additional thinking deltas...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// Additional text deltas...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

<Note>
使用启用思考的流式传输时，您可能会注意到文本有时会以较大的块到达，交替出现较小的逐令牌传输。这是预期的行为，特别是对于思考内容。

流式传输系统需要分批处理内容以获得最佳性能，这可能导致这种"块状"传输模式，流式传输事件之间可能出现延迟。我们正在不断努力改进这种体验，未来的更新将专注于使思考内容流式传输更加平稳。
</Note>

## 扩展思考与工具使用

扩展思考可以与[工具使用](/docs/zh-CN/agents-and-tools/tool-use/overview)一起使用，允许 Claude 通过工具选择和结果处理进行推理。

使用扩展思考和工具使用时，请注意以下限制：

1. **工具选择限制**：工具使用与思考仅支持 `tool_choice: {"type": "auto"}` (默认) 或 `tool_choice: {"type": "none"}`。使用 `tool_choice: {"type": "any"}` 或 `tool_choice: {"type": "tool", "name": "..."}` 将导致错误，因为这些选项强制工具使用，这与扩展思考不兼容。

2. **保留思考块**：在工具使用期间，您必须将 `thinking` 块传回 API 以获取最后的助手消息。将完整的未修改块传回 API 以保持推理连续性。

### 在对话中切换思考模式

您不能在助手转向的中间切换思考，包括在工具使用循环期间。整个助手转向必须在单一思考模式下运行：

- **如果启用了思考**，最后的助手转向必须以思考块开始。
- **如果禁用了思考**，最后的助手转向不能包含任何思考块

从模型的角度来看，**工具使用循环是助手转向的一部分**。助手转向直到 Claude 完成其完整响应后才完成，这可能包括多个工具调用和结果。

例如，此序列都是**单个助手转向**的一部分：
```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```

尽管有多个 API 消息，但工具使用循环在概念上是一个连续助手响应的一部分。

#### 常见错误场景

您可能会遇到此错误：
```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```

这通常发生在以下情况：
1. 您在工具使用序列期间**禁用了**思考
2. 您想再次启用思考
3. 您的最后一条助手消息包含工具使用块但没有思考块

#### 实用指南

**✗ 无效：在工具使用后立即切换思考**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```

**✓ 有效：首先完成助手转向**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```

**最佳实践**：在每个转向开始时规划您的思考策略，而不是尝试在中途切换。

<Note>
切换思考模式也会使消息历史的提示缓存失效。有关更多详情，请参阅[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分。
</Note>

<section title="示例：使用工具结果传递思考块">

以下是一个实际示例，展示了在提供工具结果时如何保留思考块：

<CodeGroup>
```python Python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

```typescript TypeScript
const weatherTool = {
  name: "get_weather",
  description: "Get current weather for a location",
  input_schema: {
    type: "object",
    properties: {
      location: { type: "string" }
    },
    required: ["location"]
  }
};

// First request - Claude responds with thinking and tool request
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" }
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.beta.messages.BetaTool;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingWithToolsExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        System.out.println(response);
    }
}
```
</CodeGroup>

API 响应将包括思考、文本和 tool_use 块：

```json
{
    "content": [
        {
            "type": "thinking",
            "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
            "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
        },
        {
            "type": "text",
            "text": "I can help you get the current weather information for Paris. Let me check that for you"
        },
        {
            "type": "tool_use",
            "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
            "name": "get_weather",
            "input": {
                "location": "Paris"
            }
        }
    ]
}
```

现在让我们继续对话并使用该工具

<CodeGroup>
```python Python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

```typescript TypeScript
// Extract thinking block and tool use block
const thinkingBlock = response.content.find(block =>
  block.type === 'thinking');
const toolUseBlock = response.content.find(block =>
  block.type === 'tool_use');

// Call your actual weather API, here is where your actual API call would go
// let's pretend this is what we get back
const weatherData = { temperature: 88 };

// Second request - Include thinking block and tool result
// No new thinking blocks will be generated in the response
const continuation = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" },
    // notice that the thinkingBlock is passed in as well as the toolUseBlock
    // if this is not passed in, an error is raised
    { role: "assistant", content: [thinkingBlock, toolUseBlock] },
    { role: "user", content: [{
      type: "tool_result",
      tool_use_id: toolUseBlock.id,
      content: `Current temperature: ${weatherData.temperature}°F`
    }]}
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;
import java.util.Optional;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingToolsResultExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        // Extract thinking block and tool use block
        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isThinking)
                .map(BetaContentBlock::asThinking)
                .findFirst();

        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isToolUse)
                .map(BetaContentBlock::asToolUse)
                .findFirst();

        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

            // Call your actual weather API, here is where your actual API call would go
            // let's pretend this is what we get back
            Map<String, Object> weatherData = Map.of("temperature", 88);

            // Second request - Include thinking block and tool result
            // No new thinking blocks will be generated in the response
            BetaMessage continuation = client.beta().messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_OPUS_4_0)
                            .maxTokens(16000)
                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                            .addTool(weatherTool)
                            .addUserMessage("What's the weather in Paris?")
                            .addAssistantMessageOfBetaContentBlockParams(
                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                    // if this is not passed in, an error is raised
                                    List.of(
                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                    )
                            )
                            .addUserMessageOfBetaContentBlockParams(List.of(
                                    BetaContentBlockParam.ofToolResult(
                                            BetaToolResultBlockParam.builder()
                                                    .toolUseId(toolUseBlock.id())
                                                    .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                    .build()
                                    )
                            ))
                            .build()
            );

            System.out.println(continuation);
        }
    }
}
```
</CodeGroup>

API 响应现在将**仅**包括文本

```json
{
    "content": [
        {
            "type": "text",
            "text": "Currently in Paris, the temperature is 88°F (31°C)"
        }
    ]
}
```

</section>

### 在对话中切换思考模式

您不能在助手回合的中间切换思考，包括在工具使用循环期间。整个助手回合必须在单一思考模式下运行：

- **如果启用了思考**，最终的助手回合必须以思考块开始。
- **如果禁用了思考**，最终的助手回合不能包含任何思考块

从模型的角度来看，**工具使用循环是助手回合的一部分**。助手回合直到 Claude 完成其完整响应（可能包括多个工具调用和结果）才完成。

例如，这个序列都是**单个助手回合**的一部分：
```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```

尽管有多个 API 消息，但工具使用循环在概念上是一个连续的助手响应的一部分。

#### 常见错误场景

您可能会遇到此错误：
```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```

这通常发生在以下情况：
1. 您在工具使用序列期间**禁用了**思考
2. 您想再次启用思考
3. 您的最后一条助手消息包含工具使用块但没有思考块

#### 实用指导

**✗ 无效：在工具使用后立即切换思考**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```

**✓ 有效：首先完成助手回合**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```

**最佳实践**：在每个回合开始时规划您的思考策略，而不是尝试在中间切换。

<Note>
切换思考模式也会使消息历史的提示缓存失效。有关更多详细信息，请参阅 [使用提示缓存的扩展思考](#extended-thinking-with-prompt-caching) 部分。
</Note>

<section title="示例：使用工具结果传递思考块">

这是一个实际示例，展示了在提供工具结果时如何保留思考块：

<CodeGroup>
```python Python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

```typescript TypeScript
const weatherTool = {
  name: "get_weather",
  description: "Get current weather for a location",
  input_schema: {
    type: "object",
    properties: {
      location: { type: "string" }
    },
    required: ["location"]
  }
};

// First request - Claude responds with thinking and tool request
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" }
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.beta.messages.BetaTool;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingWithToolsExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        System.out.println(response);
    }
}
```
</CodeGroup>

API 响应将包括思考、文本和 tool_use 块：

```json
{
    "content": [
        {
            "type": "thinking",
            "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
            "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
        },
        {
            "type": "text",
            "text": "I can help you get the current weather information for Paris. Let me check that for you"
        },
        {
            "type": "tool_use",
            "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
            "name": "get_weather",
            "input": {
                "location": "Paris"
            }
        }
    ]
}
```

现在让我们继续对话并使用该工具

<CodeGroup>
```python Python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

```typescript TypeScript
// Extract thinking block and tool use block
const thinkingBlock = response.content.find(block =>
  block.type === 'thinking');
const toolUseBlock = response.content.find(block =>
  block.type === 'tool_use');

// Call your actual weather API, here is where your actual API call would go
// let's pretend this is what we get back
const weatherData = { temperature: 88 };

// Second request - Include thinking block and tool result
// No new thinking blocks will be generated in the response
const continuation = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" },
    // notice that the thinkingBlock is passed in as well as the toolUseBlock
    // if this is not passed in, an error is raised
    { role: "assistant", content: [thinkingBlock, toolUseBlock] },
    { role: "user", content: [{
      type: "tool_result",
      tool_use_id: toolUseBlock.id,
      content: `Current temperature: ${weatherData.temperature}°F`
    }]}
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;
import java.util.Optional;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingToolsResultExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        // Extract thinking block and tool use block
        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isThinking)
                .map(BetaContentBlock::asThinking)
                .findFirst();

        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isToolUse)
                .map(BetaContentBlock::asToolUse)
                .findFirst();

        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

            // Call your actual weather API, here is where your actual API call would go
            // let's pretend this is what we get back
            Map<String, Object> weatherData = Map.of("temperature", 88);

            // Second request - Include thinking block and tool result
            // No new thinking blocks will be generated in the response
            BetaMessage continuation = client.beta().messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_OPUS_4_0)
                            .maxTokens(16000)
                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                            .addTool(weatherTool)
                            .addUserMessage("What's the weather in Paris?")
                            .addAssistantMessageOfBetaContentBlockParams(
                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                    // if this is not passed in, an error is raised
                                    List.of(
                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                    )
                            )
                            .addUserMessageOfBetaContentBlockParams(List.of(
                                    BetaContentBlockParam.ofToolResult(
                                            BetaToolResultBlockParam.builder()
                                                    .toolUseId(toolUseBlock.id())
                                                    .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                    .build()
                                    )
                            ))
                            .build()
            );

            System.out.println(continuation);
        }
    }
}
```
</CodeGroup>

API 响应现在**仅**包括文本

```json
{
    "content": [
        {
            "type": "text",
            "text": "Currently in Paris, the temperature is 88°F (31°C)"
        }
    ]
}
```

</section>

### 保留思考块

在工具使用期间，您必须将 `thinking` 块传递回 API，并且必须将完整的未修改块包含回 API。这对于维持模型的推理流和对话完整性至关重要。

<Tip>
虽然您可以从先前的 `assistant` 角色回合中省略 `thinking` 块，但我们建议对于任何多轮对话始终将所有思考块传递回 API。API 将：
- 自动过滤提供的思考块
- 使用必要的相关思考块来保留模型的推理
- 仅对显示给 Claude 的块的输入令牌进行计费
</Tip>

<Note>
在对话期间切换思考模式时，请记住整个助手回合（包括工具使用循环）必须在单一思考模式下运行。有关更多详细信息，请参阅 [在对话中切换思考模式](#toggling-thinking-modes-in-conversations)。
</Note>

当 Claude 调用工具时，它暂停了其响应的构建以等待外部信息。当工具结果返回时，Claude 将继续构建该现有响应。这需要在工具使用期间保留思考块，原因有几个：

1. **推理连续性**：思考块捕获了导致工具请求的 Claude 的逐步推理。当您发布工具结果时，包括原始思考可以确保 Claude 可以从中断的地方继续其推理。

2. **上下文维护**：虽然工具结果在 API 结构中显示为用户消息，但它们是连续推理流的一部分。保留思考块在多个 API 调用中维持这个概念流。有关上下文管理的更多信息，请参阅我们的 [上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)。

**重要**：提供 `thinking` 块时，连续 `thinking` 块的整个序列必须与模型在原始请求期间生成的输出相匹配；您不能重新排列或修改这些块的序列。

### 交错思考

Claude 4 模型中的扩展思考与工具使用支持交错思考，这使 Claude 能够在工具调用之间进行思考，并在接收工具结果后进行更复杂的推理。

通过交错思考，Claude 可以：
- 在工具调用结果后进行推理，然后决定接下来做什么
- 在推理步骤之间链接多个工具调用
- 根据中间结果做出更细致的决策

要启用交错思考，请将[测试版标头](/docs/zh-CN/api/beta-headers) `interleaved-thinking-2025-05-14` 添加到您的 API 请求中。

以下是交错思考的一些重要注意事项：
- 使用交错思考时，`budget_tokens` 可以超过 `max_tokens` 参数，因为它代表一个助手轮次内所有思考块的总预算。
- 交错思考仅支持[通过消息 API 使用的工具](/docs/zh-CN/agents-and-tools/tool-use/overview)。
- 交错思考仅支持 Claude 4 模型，使用测试版标头 `interleaved-thinking-2025-05-14`。
- 直接调用 Claude API 允许您在对任何模型的请求中传递 `interleaved-thinking-2025-05-14`，但不会产生任何效果。
- 在第三方平台上（例如，[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock) 和 [Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)），如果您将 `interleaved-thinking-2025-05-14` 传递给除 Claude Opus 4.5、Claude Opus 4.1、Opus 4 或 Sonnet 4 之外的任何模型，您的请求将失败。

<section title="不使用交错思考的工具使用">

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 定义工具
calculator_tool = {
    "name": "calculator",
    "description": "Perform mathematical calculations",
    "input_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
            }
        },
        "required": ["expression"]
    }
}

database_tool = {
    "name": "database_query",
    "description": "Query product database",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "SQL query to execute"
            }
        },
        "required": ["query"]
    }
}

# 第一个请求 - Claude 在所有工具调用之前思考一次
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    messages=[{
        "role": "user",
        "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    }]
)

# 响应包括思考，然后是工具使用
# 注意：Claude 在开始时思考一次，然后做出所有工具决策
print("First response:")
for block in response.content:
    if block.type == "thinking":
        print(f"Thinking (summarized): {block.thinking}")
    elif block.type == "tool_use":
        print(f"Tool use: {block.name} with input {block.input}")
    elif block.type == "text":
        print(f"Text: {block.text}")

# 您将执行工具并返回结果...
# 获得两个工具结果后，Claude 直接响应而不进行额外思考
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 定义工具
const calculatorTool = {
  name: "calculator",
  description: "Perform mathematical calculations",
  input_schema: {
    type: "object",
    properties: {
      expression: {
        type: "string",
        description: "Mathematical expression to evaluate"
      }
    },
    required: ["expression"]
  }
};

const databaseTool = {
  name: "database_query",
  description: "Query product database",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "SQL query to execute"
      }
    },
    required: ["query"]
  }
};

// 第一个请求 - Claude 在所有工具调用之前思考一次
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [{
    role: "user",
    content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
  }]
});

// 响应包括思考，然后是工具使用
// 注意：Claude 在开始时思考一次，然后做出所有工具决策
console.log("First response:");
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`Thinking (summarized): ${block.thinking}`);
  } else if (block.type === "tool_use") {
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  } else if (block.type === "text") {
    console.log(`Text: ${block.text}`);
  }
}

// 您将执行工具并返回结果...
// 获得两个工具结果后，Claude 直接响应而不进行额外思考
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.messages.Model;
import java.util.List;
import java.util.Map;

public class NonInterleavedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 定义计算器工具
        BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "expression", Map.of(
                                "type", "string",
                                "description", "Mathematical expression to evaluate"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                .build();

        BetaTool calculatorTool = BetaTool.builder()
                .name("calculator")
                .description("Perform mathematical calculations")
                .inputSchema(calculatorSchema)
                .build();

        // 定义数据库工具
        BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "SQL query to execute"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        BetaTool databaseTool = BetaTool.builder()
                .name("database_query")
                .description("Query product database")
                .inputSchema(databaseSchema)
                .build();

        // 第一个请求 - Claude 在所有工具调用之前思考一次
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .build()
        );

        // 响应包括思考，然后是工具使用
        // 注意：Claude 在开始时思考一次，然后做出所有工具决策
        System.out.println("First response:");
        for (BetaContentBlock block : response.content()) {
            if (block.isThinking()) {
                System.out.println("Thinking (summarized): " + block.asThinking().thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            } else if (block.isText()) {
                System.out.println("Text: " + block.asText().text());
            }
        }

        // 您将执行工具并返回结果...
        // 获得两个工具结果后，Claude 直接响应而不进行额外思考
    }
}
```
</CodeGroup>

在这个不使用交错思考的示例中：
1. Claude 在开始时思考一次以理解任务
2. 提前做出所有工具使用决策
3. 当工具结果返回时，Claude 立即提供响应而不进行额外思考

</section>

<section title="使用交错思考的工具使用">

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 与之前相同的工具定义
calculator_tool = {
    "name": "calculator",
    "description": "Perform mathematical calculations",
    "input_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
            }
        },
        "required": ["expression"]
    }
}

database_tool = {
    "name": "database_query",
    "description": "Query product database",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "SQL query to execute"
            }
        },
        "required": ["query"]
    }
}

# 第一个请求，启用交错思考
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[{
        "role": "user",
        "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    }]
)

print("Initial response:")
thinking_blocks = []
tool_use_blocks = []

for block in response.content:
    if block.type == "thinking":
        thinking_blocks.append(block)
        print(f"Thinking: {block.thinking}")
    elif block.type == "tool_use":
        tool_use_blocks.append(block)
        print(f"Tool use: {block.name} with input {block.input}")
    elif block.type == "text":
        print(f"Text: {block.text}")

# 第一个工具结果（计算器）
calculator_result = "7500"  # 150 * 50

# 继续第一个工具结果
response2 = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[
        {
            "role": "user",
            "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        },
        {
            "role": "assistant",
            "content": [thinking_blocks[0], tool_use_blocks[0]]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[0].id,
                "content": calculator_result
            }]
        }
    ]
)

print("\nAfter calculator result:")
# 使用交错思考，Claude 可以思考计算器结果
# 然后决定是否查询数据库
for block in response2.content:
    if block.type == "thinking":
        thinking_blocks.append(block)
        print(f"Interleaved thinking: {block.thinking}")
    elif block.type == "tool_use":
        tool_use_blocks.append(block)
        print(f"Tool use: {block.name} with input {block.input}")

# 第二个工具结果（数据库）
database_result = "5200"  # 示例平均月收入

# 继续第二个工具结果
response3 = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[
        {
            "role": "user",
            "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        },
        {
            "role": "assistant",
            "content": [thinking_blocks[0], tool_use_blocks[0]]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[0].id,
                "content": calculator_result
            }]
        },
        {
            "role": "assistant",
            "content": thinking_blocks[1:] + tool_use_blocks[1:]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[1].id,
                "content": database_result
            }]
        }
    ]
)

print("\nAfter database result:")
# 使用交错思考，Claude 可以思考两个结果
# 然后制定最终响应
for block in response3.content:
    if block.type == "thinking":
        print(f"Final thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Final response: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 与之前相同的工具定义
const calculatorTool = {
  name: "calculator",
  description: "Perform mathematical calculations",
  input_schema: {
    type: "object",
    properties: {
      expression: {
        type: "string",
        description: "Mathematical expression to evaluate"
      }
    },
    required: ["expression"]
  }
};

const databaseTool = {
  name: "database_query",
  description: "Query product database",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "SQL query to execute"
      }
    },
    required: ["query"]
  }
};

// 第一个请求，启用交错思考
const response = await client.beta.messages.create({
  // 启用交错思考
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [{
    role: "user",
    content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
  }]
});

console.log("Initial response:");
const thinkingBlocks = [];
const toolUseBlocks = [];

for (const block of response.content) {
  if (block.type === "thinking") {
    thinkingBlocks.push(block);
    console.log(`Thinking: ${block.thinking}`);
  } else if (block.type === "tool_use") {
    toolUseBlocks.push(block);
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  } else if (block.type === "text") {
    console.log(`Text: ${block.text}`);
  }
}

// 第一个工具结果（计算器）
const calculatorResult = "7500"; // 150 * 50

// 继续第一个工具结果
const response2 = await client.beta.messages.create({
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [
    {
      role: "user",
      content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    },
    {
      role: "assistant",
      content: [thinkingBlocks[0], toolUseBlocks[0]]
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[0].id,
        content: calculatorResult
      }]
    }
  ]
});

console.log("\nAfter calculator result:");
// 使用交错思考，Claude 可以思考计算器结果
// 然后决定是否查询数据库
for (const block of response2.content) {
  if (block.type === "thinking") {
    thinkingBlocks.push(block);
    console.log(`Interleaved thinking: ${block.thinking}`);
  } else if (block.type === "tool_use") {
    toolUseBlocks.push(block);
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  }
}

// 第二个工具结果（数据库）
const databaseResult = "5200"; // 示例平均月收入

// 继续第二个工具结果
const response3 = await client.beta.messages.create({
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [
    {
      role: "user",
      content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    },
    {
      role: "assistant",
      content: [thinkingBlocks[0], toolUseBlocks[0]]
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[0].id,
        content: calculatorResult
      }]
    },
    {
      role: "assistant",
      content: thinkingBlocks.slice(1).concat(toolUseBlocks.slice(1))
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[1].id,
        content: databaseResult
      }]
    }
  ]
});

console.log("\nAfter database result:");
// 使用交错思考，Claude 可以思考两个结果
// 然后制定最终响应
for (const block of response3.content) {
  if (block.type === "thinking") {
    console.log(`Final thinking: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`Final response: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.messages.Model;
import java.util.*;
import static java.util.stream.Collectors.toList;

public class InterleavedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 定义计算器工具
        BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "expression", Map.of(
                                "type", "string",
                                "description", "Mathematical expression to evaluate"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                .build();

        BetaTool calculatorTool = BetaTool.builder()
                .name("calculator")
                .description("Perform mathematical calculations")
                .inputSchema(calculatorSchema)
                .build();

        // 定义数据库工具
        BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "SQL query to execute"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        BetaTool databaseTool = BetaTool.builder()
                .name("database_query")
                .description("Query product database")
                .inputSchema(databaseSchema)
                .build();

        // 第一个请求，启用交错思考
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        // 使用测试版标头启用交错思考
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .build()
        );

        System.out.println("Initial response:");
        List<BetaThinkingBlock> thinkingBlocks = new ArrayList<>();
        List<BetaToolUseBlock> toolUseBlocks = new ArrayList<>();

        for (BetaContentBlock block : response.content()) {
            if (block.isThinking()) {
                BetaThinkingBlock thinking = block.asThinking();
                thinkingBlocks.add(thinking);
                System.out.println("Thinking: " + thinking.thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                toolUseBlocks.add(toolUse);
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            } else if (block.isText()) {
                System.out.println("Text: " + block.asText().text());
            }
        }

        // 第一个工具结果（计算器）
        String calculatorResult = "7500"; // 150 * 50

        // 继续第一个工具结果
        BetaMessage response2 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .addAssistantMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                        ))
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(0).id())
                                                .content(calculatorResult)
                                                .build()
                                )
                        ))
                        .build()
        );

        System.out.println("\nAfter calculator result:");
        // 使用交错思考，Claude 可以思考计算器结果
        // 然后决定是否查询数据库
        for (BetaContentBlock block : response2.content()) {
            if (block.isThinking()) {
                BetaThinkingBlock thinking = block.asThinking();
                thinkingBlocks.add(thinking);
                System.out.println("Interleaved thinking: " + thinking.thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                toolUseBlocks.add(toolUse);
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            }
        }

        // 第二个工具结果（数据库）
        String databaseResult = "5200"; // 示例平均月收入

        // 准备助手消息的组合内容
        List<BetaContentBlockParam> combinedContent = new ArrayList<>();
        for (int i = 1; i < thinkingBlocks.size(); i++) {
            combinedContent.add(BetaContentBlockParam.ofThinking(thinkingBlocks.get(i).toParam()));
        }
        for (int i = 1; i < toolUseBlocks.size(); i++) {
            combinedContent.add(BetaContentBlockParam.ofToolUse(toolUseBlocks.get(i).toParam()));
        }

        // 继续第二个工具结果
        BetaMessage response3 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .addAssistantMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                        ))
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(0).id())
                                                .content(calculatorResult)
                                                .build()
                                )
                        ))
                        .addAssistantMessageOfBetaContentBlockParams(combinedContent)
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(1).id())
                                                .content(databaseResult)
                                                .build()
                                )
                        ))
                        .build()
        );

        System.out.println("\nAfter database result:");
        // 使用交错思考，Claude 可以思考两个结果
        // 然后制定最终响应
        for (BetaContentBlock block : response3.content()) {
            if (block.isThinking()) {
                System.out.println("Final thinking: " + block.asThinking().thinking());
            } else if (block.isText()) {
                System.out.println("Final response: " + block.asText().text());
            }
        }
    }
}
```
</CodeGroup>

在这个使用交错思考的示例中：
1. Claude 最初思考任务
2. 收到计算器结果后，Claude 可以再次思考该结果的含义
3. Claude 然后根据第一个结果决定如何查询数据库
4. 收到数据库结果后，Claude 再思考一次两个结果，然后制定最终响应
5. 思考预算分布在该轮次内的所有思考块中

这种模式允许更复杂的推理链，其中每个工具的输出为下一个决策提供信息。

</section>

## 扩展思考与提示缓存

[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)与思考有几个重要的注意事项：

<Tip>
扩展思考任务通常需要超过 5 分钟才能完成。考虑使用[1 小时缓存持续时间](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration)来维护跨越较长思考会话和多步骤工作流的缓存命中。
</Tip>

**思考块上下文移除**
- 来自先前轮次的思考块从上下文中移除，这可能会影响缓存断点
- 继续使用工具的对话时，思考块被缓存并在从缓存读取时计为输入令牌
- 这产生了一个权衡：虽然思考块在视觉上不消耗上下文窗口空间，但在缓存时仍然计入您的输入令牌使用
- 如果思考被禁用，如果您在当前工具使用轮次中传递思考内容，请求将失败。在其他情况下，传递给 API 的思考内容将被忽略

**缓存失效模式**
- 思考参数的更改（启用/禁用或预算分配）会使消息缓存断点失效
- [交错思考](#interleaved-thinking)放大了缓存失效，因为思考块可能发生在多个[工具调用](#extended-thinking-with-tool-use)之间
- 系统提示和工具保持缓存，尽管思考参数更改或块移除

<Note>
虽然思考块被移除用于缓存和上下文计算，但在继续使用[工具使用](#extended-thinking-with-tool-use)的对话时必须保留它们，特别是使用[交错思考](#interleaved-thinking)时。
</Note>

## 扩展思考与提示缓存

[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)与思考有几个重要的考虑事项：

<Tip>
扩展思考任务通常需要超过5分钟才能完成。考虑使用[1小时缓存时长](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration)来维持跨越更长思考会话和多步骤工作流的缓存命中。
</Tip>

**思考块上下文移除**
- 来自前面轮次的思考块会从上下文中移除，这可能会影响缓存断点
- 当继续使用工具的对话时，思考块会被缓存，并在从缓存读取时计为输入令牌
- 这产生了一个权衡：虽然思考块在视觉上不消耗上下文窗口空间，但在缓存时仍然计入您的输入令牌使用量
- 如果思考被禁用，如果您在当前工具使用轮次中传递思考内容，请求将失败。在其他上下文中，传递给API的思考内容会被简单地忽略

**缓存失效模式**
- 对思考参数的更改（启用/禁用或预算分配）会使消息缓存断点失效
- [交错思考](#interleaved-thinking)会放大缓存失效，因为思考块可能出现在多个[工具调用](#extended-thinking-with-tool-use)之间
- 系统提示和工具尽管思考参数更改或块移除，仍保持缓存

<Note>
虽然思考块会因缓存和上下文计算而被移除，但在继续使用[工具使用](#extended-thinking-with-tool-use)的对话时，特别是使用[交错思考](#interleaved-thinking)时，必须保留它们。
</Note>

### 理解思考块缓存行为

当使用扩展思考与工具使用时，思考块表现出特定的缓存行为，这会影响令牌计数：

**工作原理：**

1. 仅当您发出包含工具结果的后续请求时，才会发生缓存
2. 当发出后续请求时，之前的对话历史（包括思考块）可以被缓存
3. 这些缓存的思考块在从缓存读取时计为您使用指标中的输入令牌
4. 当包含非工具结果用户块时，所有之前的思考块都会被忽略并从上下文中移除

**详细示例流程：**

**请求1：**
```
User: "What's the weather in Paris?"
```
**响应1：**
```
[thinking_block_1] + [tool_use block 1]
```

**请求2：**
```
User: ["What's the weather in Paris?"], 
Assistant: [thinking_block_1] + [tool_use block 1], 
User: [tool_result_1, cache=True]
```
**响应2：**
```
[thinking_block_2] + [text block 2]
```
请求2写入请求内容的缓存（不是响应）。缓存包括原始用户消息、第一个思考块、工具使用块和工具结果。

**请求3：**
```
User: ["What's the weather in Paris?"],
Assistant: [thinking_block_1] + [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [thinking_block_2] + [text block 2],
User: [Text response, cache=True]
```
对于Claude Opus 4.5及更高版本，默认情况下保留所有之前的思考块。对于较旧的模型，由于包含了非工具结果用户块，所有之前的思考块都会被忽略。此请求将按以下方式处理：
```
User: ["What's the weather in Paris?"],
Assistant: [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [text block 2],
User: [Text response, cache=True]
```

**关键点：**
- 此缓存行为会自动发生，即使没有显式的`cache_control`标记
- 此行为在使用常规思考或交错思考时是一致的

<section title="系统提示缓存（思考更改时保留）">

<CodeGroup>
```python Python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

SYSTEM_PROMPT=[
    {
        "type": "text",
        "text": "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
    },
    {
        "type": "text",
        "text": LARGE_TEXT,
        "cache_control": {"type": "ephemeral"}
    }
]

MESSAGES = [
    {
        "role": "user",
        "content": "Analyze the tone of this passage."
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

# Third request - different thinking parameters (cache miss for messages)
print("\nThird request - different thinking parameters (cache miss for messages)")
response3 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Changed thinking budget
    },
    system=SYSTEM_PROMPT,  # System prompt remains cached
    messages=MESSAGES  # Messages cache is invalidated
)

print(f"Third response usage: {response3.usage}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import axios from 'axios';
import * as cheerio from 'cheerio';

const client = new Anthropic();

async function fetchArticleContent(url: string): Promise<string> {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);
  
  // Remove script and style elements
  $('script, style').remove();
  
  // Get text
  let text = $.text();
  
  // Break into lines and remove leading and trailing space on each
  const lines = text.split('\n').map(line => line.trim());
  // Drop blank lines
  text = lines.filter(line => line.length > 0).join('\n');
  
  return text;
}

// Fetch the content of the article
const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
const bookContent = await fetchArticleContent(bookUrl);
// Use just enough text for caching (first few chapters)
const LARGE_TEXT = bookContent.slice(0, 5000);

const SYSTEM_PROMPT = [
  {
    type: "text",
    text: "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
  },
  {
    type: "text",
    text: LARGE_TEXT,
    cache_control: { type: "ephemeral" }
  }
];

const MESSAGES = [
  {
    role: "user",
    content: "Analyze the tone of this passage."
  }
];

// First request - establish cache
console.log("First request - establishing cache");
const response1 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 4000
  },
  system: SYSTEM_PROMPT,
  messages: MESSAGES
});

console.log(`First response usage: ${response1.usage}`);

MESSAGES.push({
  role: "assistant",
  content: response1.content
});
MESSAGES.push({
  role: "user",
  content: "Analyze the characters in this passage."
});

// Second request - same thinking parameters (cache hit expected)
console.log("\nSecond request - same thinking parameters (cache hit expected)");
const response2 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 4000
  },
  system: SYSTEM_PROMPT,
  messages: MESSAGES
});

console.log(`Second response usage: ${response2.usage}`);

// Third request - different thinking parameters (cache miss for messages)
console.log("\nThird request - different thinking parameters (cache miss for messages)");
const response3 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 8000  // Changed thinking budget
  },
  system: SYSTEM_PROMPT,  // System prompt remains cached
  messages: MESSAGES  // Messages cache is invalidated
});

console.log(`Third response usage: ${response3.usage}`);
```
</CodeGroup>

</section>
<section title="消息缓存（思考更改时失效）">

<CodeGroup>
```python Python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

# No system prompt - caching in messages instead
MESSAGES = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": LARGE_TEXT,
                "cache_control": {"type": "ephemeral"},
            },
            {
                "type": "text",
                "text": "Analyze the tone of this passage."
            }
        ]
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000  # Same thinking budget
    },
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response2.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the setting in this passage."
})

# Third request - different thinking budget (cache miss expected)
print("\nThird request - different thinking budget (cache miss expected)")
response3 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Different thinking budget breaks cache
    },
    messages=MESSAGES
)

print(f"Third response usage: {response3.usage}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import axios from 'axios';
import * as cheerio from 'cheerio';

const client = new Anthropic();

async function fetchArticleContent(url: string): Promise<string> {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);

  // Remove script and style elements
  $('script, style').remove();

  // Get text
  let text = $.text();

  // Clean up text (break into lines, remove whitespace)
  const lines = text.split('\n').map(line => line.trim());
  const chunks = lines.flatMap(line => line.split('  ').map(phrase => phrase.trim()));
  text = chunks.filter(chunk => chunk).join('\n');

  return text;
}

async function main() {
  // Fetch the content of the article
  const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
  const bookContent = await fetchArticleContent(bookUrl);
  // Use just enough text for caching (first few chapters)
  const LARGE_TEXT = bookContent.substring(0, 5000);

  // No system prompt - caching in messages instead
  let MESSAGES = [
    {
      role: "user",
      content: [
        {
          type: "text",
          text: LARGE_TEXT,
          cache_control: {type: "ephemeral"},
        },
        {
          type: "text",
          text: "Analyze the tone of this passage."
        }
      ]
    }
  ];

  // First request - establish cache
  console.log("First request - establishing cache");
  const response1 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 4000
    },
    messages: MESSAGES
  });

  console.log(`First response usage: `, response1.usage);

  MESSAGES = [
    ...MESSAGES,
    {
      role: "assistant",
      content: response1.content
    },
    {
      role: "user",
      content: "Analyze the characters in this passage."
    }
  ];

  // Second request - same thinking parameters (cache hit expected)
  console.log("\nSecond request - same thinking parameters (cache hit expected)");
  const response2 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 4000  // Same thinking budget
    },
    messages: MESSAGES
  });

  console.log(`Second response usage: `, response2.usage);

  MESSAGES = [
    ...MESSAGES,
    {
      role: "assistant",
      content: response2.content
    },
    {
      role: "user",
      content: "Analyze the setting in this passage."
    }
  ];

  // Third request - different thinking budget (cache miss expected)
  console.log("\nThird request - different thinking budget (cache miss expected)");
  const response3 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 8000  // Different thinking budget breaks cache
    },
    messages: MESSAGES
  });

  console.log(`Third response usage: `, response3.usage);
}

main().catch(console.error);
```

```java Java
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.net.URL;
import java.util.Arrays;
import java.util.regex.Pattern;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;

import static java.util.stream.Collectors.joining;
import static java.util.stream.Collectors.toList;

public class ThinkingCacheExample {
    public static void main(String[] args) throws IOException {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // Fetch the content of the article
        String bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
        String bookContent = fetchArticleContent(bookUrl);
        // Use just enough text for caching (first few chapters)
        String largeText = bookContent.substring(0, 5000);

        List<BetaTextBlockParam> systemPrompt = List.of(
                BetaTextBlockParam.builder()
                        .text("You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.")
                        .build(),
                BetaTextBlockParam.builder()
                        .text(largeText)
                        .cacheControl(BetaCacheControlEphemeral.builder().build())
                        .build()
        );

        List<BetaMessageParam> messages = new ArrayList<>();
        messages.add(BetaMessageParam.builder()
                .role(BetaMessageParam.Role.USER)
                .content("Analyze the tone of this passage.")
                .build());

        // First request - establish cache
        System.out.println("First request - establishing cache");
        BetaMessage response1 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .messages(messages)
                        .build()
        );

        System.out.println("First response usage: " + response1.usage());

        // Second request - same thinking parameters (cache hit expected)
        System.out.println("\nSecond request - same thinking parameters (cache hit expected)");
        BetaMessage response2 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .addMessage(response1)
                        .addUserMessage("Analyze the characters in this passage.")
                        .messages(messages)
                        .build()
        );

        System.out.println("Second response usage: " + response2.usage());

        // Third request - different thinking budget (cache hit expected because system prompt caching)
        System.out.println("\nThird request - different thinking budget (cache hit expected)");
        BetaMessage response3 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(8000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .addMessage(response1)
                        .addUserMessage("Analyze the characters in this passage.")
                        .addMessage(response2)
                        .addUserMessage("Analyze the setting in this passage.")
                        .build()
        );

        System.out.println("Third response usage: " + response3.usage());
    }

    private static String fetchArticleContent(String url) throws IOException {
        // Fetch HTML content
        String htmlContent = fetchHtml(url);

        // Remove script and style elements
        String noScriptStyle = removeElements(htmlContent, "script", "style");

        // Extract text (simple approach - remove HTML tags)
        String text = removeHtmlTags(noScriptStyle);

        // Clean up text (break into lines, remove whitespace)
        List<String> lines = Arrays.asList(text.split("\n"));
        List<String> trimmedLines = lines.stream()
                .map(String::trim)
                .collect(toList());

        // Split on double spaces and flatten
        List<String> chunks = trimmedLines.stream()
                .flatMap(line -> Arrays.stream(line.split("  "))
                        .map(String::trim))
                .collect(toList());

        // Filter empty chunks and join with newlines
        return chunks.stream()
                .filter(chunk -> !chunk.isEmpty())
                .collect(joining("\n"));
    }

    /**
     * Fetches HTML content from a URL
     */
    private static String fetchHtml(String urlString) throws IOException {
        try (InputStream inputStream = new URL(urlString).openStream()) {
            StringBuilder content = new StringBuilder();
            try (BufferedReader reader = new BufferedReader(
                    new InputStreamReader(inputStream))) {
                String line;
                while ((line = reader.readLine()) != null) {
                    content.append(line).append("\n");
                }
            }
            return content.toString();
        }
    }

    /**
     * Removes specified HTML elements and their content
     */
    private static String removeElements(String html, String... elementNames) {
        String result = html;
        for (String element : elementNames) {
            // Pattern to match <element>...</element> and self-closing tags
            String pattern = "<" + element + "\\s*[^>]*>.*?</" + element + ">|<" + element + "\\s*[^>]*/?>";
            result = Pattern.compile(pattern, Pattern.DOTALL).matcher(result).replaceAll("");
        }
        return result;
    }

    /**
     * Removes all HTML tags from content
     */
    private static String removeHtmlTags(String html) {
        // Replace <br> and <p> tags with newlines for better text formatting
        String withLineBreaks = html.replaceAll("<br\\s*/?\\s*>|</?p\\s*[^>]*>", "\n");

        // Remove remaining HTML tags
        String noTags = withLineBreaks.replaceAll("<[^>]*>", "");

        // Decode HTML entities (simplified for common entities)
        return decodeHtmlEntities(noTags);
    }

    /**
     * Simple HTML entity decoder for common entities
     */
    private static String decodeHtmlEntities(String text) {
        return text
                .replaceAll("&nbsp;", " ")
                .replaceAll("&amp;", "&")
                .replaceAll("&lt;", "<")
                .replaceAll("&gt;", ">")
                .replaceAll("&quot;", "\"")
                .replaceAll("&#39;", "'")
                .replaceAll("&hellip;", "...")
                .replaceAll("&mdash;", "—");
    }

}
```
</CodeGroup>

脚本的输出如下（您可能会看到略有不同的数字）

```
First request - establishing cache
First response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }

Second request - same thinking parameters (cache hit expected)

Second response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }

Third request - different thinking budget (cache miss expected)
Third response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }
```

此示例演示了当缓存在消息数组中设置时，更改思考参数（预算令牌从4000增加到8000）**使缓存失效**。第三个请求显示没有缓存命中，`cache_creation_input_tokens=1370`和`cache_read_input_tokens=0`，证明当思考参数更改时，基于消息的缓存会失效。

</section>

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 扩展思考与工具使用中的上下文窗口

当使用扩展思考与工具使用时，思考块必须显式保留并与工具结果一起返回。

扩展思考与工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下图说明了扩展思考与工具使用的令牌管理：

![扩展思考和工具使用的上下文窗口图](/docs/images/context-window-thinking-tools.svg)

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 扩展思考与工具使用中的上下文窗口

当使用扩展思考与工具使用时，思考块必须显式保留并与工具结果一起返回。

扩展思考与工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下图说明了扩展思考与工具使用的令牌管理：

![扩展思考和工具使用的上下文窗口图](/docs/images/context-window-thinking-tools.svg)

### 使用扩展思考管理令牌

鉴于Claude 3.7和4模型的上下文窗口和`max_tokens`行为与扩展思考，您可能需要：

- 更积极地监控和管理您的令牌使用
- 随着提示长度的变化调整`max_tokens`值
- 可能更频繁地使用[令牌计数端点](/docs/zh-CN/build-with-claude/token-counting)
- 意识到之前的思考块不会在您的上下文窗口中累积

这一变化是为了提供更可预测和透明的行为，特别是随着最大令牌限制显著增加。

## 最大令牌数和扩展思考的上下文窗口大小

在较早的 Claude 模型（Claude Sonnet 3.7 之前），如果提示令牌和 `max_tokens` 的总和超过了模型的上下文窗口，系统会自动调整 `max_tokens` 以适应上下文限制。这意味着您可以设置一个较大的 `max_tokens` 值，系统会根据需要静默地减少它。

使用 Claude 3.7 和 4 模型时，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌 + `max_tokens` 超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 启用扩展思考的上下文窗口

在计算启用思考的上下文窗口使用情况时，需要注意以下几点：

- 来自先前轮次的思考块被剥离，不计入您的上下文窗口
- 当前轮次的思考计入该轮次的 `max_tokens` 限制

下面的图表演示了启用扩展思考时的专门令牌管理：

![启用扩展思考的上下文窗口图表](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数 API](/docs/zh-CN/build-with-claude/token-counting)来获得您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 启用扩展思考和工具使用的上下文窗口

使用扩展思考和工具使用时，思考块必须被显式保留并与工具结果一起返回。

启用扩展思考和工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下面的图表说明了扩展思考和工具使用的令牌管理：

![启用扩展思考和工具使用的上下文窗口图表](/docs/images/context-window-thinking-tools.svg)

### 使用扩展思考管理令牌

考虑到 Claude 3.7 和 4 模型的上下文窗口和 `max_tokens` 行为与扩展思考的关系，您可能需要：

- 更主动地监控和管理您的令牌使用
- 随着提示长度的变化调整 `max_tokens` 值
- 可能需要更频繁地使用[令牌计数端点](/docs/zh-CN/build-with-claude/token-counting)
- 意识到先前的思考块不会在您的上下文窗口中累积

做出这一改变是为了提供更可预测和透明的行为，特别是因为最大令牌限制已大幅增加。

## 思考加密

完整的思考内容被加密并在 `signature` 字段中返回。该字段用于验证思考块是由 Claude 生成的，当传回 API 时使用。

<Note>
只有在使用[带扩展思考的工具](#extended-thinking-with-tool-use)时才严格需要发送回思考块。否则，您可以省略先前轮次的思考块，或者如果您传回它们，让 API 为您剥离它们。

如果发送回思考块，我们建议按照您接收的方式传回所有内容以保持一致性并避免潜在问题。
</Note>

以下是关于思考加密的一些重要注意事项：
- 当[流式传输响应](#streaming-thinking)时，签名通过 `content_block_delta` 事件中的 `signature_delta` 添加，位于 `content_block_stop` 事件之前。
- `signature` 值在 Claude 4 模型中的长度明显长于之前的模型。
- `signature` 字段是一个不透明字段，不应被解释或解析 - 它仅用于验证目的。
- `signature` 值在各平台上兼容（Claude API、[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock) 和 [Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)）。在一个平台上生成的值将与另一个平台兼容。

### 思考编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密 `thinking` 块的部分或全部内容，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 意识到编辑的思考块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的一些内部推理已自动加密以确保安全。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉编辑的块，同时保留正常的思考块
- 透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理以优雅地管理编辑的思考，而不会破坏您的 UI

以下是显示正常和编辑思考块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到编辑的思考块是预期的行为。该模型仍然可以使用这个编辑的推理来为其响应提供信息，同时保持安全护栏。

如果您需要在应用程序中测试编辑的思考处理，您可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手轮次的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用编辑的思考块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发编辑的思考（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别编辑的思考块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块仍然可用于后续请求

    # 提取所有块（编辑和非编辑）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 在传递到后续请求时，包括所有块而不进行修改
    # 这保留了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发编辑的思考（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别编辑的思考块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块仍然可用于后续请求

  // 提取所有块（编辑和非编辑）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 在传递到后续请求时，包括所有块而不进行修改
  // 这保留了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发编辑的思考（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别编辑的思考块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块仍然可用于后续请求
            // 提取所有块（编辑和非编辑）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 在传递到后续请求时，包括所有块而不进行修改
            // 这保留了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

### 思考编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密 `thinking` 块的部分或全部内容，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 意识到编辑的思考块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的一些内部推理已自动加密以确保安全。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉编辑的块，同时保留正常的思考块
- 透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理以优雅地管理编辑的思考，而不会破坏您的 UI

以下是显示正常和编辑思考块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到编辑的思考块是预期的行为。该模型仍然可以使用这个编辑的推理来为其响应提供信息，同时保持安全护栏。

如果您需要在应用程序中测试编辑的思考处理，您可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手轮次的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用编辑的思考块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发编辑的思考（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别编辑的思考块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块仍然可用于后续请求

    # 提取所有块（编辑和非编辑）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 在传递到后续请求时，包括所有块而不进行修改
    # 这保留了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发编辑的思考（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别编辑的思考块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块仍然可用于后续请求

  // 提取所有块（编辑和非编辑）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 在传递到后续请求时，包括所有块而不进行修改
  // 这保留了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发编辑的思考（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别编辑的思考块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块仍然可用于后续请求
            // 提取所有块（编辑和非编辑）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 在传递到后续请求时，包括所有块而不进行修改
            // 这保留了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

## 不同模型版本之间的思考差异

Messages API 在 Claude Sonnet 3.7 和 Claude 4 模型之间处理思考的方式不同，主要在编辑和总结行为方面。

请参阅下表以获得简明比较：

| 功能 | Claude Sonnet 3.7 | Claude 4 模型（Opus 4.5 之前） | Claude Opus 4.5 及更高版本 |
|---------|------------------|-------------------------------|--------------------------|
| **思考输出** | 返回完整思考输出 | 返回总结的思考 | 返回总结的思考 |
| **交错思考** | 不支持 | 支持 `interleaved-thinking-2025-05-14` beta 标头 | 支持 `interleaved-thinking-2025-05-14` beta 标头 |
| **思考块保留** | 不跨轮保留 | 不跨轮保留 | **默认保留**（启用缓存优化、令牌节省） |

### Claude Opus 4.5 中的思考块保留

Claude Opus 4.5 引入了一个新的默认行为：**来自先前助手轮次的思考块默认在模型上下文中保留**。这与较早的模型不同，较早的模型会删除先前轮次的思考块。

**思考块保留的好处：**

- **缓存优化**：使用工具使用时，保留的思考块启用缓存命中，因为它们与工具结果一起传回并在助手轮次中增量缓存，在多步工作流中产生令牌节省
- **无智能影响**：保留思考块对模型性能没有负面影响

**重要注意事项：**

- **上下文使用**：长对话将消耗更多上下文空间，因为思考块保留在上下文中
- **自动行为**：这是 Claude Opus 4.5 的默认行为 - 不需要代码更改或 beta 标头
- **向后兼容性**：要利用此功能，继续将完整的、未修改的思考块传回 API，就像您对工具使用所做的那样

<Note>
对于较早的模型（Claude Sonnet 4.5、Opus 4.1 等），来自先前轮次的思考块继续从上下文中删除。[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分中描述的现有行为适用于这些模型。
</Note>

### 思考内容编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密部分或全部 `thinking` 块，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时会被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 请注意 redacted thinking 块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的某些内部推理已因安全原因自动加密。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉 redacted 块，同时保留正常的思考块
- 要透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理，以优雅地管理 redacted thinking，而不会破坏您的 UI

以下是显示正常和 redacted thinking 块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到 redacted thinking 块是预期的行为。该模型仍然可以使用这个 redacted 推理来为其响应提供信息，同时维护安全护栏。

如果您需要在应用程序中测试 redacted thinking 处理，可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手转向的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用 redacted thinking 块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发 redacted thinking（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别 redacted thinking 块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块在后续请求中仍然可用

    # 提取所有块（redacted 和非 redacted）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 传递到后续请求时，包括所有块而不进行修改
    # 这保持了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发 redacted thinking（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别 redacted thinking 块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块在后续请求中仍然可用

  // 提取所有块（redacted 和非 redacted）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 传递到后续请求时，包括所有块而不进行修改
  // 这保持了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发 redacted thinking（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别 redacted thinking 块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块在后续请求中仍然可用
            // 提取所有块（redacted 和非 redacted）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 传递到后续请求时，包括所有块而不进行修改
            // 这保持了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

## 不同模型版本中的思考差异

Messages API 在 Claude Sonnet 3.7 和 Claude 4 模型中处理思考的方式不同，主要在编辑和总结行为方面。

请参阅下表了解简明比较：

| 功能 | Claude Sonnet 3.7 | Claude 4 模型（Opus 4.5 之前） | Claude Opus 4.5 及更高版本 |
|---------|------------------|-------------------------------|--------------------------|
| **思考输出** | 返回完整思考输出 | 返回总结的思考 | 返回总结的思考 |
| **交错思考** | 不支持 | 支持 `interleaved-thinking-2025-05-14` beta 标头 | 支持 `interleaved-thinking-2025-05-14` beta 标头 |
| **思考块保留** | 不跨转向保留 | 不跨转向保留 | **默认保留**（启用缓存优化、节省令牌） |

### Claude Opus 4.5 中的思考块保留

Claude Opus 4.5 引入了一个新的默认行为：**来自先前助手转向的思考块默认在模型上下文中保留**。这与早期模型不同，早期模型会删除先前转向的思考块。

**思考块保留的好处：**

- **缓存优化**：使用工具使用时，保留的思考块启用缓存命中，因为它们与工具结果一起传回并在助手转向中增量缓存，在多步工作流中节省令牌
- **无智能影响**：保留思考块对模型性能没有负面影响

**重要考虑事项：**

- **上下文使用**：长对话将消耗更多上下文空间，因为思考块保留在上下文中
- **自动行为**：这是 Claude Opus 4.5 的默认行为——不需要代码更改或 beta 标头
- **向后兼容性**：要利用此功能，继续将完整的、未修改的思考块传回 API，就像您对工具使用所做的那样

<Note>
对于早期模型（Claude Sonnet 4.5、Opus 4.1 等），来自先前转向的思考块继续从上下文中删除。[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分中描述的现有行为适用于这些模型。
</Note>

## 定价

有关完整的定价信息，包括基础费率、缓存写入、缓存命中和输出令牌，请参阅[定价页面](/docs/zh-CN/about-claude/pricing)。

思考过程产生的费用包括：
- 思考期间使用的令牌（输出令牌）
- 后续请求中包含的最后一个助手转向的思考块（输入令牌）
- 标准文本输出令牌

<Note>
启用扩展思考时，会自动包含专门的系统提示以支持此功能。
</Note>

使用总结思考时：
- **输入令牌**：您原始请求中的令牌（不包括先前转向的思考令牌）
- **输出令牌（计费）**：Claude 内部生成的原始思考令牌
- **输出令牌（可见）**：您在响应中看到的总结思考令牌
- **无费用**：用于生成摘要的令牌

<Warning>
计费的输出令牌计数将**不**与响应中的可见令牌计数匹配。您需要为完整的思考过程付费，而不是您看到的摘要。
</Warning>

## 扩展思考的最佳实践和注意事项

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

### 使用指南

- **任务选择**：对于特别复杂的任务使用扩展思考，这些任务受益于逐步推理，如数学、编码和分析。
- **上下文处理**：您不需要自己删除以前的思考块。Claude API 会自动忽略先前转向的思考块，在计算上下文使用时不包括它们。
- **提示工程**：如果您想最大化 Claude 的思考能力，请查看我们的[扩展思考提示技巧](/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips)。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

### 使用指南

- **任务选择**：对于特别复杂的任务使用扩展思考，这些任务受益于逐步推理，如数学、编码和分析。
- **上下文处理**：您不需要自己删除以前的思考块。Claude API 会自动忽略先前转向的思考块，在计算上下文使用时不包括它们。
- **提示工程**：如果您想最大化 Claude 的思考能力，请查看我们的[扩展思考提示技巧](/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips)。

## 后续步骤

<CardGroup>
  <Card title="尝试扩展思考食谱" icon="book" href="https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking">
    在我们的食谱中探索思考的实际示例。
  </Card>
  <Card title="扩展思考提示技巧" icon="code" href="/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips">
    学习扩展思考的提示工程最佳实践。
  </Card>
</CardGroup>
# 使用扩展思考进行构建

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

---

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

### 总结的思考

启用扩展思考后，Claude 4 模型的消息 API 会返回 Claude 完整思考过程的摘要。总结的思考提供了扩展思考的全部智能优势，同时防止滥用。

以下是关于总结思考的一些重要注意事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了特别有助于提示工程的详细推理。
- 随着 Anthropic 寻求改进扩展思考功能，总结行为可能会发生变化。
- 总结保留了 Claude 思考过程的关键思想，延迟最少，实现了可流式传输的用户体验，并便于从 Claude Sonnet 3.7 迁移到 Claude 4 模型。
- 总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于复杂任务，同时在提供最终答案之前提供对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制定最终响应之前会整合来自此推理的见解。

API 响应将包括 `thinking` 内容块，后跟 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "让我逐步分析这个...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "根据我的分析..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅[消息 API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在消息 API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# 响应将包含总结的思考块和文本块
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// 响应将包含总结的思考块和文本块
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不适用于[总结的输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变成您的整个上下文窗口（200k 令牌）。

### 总结的思考

启用扩展思考后，Claude 4 模型的消息 API 会返回 Claude 完整思考过程的摘要。总结的思考提供了扩展思考的全部智能优势，同时防止滥用。

以下是关于总结思考的一些重要注意事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了特别有助于提示工程的详细推理。
- 随着 Anthropic 寻求改进扩展思考功能，总结行为可能会发生变化。
- 总结保留了 Claude 思考过程的关键思想，延迟最少，实现了可流式传输的用户体验，并便于从 Claude Sonnet 3.7 迁移到 Claude 4 模型。
- 总结由与您在请求中指定的模型不同的模型处理。思考模型看不到总结的输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

### 流式传输思考

您可以使用[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) 流式传输扩展思考响应。

启用扩展思考的流式传输后，您会通过 `thinking_delta` 事件接收思考内容。

有关通过消息 API 进行流式传输的更多文档，请参阅[流式传输消息](/docs/zh-CN/build-with-claude/streaming)。

以下是如何处理思考流式传输的方法：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "stream": true,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "What is 27 * 453?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={"type": "enabled", "budget_tokens": 10000},
    messages=[{"role": "user", "content": "What is 27 * 453?"}],
) as stream:
    thinking_started = False
    response_started = False

    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
            # 为每个新块重置标志
            thinking_started = False
            response_started = False
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                if not thinking_started:
                    print("Thinking: ", end="", flush=True)
                    thinking_started = True
                print(event.delta.thinking, end="", flush=True)
            elif event.delta.type == "text_delta":
                if not response_started:
                    print("Response: ", end="", flush=True)
                    response_started = True
                print(event.delta.text, end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const stream = await client.messages.stream({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "What is 27 * 453?"
  }]
});

let thinkingStarted = false;
let responseStarted = false;

for await (const event of stream) {
  if (event.type === 'content_block_start') {
    console.log(`\nStarting ${event.content_block.type} block...`);
    // 为每个新块重置标志
    thinkingStarted = false;
    responseStarted = false;
  } else if (event.type === 'content_block_delta') {
    if (event.delta.type === 'thinking_delta') {
      if (!thinkingStarted) {
        process.stdout.write('Thinking: ');
        thinkingStarted = true;
      }
      process.stdout.write(event.delta.thinking);
    } else if (event.delta.type === 'text_delta') {
      if (!responseStarted) {
        process.stdout.write('Response: ');
        responseStarted = true;
      }
      process.stdout.write(event.delta.text);
    }
  } else if (event.type === 'content_block_stop') {
    console.log('\nBlock complete.');
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.http.StreamResponse;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class SimpleThinkingStreamingExample {
    private static boolean thinkingStarted = false;
    private static boolean responseStarted = false;
    
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams createParams = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_0)
                .maxTokens(16000)
                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                .addUserMessage("What is 27 * 453?")
                .build();

        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                     client.beta().messages().createStreaming(createParams)) {
            streamResponse.stream()
                    .forEach(event -> {
                        if (event.isContentBlockStart()) {
                            System.out.printf("\nStarting %s block...%n",
                                    event.asContentBlockStart()._type());
                            // 为每个新块重置标志
                            thinkingStarted = false;
                            responseStarted = false;
                        } else if (event.isContentBlockDelta()) {
                            var delta = event.asContentBlockDelta().delta();
                            if (delta.isBetaThinking()) {
                                if (!thinkingStarted) {
                                    System.out.print("Thinking: ");
                                    thinkingStarted = true;
                                }
                                System.out.print(delta.asBetaThinking().thinking());
                                System.out.flush();
                            } else if (delta.isBetaText()) {
                                if (!responseStarted) {
                                    System.out.print("Response: ");
                                    responseStarted = true;
                                }
                                System.out.print(delta.asBetaText().text());
                                System.out.flush();
                            }
                        } else if (event.isContentBlockStop()) {
                            System.out.println("\nBlock complete.");
                        }
                    });
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  在控制台中尝试
</TryInConsoleButton>

示例流式传输输出：
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "让我逐步解决这个问题：\n\n1. 首先分解 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// 其他思考增量...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// 其他文本增量...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

<Note>
使用启用思考的流式传输时，您可能会注意到文本有时以较大的块到达，交替出现较小的逐令牌传递。这是预期的行为，特别是对于思考内容。

流式传输系统需要分批处理内容以获得最佳性能，这可能导致这种"分块"传递模式，流式传输事件之间可能出现延迟。我们正在不断努力改进这种体验，未来的更新将专注于使思考内容流式传输更加平顺。
</Note>

扩展思考为 Claude 提供了增强的推理能力，用于处理复杂任务，同时在提供最终答案之前提供了对其逐步思考过程的不同透明度级别。

## 支持的模型

以下模型支持扩展思考：

- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([已弃用](/docs/zh-CN/about-claude/model-deprecations))
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
API 行为在 Claude Sonnet 3.7 和 Claude 4 模型之间有所不同，但 API 形状保持完全相同。

有关更多信息，请参阅[不同模型版本中的思考差异](#differences-in-thinking-across-model-versions)。
</Note>

## 扩展思考的工作原理

启用扩展思考后，Claude 会创建 `thinking` 内容块，其中输出其内部推理。Claude 在制作最终响应之前会结合来自此推理的见解。

API 响应将包括 `thinking` 内容块，然后是 `text` 内容块。

以下是默认响应格式的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

有关扩展思考响应格式的更多信息，请参阅 [Messages API 参考](/docs/zh-CN/api/messages)。

## 如何使用扩展思考

以下是在 Messages API 中使用扩展思考的示例：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)

# The response will contain summarized thinking blocks and text blocks
for block in response.content:
    if block.type == "thinking":
        print(f"\nThinking summary: {block.thinking}")
    elif block.type == "text":
        print(f"\nResponse: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
  }]
});

// The response will contain summarized thinking blocks and text blocks
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`\nThinking summary: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`\nResponse: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.*;

public class SimpleThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                        .build()
        );

        System.out.println(response);
    }
}
```

</CodeGroup>

要启用扩展思考，请添加一个 `thinking` 对象，将 `type` 参数设置为 `enabled`，并将 `budget_tokens` 设置为扩展思考的指定令牌预算。

`budget_tokens` 参数确定 Claude 允许用于其内部推理过程的最大令牌数。在 Claude 4 模型中，此限制适用于完整思考令牌，而不是[摘要输出](#summarized-thinking)。较大的预算可以通过为复杂问题启用更彻底的分析来改进响应质量，尽管 Claude 可能不会使用整个分配的预算，特别是在 32k 以上的范围内。

`budget_tokens` 必须设置为小于 `max_tokens` 的值。但是，当使用[与工具交错的思考](#interleaved-thinking)时，您可以超过此限制，因为令牌限制变为您的整个上下文窗口（200k 令牌）。

### 摘要思考

启用扩展思考后，Claude 4 模型的 Messages API 返回 Claude 完整思考过程的摘要。摘要思考提供了扩展思考的全部智能优势，同时防止了滥用。

以下是摘要思考的一些重要考虑事项：

- 您需要为原始请求生成的完整思考令牌付费，而不是摘要令牌。
- 计费的输出令牌计数将**不匹配**您在响应中看到的令牌计数。
- 思考输出的前几行更详细，提供了详细的推理，这对提示工程目的特别有帮助。
- 随着 Anthropic 寻求改进扩展思考功能，摘要行为可能会发生变化。
- 摘要保留了 Claude 思考过程的关键思想，同时增加了最少的延迟，实现了可流式传输的用户体验和从 Claude Sonnet 3.7 到 Claude 4 模型的轻松迁移。
- 摘要由与您在请求中指定的模型不同的模型处理。思考模型看不到摘要输出。

<Note>
Claude Sonnet 3.7 继续返回完整的思考输出。

在极少数情况下，如果您需要访问 Claude 4 模型的完整思考输出，请[联系我们的销售团队](mailto:sales@anthropic.com)。
</Note>

### 流式思考

您可以使用[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) 流式传输扩展思考响应。

启用扩展思考的流式传输后，您会通过 `thinking_delta` 事件接收思考内容。

有关通过 Messages API 进行流式传输的更多文档，请参阅[流式传输消息](/docs/zh-CN/build-with-claude/streaming)。

以下是如何处理思考流式传输的方法：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 16000,
    "stream": true,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 10000
    },
    "messages": [
        {
            "role": "user",
            "content": "What is 27 * 453?"
        }
    ]
}'
```

```python Python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={"type": "enabled", "budget_tokens": 10000},
    messages=[{"role": "user", "content": "What is 27 * 453?"}],
) as stream:
    thinking_started = False
    response_started = False

    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
            # Reset flags for each new block
            thinking_started = False
            response_started = False
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                if not thinking_started:
                    print("Thinking: ", end="", flush=True)
                    thinking_started = True
                print(event.delta.thinking, end="", flush=True)
            elif event.delta.type == "text_delta":
                if not response_started:
                    print("Response: ", end="", flush=True)
                    response_started = True
                print(event.delta.text, end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const stream = await client.messages.stream({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "What is 27 * 453?"
  }]
});

let thinkingStarted = false;
let responseStarted = false;

for await (const event of stream) {
  if (event.type === 'content_block_start') {
    console.log(`\nStarting ${event.content_block.type} block...`);
    // Reset flags for each new block
    thinkingStarted = false;
    responseStarted = false;
  } else if (event.type === 'content_block_delta') {
    if (event.delta.type === 'thinking_delta') {
      if (!thinkingStarted) {
        process.stdout.write('Thinking: ');
        thinkingStarted = true;
      }
      process.stdout.write(event.delta.thinking);
    } else if (event.delta.type === 'text_delta') {
      if (!responseStarted) {
        process.stdout.write('Response: ');
        responseStarted = true;
      }
      process.stdout.write(event.delta.text);
    }
  } else if (event.type === 'content_block_stop') {
    console.log('\nBlock complete.');
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.http.StreamResponse;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class SimpleThinkingStreamingExample {
    private static boolean thinkingStarted = false;
    private static boolean responseStarted = false;
    
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCreateParams createParams = MessageCreateParams.builder()
                .model(Model.CLAUDE_OPUS_4_0)
                .maxTokens(16000)
                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                .addUserMessage("What is 27 * 453?")
                .build();

        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                     client.beta().messages().createStreaming(createParams)) {
            streamResponse.stream()
                    .forEach(event -> {
                        if (event.isContentBlockStart()) {
                            System.out.printf("\nStarting %s block...%n",
                                    event.asContentBlockStart()._type());
                            // Reset flags for each new block
                            thinkingStarted = false;
                            responseStarted = false;
                        } else if (event.isContentBlockDelta()) {
                            var delta = event.asContentBlockDelta().delta();
                            if (delta.isBetaThinking()) {
                                if (!thinkingStarted) {
                                    System.out.print("Thinking: ");
                                    thinkingStarted = true;
                                }
                                System.out.print(delta.asBetaThinking().thinking());
                                System.out.flush();
                            } else if (delta.isBetaText()) {
                                if (!responseStarted) {
                                    System.out.print("Response: ");
                                    responseStarted = true;
                                }
                                System.out.print(delta.asBetaText().text());
                                System.out.flush();
                            }
                        } else if (event.isContentBlockStop()) {
                            System.out.println("\nBlock complete.");
                        }
                    });
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  在控制台中尝试
</TryInConsoleButton>

示例流式输出：
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "Let me solve this step by step:\n\n1. First break down 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// Additional thinking deltas...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// Additional text deltas...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

<Note>
使用启用思考的流式传输时，您可能会注意到文本有时会以较大的块到达，交替出现较小的逐令牌传输。这是预期的行为，特别是对于思考内容。

流式传输系统需要分批处理内容以获得最佳性能，这可能导致这种"块状"传输模式，流式传输事件之间可能出现延迟。我们正在不断努力改进这种体验，未来的更新将专注于使思考内容流式传输更加平稳。
</Note>

## 扩展思考与工具使用

扩展思考可以与[工具使用](/docs/zh-CN/agents-and-tools/tool-use/overview)一起使用，允许 Claude 通过工具选择和结果处理进行推理。

使用扩展思考和工具使用时，请注意以下限制：

1. **工具选择限制**：工具使用与思考仅支持 `tool_choice: {"type": "auto"}` (默认) 或 `tool_choice: {"type": "none"}`。使用 `tool_choice: {"type": "any"}` 或 `tool_choice: {"type": "tool", "name": "..."}` 将导致错误，因为这些选项强制工具使用，这与扩展思考不兼容。

2. **保留思考块**：在工具使用期间，您必须将 `thinking` 块传回 API 以获取最后的助手消息。将完整的未修改块传回 API 以保持推理连续性。

### 在对话中切换思考模式

您不能在助手转向的中间切换思考，包括在工具使用循环期间。整个助手转向必须在单一思考模式下运行：

- **如果启用了思考**，最后的助手转向必须以思考块开始。
- **如果禁用了思考**，最后的助手转向不能包含任何思考块

从模型的角度来看，**工具使用循环是助手转向的一部分**。助手转向直到 Claude 完成其完整响应后才完成，这可能包括多个工具调用和结果。

例如，此序列都是**单个助手转向**的一部分：
```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```

尽管有多个 API 消息，但工具使用循环在概念上是一个连续助手响应的一部分。

#### 常见错误场景

您可能会遇到此错误：
```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```

这通常发生在以下情况：
1. 您在工具使用序列期间**禁用了**思考
2. 您想再次启用思考
3. 您的最后一条助手消息包含工具使用块但没有思考块

#### 实用指南

**✗ 无效：在工具使用后立即切换思考**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```

**✓ 有效：首先完成助手转向**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```

**最佳实践**：在每个转向开始时规划您的思考策略，而不是尝试在中途切换。

<Note>
切换思考模式也会使消息历史的提示缓存失效。有关更多详情，请参阅[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分。
</Note>

<section title="示例：使用工具结果传递思考块">

以下是一个实际示例，展示了在提供工具结果时如何保留思考块：

<CodeGroup>
```python Python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

```typescript TypeScript
const weatherTool = {
  name: "get_weather",
  description: "Get current weather for a location",
  input_schema: {
    type: "object",
    properties: {
      location: { type: "string" }
    },
    required: ["location"]
  }
};

// First request - Claude responds with thinking and tool request
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" }
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.beta.messages.BetaTool;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingWithToolsExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        System.out.println(response);
    }
}
```
</CodeGroup>

API 响应将包括思考、文本和 tool_use 块：

```json
{
    "content": [
        {
            "type": "thinking",
            "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
            "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
        },
        {
            "type": "text",
            "text": "I can help you get the current weather information for Paris. Let me check that for you"
        },
        {
            "type": "tool_use",
            "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
            "name": "get_weather",
            "input": {
                "location": "Paris"
            }
        }
    ]
}
```

现在让我们继续对话并使用该工具

<CodeGroup>
```python Python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

```typescript TypeScript
// Extract thinking block and tool use block
const thinkingBlock = response.content.find(block =>
  block.type === 'thinking');
const toolUseBlock = response.content.find(block =>
  block.type === 'tool_use');

// Call your actual weather API, here is where your actual API call would go
// let's pretend this is what we get back
const weatherData = { temperature: 88 };

// Second request - Include thinking block and tool result
// No new thinking blocks will be generated in the response
const continuation = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" },
    // notice that the thinkingBlock is passed in as well as the toolUseBlock
    // if this is not passed in, an error is raised
    { role: "assistant", content: [thinkingBlock, toolUseBlock] },
    { role: "user", content: [{
      type: "tool_result",
      tool_use_id: toolUseBlock.id,
      content: `Current temperature: ${weatherData.temperature}°F`
    }]}
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;
import java.util.Optional;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingToolsResultExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        // Extract thinking block and tool use block
        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isThinking)
                .map(BetaContentBlock::asThinking)
                .findFirst();

        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isToolUse)
                .map(BetaContentBlock::asToolUse)
                .findFirst();

        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

            // Call your actual weather API, here is where your actual API call would go
            // let's pretend this is what we get back
            Map<String, Object> weatherData = Map.of("temperature", 88);

            // Second request - Include thinking block and tool result
            // No new thinking blocks will be generated in the response
            BetaMessage continuation = client.beta().messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_OPUS_4_0)
                            .maxTokens(16000)
                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                            .addTool(weatherTool)
                            .addUserMessage("What's the weather in Paris?")
                            .addAssistantMessageOfBetaContentBlockParams(
                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                    // if this is not passed in, an error is raised
                                    List.of(
                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                    )
                            )
                            .addUserMessageOfBetaContentBlockParams(List.of(
                                    BetaContentBlockParam.ofToolResult(
                                            BetaToolResultBlockParam.builder()
                                                    .toolUseId(toolUseBlock.id())
                                                    .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                    .build()
                                    )
                            ))
                            .build()
            );

            System.out.println(continuation);
        }
    }
}
```
</CodeGroup>

API 响应现在将**仅**包括文本

```json
{
    "content": [
        {
            "type": "text",
            "text": "Currently in Paris, the temperature is 88°F (31°C)"
        }
    ]
}
```

</section>

### 在对话中切换思考模式

您不能在助手回合的中间切换思考，包括在工具使用循环期间。整个助手回合必须在单一思考模式下运行：

- **如果启用了思考**，最终的助手回合必须以思考块开始。
- **如果禁用了思考**，最终的助手回合不能包含任何思考块

从模型的角度来看，**工具使用循环是助手回合的一部分**。助手回合直到 Claude 完成其完整响应（可能包括多个工具调用和结果）才完成。

例如，这个序列都是**单个助手回合**的一部分：
```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```

尽管有多个 API 消息，但工具使用循环在概念上是一个连续的助手响应的一部分。

#### 常见错误场景

您可能会遇到此错误：
```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```

这通常发生在以下情况：
1. 您在工具使用序列期间**禁用了**思考
2. 您想再次启用思考
3. 您的最后一条助手消息包含工具使用块但没有思考块

#### 实用指导

**✗ 无效：在工具使用后立即切换思考**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```

**✓ 有效：首先完成助手回合**
```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```

**最佳实践**：在每个回合开始时规划您的思考策略，而不是尝试在中间切换。

<Note>
切换思考模式也会使消息历史的提示缓存失效。有关更多详细信息，请参阅 [使用提示缓存的扩展思考](#extended-thinking-with-prompt-caching) 部分。
</Note>

<section title="示例：使用工具结果传递思考块">

这是一个实际示例，展示了在提供工具结果时如何保留思考块：

<CodeGroup>
```python Python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

```typescript TypeScript
const weatherTool = {
  name: "get_weather",
  description: "Get current weather for a location",
  input_schema: {
    type: "object",
    properties: {
      location: { type: "string" }
    },
    required: ["location"]
  }
};

// First request - Claude responds with thinking and tool request
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" }
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.beta.messages.BetaTool;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingWithToolsExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        System.out.println(response);
    }
}
```
</CodeGroup>

API 响应将包括思考、文本和 tool_use 块：

```json
{
    "content": [
        {
            "type": "thinking",
            "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
            "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
        },
        {
            "type": "text",
            "text": "I can help you get the current weather information for Paris. Let me check that for you"
        },
        {
            "type": "tool_use",
            "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
            "name": "get_weather",
            "input": {
                "location": "Paris"
            }
        }
    ]
}
```

现在让我们继续对话并使用该工具

<CodeGroup>
```python Python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

```typescript TypeScript
// Extract thinking block and tool use block
const thinkingBlock = response.content.find(block =>
  block.type === 'thinking');
const toolUseBlock = response.content.find(block =>
  block.type === 'tool_use');

// Call your actual weather API, here is where your actual API call would go
// let's pretend this is what we get back
const weatherData = { temperature: 88 };

// Second request - Include thinking block and tool result
// No new thinking blocks will be generated in the response
const continuation = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [weatherTool],
  messages: [
    { role: "user", content: "What's the weather in Paris?" },
    // notice that the thinkingBlock is passed in as well as the toolUseBlock
    // if this is not passed in, an error is raised
    { role: "assistant", content: [thinkingBlock, toolUseBlock] },
    { role: "user", content: [{
      type: "tool_result",
      tool_use_id: toolUseBlock.id,
      content: `Current temperature: ${weatherData.temperature}°F`
    }]}
  ]
});
```

```java Java
import java.util.List;
import java.util.Map;
import java.util.Optional;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.BetaTool.InputSchema;
import com.anthropic.models.messages.Model;

public class ThinkingToolsResultExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of("type", "string")
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        BetaTool weatherTool = BetaTool.builder()
                .name("get_weather")
                .description("Get current weather for a location")
                .inputSchema(schema)
                .build();

        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addTool(weatherTool)
                        .addUserMessage("What's the weather in Paris?")
                        .build()
        );

        // Extract thinking block and tool use block
        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isThinking)
                .map(BetaContentBlock::asThinking)
                .findFirst();

        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                .filter(BetaContentBlock::isToolUse)
                .map(BetaContentBlock::asToolUse)
                .findFirst();

        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

            // Call your actual weather API, here is where your actual API call would go
            // let's pretend this is what we get back
            Map<String, Object> weatherData = Map.of("temperature", 88);

            // Second request - Include thinking block and tool result
            // No new thinking blocks will be generated in the response
            BetaMessage continuation = client.beta().messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_OPUS_4_0)
                            .maxTokens(16000)
                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                            .addTool(weatherTool)
                            .addUserMessage("What's the weather in Paris?")
                            .addAssistantMessageOfBetaContentBlockParams(
                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                    // if this is not passed in, an error is raised
                                    List.of(
                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                    )
                            )
                            .addUserMessageOfBetaContentBlockParams(List.of(
                                    BetaContentBlockParam.ofToolResult(
                                            BetaToolResultBlockParam.builder()
                                                    .toolUseId(toolUseBlock.id())
                                                    .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                    .build()
                                    )
                            ))
                            .build()
            );

            System.out.println(continuation);
        }
    }
}
```
</CodeGroup>

API 响应现在**仅**包括文本

```json
{
    "content": [
        {
            "type": "text",
            "text": "Currently in Paris, the temperature is 88°F (31°C)"
        }
    ]
}
```

</section>

### 保留思考块

在工具使用期间，您必须将 `thinking` 块传递回 API，并且必须将完整的未修改块包含回 API。这对于维持模型的推理流和对话完整性至关重要。

<Tip>
虽然您可以从先前的 `assistant` 角色回合中省略 `thinking` 块，但我们建议对于任何多轮对话始终将所有思考块传递回 API。API 将：
- 自动过滤提供的思考块
- 使用必要的相关思考块来保留模型的推理
- 仅对显示给 Claude 的块的输入令牌进行计费
</Tip>

<Note>
在对话期间切换思考模式时，请记住整个助手回合（包括工具使用循环）必须在单一思考模式下运行。有关更多详细信息，请参阅 [在对话中切换思考模式](#toggling-thinking-modes-in-conversations)。
</Note>

当 Claude 调用工具时，它暂停了其响应的构建以等待外部信息。当工具结果返回时，Claude 将继续构建该现有响应。这需要在工具使用期间保留思考块，原因有几个：

1. **推理连续性**：思考块捕获了导致工具请求的 Claude 的逐步推理。当您发布工具结果时，包括原始思考可以确保 Claude 可以从中断的地方继续其推理。

2. **上下文维护**：虽然工具结果在 API 结构中显示为用户消息，但它们是连续推理流的一部分。保留思考块在多个 API 调用中维持这个概念流。有关上下文管理的更多信息，请参阅我们的 [上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)。

**重要**：提供 `thinking` 块时，连续 `thinking` 块的整个序列必须与模型在原始请求期间生成的输出相匹配；您不能重新排列或修改这些块的序列。

### 交错思考

Claude 4 模型中的扩展思考与工具使用支持交错思考，这使 Claude 能够在工具调用之间进行思考，并在接收工具结果后进行更复杂的推理。

通过交错思考，Claude 可以：
- 在工具调用结果后进行推理，然后决定接下来做什么
- 在推理步骤之间链接多个工具调用
- 根据中间结果做出更细致的决策

要启用交错思考，请将[测试版标头](/docs/zh-CN/api/beta-headers) `interleaved-thinking-2025-05-14` 添加到您的 API 请求中。

以下是交错思考的一些重要注意事项：
- 使用交错思考时，`budget_tokens` 可以超过 `max_tokens` 参数，因为它代表一个助手轮次内所有思考块的总预算。
- 交错思考仅支持[通过消息 API 使用的工具](/docs/zh-CN/agents-and-tools/tool-use/overview)。
- 交错思考仅支持 Claude 4 模型，使用测试版标头 `interleaved-thinking-2025-05-14`。
- 直接调用 Claude API 允许您在对任何模型的请求中传递 `interleaved-thinking-2025-05-14`，但不会产生任何效果。
- 在第三方平台上（例如，[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock) 和 [Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)），如果您将 `interleaved-thinking-2025-05-14` 传递给除 Claude Opus 4.5、Claude Opus 4.1、Opus 4 或 Sonnet 4 之外的任何模型，您的请求将失败。

<section title="不使用交错思考的工具使用">

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 定义工具
calculator_tool = {
    "name": "calculator",
    "description": "Perform mathematical calculations",
    "input_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
            }
        },
        "required": ["expression"]
    }
}

database_tool = {
    "name": "database_query",
    "description": "Query product database",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "SQL query to execute"
            }
        },
        "required": ["query"]
    }
}

# 第一个请求 - Claude 在所有工具调用之前思考一次
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    messages=[{
        "role": "user",
        "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    }]
)

# 响应包括思考，然后是工具使用
# 注意：Claude 在开始时思考一次，然后做出所有工具决策
print("First response:")
for block in response.content:
    if block.type == "thinking":
        print(f"Thinking (summarized): {block.thinking}")
    elif block.type == "tool_use":
        print(f"Tool use: {block.name} with input {block.input}")
    elif block.type == "text":
        print(f"Text: {block.text}")

# 您将执行工具并返回结果...
# 获得两个工具结果后，Claude 直接响应而不进行额外思考
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 定义工具
const calculatorTool = {
  name: "calculator",
  description: "Perform mathematical calculations",
  input_schema: {
    type: "object",
    properties: {
      expression: {
        type: "string",
        description: "Mathematical expression to evaluate"
      }
    },
    required: ["expression"]
  }
};

const databaseTool = {
  name: "database_query",
  description: "Query product database",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "SQL query to execute"
      }
    },
    required: ["query"]
  }
};

// 第一个请求 - Claude 在所有工具调用之前思考一次
const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [{
    role: "user",
    content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
  }]
});

// 响应包括思考，然后是工具使用
// 注意：Claude 在开始时思考一次，然后做出所有工具决策
console.log("First response:");
for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`Thinking (summarized): ${block.thinking}`);
  } else if (block.type === "tool_use") {
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  } else if (block.type === "text") {
    console.log(`Text: ${block.text}`);
  }
}

// 您将执行工具并返回结果...
// 获得两个工具结果后，Claude 直接响应而不进行额外思考
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.messages.Model;
import java.util.List;
import java.util.Map;

public class NonInterleavedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 定义计算器工具
        BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "expression", Map.of(
                                "type", "string",
                                "description", "Mathematical expression to evaluate"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                .build();

        BetaTool calculatorTool = BetaTool.builder()
                .name("calculator")
                .description("Perform mathematical calculations")
                .inputSchema(calculatorSchema)
                .build();

        // 定义数据库工具
        BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "SQL query to execute"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        BetaTool databaseTool = BetaTool.builder()
                .name("database_query")
                .description("Query product database")
                .inputSchema(databaseSchema)
                .build();

        // 第一个请求 - Claude 在所有工具调用之前思考一次
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .build()
        );

        // 响应包括思考，然后是工具使用
        // 注意：Claude 在开始时思考一次，然后做出所有工具决策
        System.out.println("First response:");
        for (BetaContentBlock block : response.content()) {
            if (block.isThinking()) {
                System.out.println("Thinking (summarized): " + block.asThinking().thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            } else if (block.isText()) {
                System.out.println("Text: " + block.asText().text());
            }
        }

        // 您将执行工具并返回结果...
        // 获得两个工具结果后，Claude 直接响应而不进行额外思考
    }
}
```
</CodeGroup>

在这个不使用交错思考的示例中：
1. Claude 在开始时思考一次以理解任务
2. 提前做出所有工具使用决策
3. 当工具结果返回时，Claude 立即提供响应而不进行额外思考

</section>

<section title="使用交错思考的工具使用">

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 与之前相同的工具定义
calculator_tool = {
    "name": "calculator",
    "description": "Perform mathematical calculations",
    "input_schema": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
            }
        },
        "required": ["expression"]
    }
}

database_tool = {
    "name": "database_query",
    "description": "Query product database",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "SQL query to execute"
            }
        },
        "required": ["query"]
    }
}

# 第一个请求，启用交错思考
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[{
        "role": "user",
        "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    }]
)

print("Initial response:")
thinking_blocks = []
tool_use_blocks = []

for block in response.content:
    if block.type == "thinking":
        thinking_blocks.append(block)
        print(f"Thinking: {block.thinking}")
    elif block.type == "tool_use":
        tool_use_blocks.append(block)
        print(f"Tool use: {block.name} with input {block.input}")
    elif block.type == "text":
        print(f"Text: {block.text}")

# 第一个工具结果（计算器）
calculator_result = "7500"  # 150 * 50

# 继续第一个工具结果
response2 = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[
        {
            "role": "user",
            "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        },
        {
            "role": "assistant",
            "content": [thinking_blocks[0], tool_use_blocks[0]]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[0].id,
                "content": calculator_result
            }]
        }
    ]
)

print("\nAfter calculator result:")
# 使用交错思考，Claude 可以思考计算器结果
# 然后决定是否查询数据库
for block in response2.content:
    if block.type == "thinking":
        thinking_blocks.append(block)
        print(f"Interleaved thinking: {block.thinking}")
    elif block.type == "tool_use":
        tool_use_blocks.append(block)
        print(f"Tool use: {block.name} with input {block.input}")

# 第二个工具结果（数据库）
database_result = "5200"  # 示例平均月收入

# 继续第二个工具结果
response3 = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    tools=[calculator_tool, database_tool],
    betas=["interleaved-thinking-2025-05-14"],
    messages=[
        {
            "role": "user",
            "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        },
        {
            "role": "assistant",
            "content": [thinking_blocks[0], tool_use_blocks[0]]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[0].id,
                "content": calculator_result
            }]
        },
        {
            "role": "assistant",
            "content": thinking_blocks[1:] + tool_use_blocks[1:]
        },
        {
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use_blocks[1].id,
                "content": database_result
            }]
        }
    ]
)

print("\nAfter database result:")
# 使用交错思考，Claude 可以思考两个结果
# 然后制定最终响应
for block in response3.content:
    if block.type == "thinking":
        print(f"Final thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Final response: {block.text}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 与之前相同的工具定义
const calculatorTool = {
  name: "calculator",
  description: "Perform mathematical calculations",
  input_schema: {
    type: "object",
    properties: {
      expression: {
        type: "string",
        description: "Mathematical expression to evaluate"
      }
    },
    required: ["expression"]
  }
};

const databaseTool = {
  name: "database_query",
  description: "Query product database",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "SQL query to execute"
      }
    },
    required: ["query"]
  }
};

// 第一个请求，启用交错思考
const response = await client.beta.messages.create({
  // 启用交错思考
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [{
    role: "user",
    content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
  }]
});

console.log("Initial response:");
const thinkingBlocks = [];
const toolUseBlocks = [];

for (const block of response.content) {
  if (block.type === "thinking") {
    thinkingBlocks.push(block);
    console.log(`Thinking: ${block.thinking}`);
  } else if (block.type === "tool_use") {
    toolUseBlocks.push(block);
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  } else if (block.type === "text") {
    console.log(`Text: ${block.text}`);
  }
}

// 第一个工具结果（计算器）
const calculatorResult = "7500"; // 150 * 50

// 继续第一个工具结果
const response2 = await client.beta.messages.create({
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [
    {
      role: "user",
      content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    },
    {
      role: "assistant",
      content: [thinkingBlocks[0], toolUseBlocks[0]]
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[0].id,
        content: calculatorResult
      }]
    }
  ]
});

console.log("\nAfter calculator result:");
// 使用交错思考，Claude 可以思考计算器结果
// 然后决定是否查询数据库
for (const block of response2.content) {
  if (block.type === "thinking") {
    thinkingBlocks.push(block);
    console.log(`Interleaved thinking: ${block.thinking}`);
  } else if (block.type === "tool_use") {
    toolUseBlocks.push(block);
    console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
  }
}

// 第二个工具结果（数据库）
const databaseResult = "5200"; // 示例平均月收入

// 继续第二个工具结果
const response3 = await client.beta.messages.create({
  betas: ["interleaved-thinking-2025-05-14"],
  model: "claude-sonnet-4-5",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  tools: [calculatorTool, databaseTool],
  messages: [
    {
      role: "user",
      content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
    },
    {
      role: "assistant",
      content: [thinkingBlocks[0], toolUseBlocks[0]]
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[0].id,
        content: calculatorResult
      }]
    },
    {
      role: "assistant",
      content: thinkingBlocks.slice(1).concat(toolUseBlocks.slice(1))
    },
    {
      role: "user",
      content: [{
        type: "tool_result",
        tool_use_id: toolUseBlocks[1].id,
        content: databaseResult
      }]
    }
  ]
});

console.log("\nAfter database result:");
// 使用交错思考，Claude 可以思考两个结果
// 然后制定最终响应
for (const block of response3.content) {
  if (block.type === "thinking") {
    console.log(`Final thinking: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`Final response: ${block.text}`);
  }
}
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.messages.Model;
import java.util.*;
import static java.util.stream.Collectors.toList;

public class InterleavedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 定义计算器工具
        BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "expression", Map.of(
                                "type", "string",
                                "description", "Mathematical expression to evaluate"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                .build();

        BetaTool calculatorTool = BetaTool.builder()
                .name("calculator")
                .description("Perform mathematical calculations")
                .inputSchema(calculatorSchema)
                .build();

        // 定义数据库工具
        BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "query", Map.of(
                                "type", "string",
                                "description", "SQL query to execute"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                .build();

        BetaTool databaseTool = BetaTool.builder()
                .name("database_query")
                .description("Query product database")
                .inputSchema(databaseSchema)
                .build();

        // 第一个请求，启用交错思考
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        // 使用测试版标头启用交错思考
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .build()
        );

        System.out.println("Initial response:");
        List<BetaThinkingBlock> thinkingBlocks = new ArrayList<>();
        List<BetaToolUseBlock> toolUseBlocks = new ArrayList<>();

        for (BetaContentBlock block : response.content()) {
            if (block.isThinking()) {
                BetaThinkingBlock thinking = block.asThinking();
                thinkingBlocks.add(thinking);
                System.out.println("Thinking: " + thinking.thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                toolUseBlocks.add(toolUse);
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            } else if (block.isText()) {
                System.out.println("Text: " + block.asText().text());
            }
        }

        // 第一个工具结果（计算器）
        String calculatorResult = "7500"; // 150 * 50

        // 继续第一个工具结果
        BetaMessage response2 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .addAssistantMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                        ))
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(0).id())
                                                .content(calculatorResult)
                                                .build()
                                )
                        ))
                        .build()
        );

        System.out.println("\nAfter calculator result:");
        // 使用交错思考，Claude 可以思考计算器结果
        // 然后决定是否查询数据库
        for (BetaContentBlock block : response2.content()) {
            if (block.isThinking()) {
                BetaThinkingBlock thinking = block.asThinking();
                thinkingBlocks.add(thinking);
                System.out.println("Interleaved thinking: " + thinking.thinking());
            } else if (block.isToolUse()) {
                BetaToolUseBlock toolUse = block.asToolUse();
                toolUseBlocks.add(toolUse);
                System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
            }
        }

        // 第二个工具结果（数据库）
        String databaseResult = "5200"; // 示例平均月收入

        // 准备助手消息的组合内容
        List<BetaContentBlockParam> combinedContent = new ArrayList<>();
        for (int i = 1; i < thinkingBlocks.size(); i++) {
            combinedContent.add(BetaContentBlockParam.ofThinking(thinkingBlocks.get(i).toParam()));
        }
        for (int i = 1; i < toolUseBlocks.size(); i++) {
            combinedContent.add(BetaContentBlockParam.ofToolUse(toolUseBlocks.get(i).toParam()));
        }

        // 继续第二个工具结果
        BetaMessage response3 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder()
                                .budgetTokens(10000)
                                .build())
                        .addTool(calculatorTool)
                        .addTool(databaseTool)
                        .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                        .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                        .addAssistantMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                        ))
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(0).id())
                                                .content(calculatorResult)
                                                .build()
                                )
                        ))
                        .addAssistantMessageOfBetaContentBlockParams(combinedContent)
                        .addUserMessageOfBetaContentBlockParams(List.of(
                                BetaContentBlockParam.ofToolResult(
                                        BetaToolResultBlockParam.builder()
                                                .toolUseId(toolUseBlocks.get(1).id())
                                                .content(databaseResult)
                                                .build()
                                )
                        ))
                        .build()
        );

        System.out.println("\nAfter database result:");
        // 使用交错思考，Claude 可以思考两个结果
        // 然后制定最终响应
        for (BetaContentBlock block : response3.content()) {
            if (block.isThinking()) {
                System.out.println("Final thinking: " + block.asThinking().thinking());
            } else if (block.isText()) {
                System.out.println("Final response: " + block.asText().text());
            }
        }
    }
}
```
</CodeGroup>

在这个使用交错思考的示例中：
1. Claude 最初思考任务
2. 收到计算器结果后，Claude 可以再次思考该结果的含义
3. Claude 然后根据第一个结果决定如何查询数据库
4. 收到数据库结果后，Claude 再思考一次两个结果，然后制定最终响应
5. 思考预算分布在该轮次内的所有思考块中

这种模式允许更复杂的推理链，其中每个工具的输出为下一个决策提供信息。

</section>

## 扩展思考与提示缓存

[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)与思考有几个重要的注意事项：

<Tip>
扩展思考任务通常需要超过 5 分钟才能完成。考虑使用[1 小时缓存持续时间](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration)来维护跨越较长思考会话和多步骤工作流的缓存命中。
</Tip>

**思考块上下文移除**
- 来自先前轮次的思考块从上下文中移除，这可能会影响缓存断点
- 继续使用工具的对话时，思考块被缓存并在从缓存读取时计为输入令牌
- 这产生了一个权衡：虽然思考块在视觉上不消耗上下文窗口空间，但在缓存时仍然计入您的输入令牌使用
- 如果思考被禁用，如果您在当前工具使用轮次中传递思考内容，请求将失败。在其他情况下，传递给 API 的思考内容将被忽略

**缓存失效模式**
- 思考参数的更改（启用/禁用或预算分配）会使消息缓存断点失效
- [交错思考](#interleaved-thinking)放大了缓存失效，因为思考块可能发生在多个[工具调用](#extended-thinking-with-tool-use)之间
- 系统提示和工具保持缓存，尽管思考参数更改或块移除

<Note>
虽然思考块被移除用于缓存和上下文计算，但在继续使用[工具使用](#extended-thinking-with-tool-use)的对话时必须保留它们，特别是使用[交错思考](#interleaved-thinking)时。
</Note>

## 扩展思考与提示缓存

[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)与思考有几个重要的考虑事项：

<Tip>
扩展思考任务通常需要超过5分钟才能完成。考虑使用[1小时缓存时长](/docs/zh-CN/build-with-claude/prompt-caching#1-hour-cache-duration)来维持跨越更长思考会话和多步骤工作流的缓存命中。
</Tip>

**思考块上下文移除**
- 来自前面轮次的思考块会从上下文中移除，这可能会影响缓存断点
- 当继续使用工具的对话时，思考块会被缓存，并在从缓存读取时计为输入令牌
- 这产生了一个权衡：虽然思考块在视觉上不消耗上下文窗口空间，但在缓存时仍然计入您的输入令牌使用量
- 如果思考被禁用，如果您在当前工具使用轮次中传递思考内容，请求将失败。在其他上下文中，传递给API的思考内容会被简单地忽略

**缓存失效模式**
- 对思考参数的更改（启用/禁用或预算分配）会使消息缓存断点失效
- [交错思考](#interleaved-thinking)会放大缓存失效，因为思考块可能出现在多个[工具调用](#extended-thinking-with-tool-use)之间
- 系统提示和工具尽管思考参数更改或块移除，仍保持缓存

<Note>
虽然思考块会因缓存和上下文计算而被移除，但在继续使用[工具使用](#extended-thinking-with-tool-use)的对话时，特别是使用[交错思考](#interleaved-thinking)时，必须保留它们。
</Note>

### 理解思考块缓存行为

当使用扩展思考与工具使用时，思考块表现出特定的缓存行为，这会影响令牌计数：

**工作原理：**

1. 仅当您发出包含工具结果的后续请求时，才会发生缓存
2. 当发出后续请求时，之前的对话历史（包括思考块）可以被缓存
3. 这些缓存的思考块在从缓存读取时计为您使用指标中的输入令牌
4. 当包含非工具结果用户块时，所有之前的思考块都会被忽略并从上下文中移除

**详细示例流程：**

**请求1：**
```
User: "What's the weather in Paris?"
```
**响应1：**
```
[thinking_block_1] + [tool_use block 1]
```

**请求2：**
```
User: ["What's the weather in Paris?"], 
Assistant: [thinking_block_1] + [tool_use block 1], 
User: [tool_result_1, cache=True]
```
**响应2：**
```
[thinking_block_2] + [text block 2]
```
请求2写入请求内容的缓存（不是响应）。缓存包括原始用户消息、第一个思考块、工具使用块和工具结果。

**请求3：**
```
User: ["What's the weather in Paris?"],
Assistant: [thinking_block_1] + [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [thinking_block_2] + [text block 2],
User: [Text response, cache=True]
```
对于Claude Opus 4.5及更高版本，默认情况下保留所有之前的思考块。对于较旧的模型，由于包含了非工具结果用户块，所有之前的思考块都会被忽略。此请求将按以下方式处理：
```
User: ["What's the weather in Paris?"],
Assistant: [tool_use block 1],
User: [tool_result_1, cache=True],
Assistant: [text block 2],
User: [Text response, cache=True]
```

**关键点：**
- 此缓存行为会自动发生，即使没有显式的`cache_control`标记
- 此行为在使用常规思考或交错思考时是一致的

<section title="系统提示缓存（思考更改时保留）">

<CodeGroup>
```python Python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

SYSTEM_PROMPT=[
    {
        "type": "text",
        "text": "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
    },
    {
        "type": "text",
        "text": LARGE_TEXT,
        "cache_control": {"type": "ephemeral"}
    }
]

MESSAGES = [
    {
        "role": "user",
        "content": "Analyze the tone of this passage."
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

# Third request - different thinking parameters (cache miss for messages)
print("\nThird request - different thinking parameters (cache miss for messages)")
response3 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Changed thinking budget
    },
    system=SYSTEM_PROMPT,  # System prompt remains cached
    messages=MESSAGES  # Messages cache is invalidated
)

print(f"Third response usage: {response3.usage}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import axios from 'axios';
import * as cheerio from 'cheerio';

const client = new Anthropic();

async function fetchArticleContent(url: string): Promise<string> {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);
  
  // Remove script and style elements
  $('script, style').remove();
  
  // Get text
  let text = $.text();
  
  // Break into lines and remove leading and trailing space on each
  const lines = text.split('\n').map(line => line.trim());
  // Drop blank lines
  text = lines.filter(line => line.length > 0).join('\n');
  
  return text;
}

// Fetch the content of the article
const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
const bookContent = await fetchArticleContent(bookUrl);
// Use just enough text for caching (first few chapters)
const LARGE_TEXT = bookContent.slice(0, 5000);

const SYSTEM_PROMPT = [
  {
    type: "text",
    text: "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
  },
  {
    type: "text",
    text: LARGE_TEXT,
    cache_control: { type: "ephemeral" }
  }
];

const MESSAGES = [
  {
    role: "user",
    content: "Analyze the tone of this passage."
  }
];

// First request - establish cache
console.log("First request - establishing cache");
const response1 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 4000
  },
  system: SYSTEM_PROMPT,
  messages: MESSAGES
});

console.log(`First response usage: ${response1.usage}`);

MESSAGES.push({
  role: "assistant",
  content: response1.content
});
MESSAGES.push({
  role: "user",
  content: "Analyze the characters in this passage."
});

// Second request - same thinking parameters (cache hit expected)
console.log("\nSecond request - same thinking parameters (cache hit expected)");
const response2 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 4000
  },
  system: SYSTEM_PROMPT,
  messages: MESSAGES
});

console.log(`Second response usage: ${response2.usage}`);

// Third request - different thinking parameters (cache miss for messages)
console.log("\nThird request - different thinking parameters (cache miss for messages)");
const response3 = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 20000,
  thinking: {
    type: "enabled",
    budget_tokens: 8000  // Changed thinking budget
  },
  system: SYSTEM_PROMPT,  // System prompt remains cached
  messages: MESSAGES  // Messages cache is invalidated
});

console.log(`Third response usage: ${response3.usage}`);
```
</CodeGroup>

</section>
<section title="消息缓存（思考更改时失效）">

<CodeGroup>
```python Python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

# No system prompt - caching in messages instead
MESSAGES = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": LARGE_TEXT,
                "cache_control": {"type": "ephemeral"},
            },
            {
                "type": "text",
                "text": "Analyze the tone of this passage."
            }
        ]
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000  # Same thinking budget
    },
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response2.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the setting in this passage."
})

# Third request - different thinking budget (cache miss expected)
print("\nThird request - different thinking budget (cache miss expected)")
response3 = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Different thinking budget breaks cache
    },
    messages=MESSAGES
)

print(f"Third response usage: {response3.usage}")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import axios from 'axios';
import * as cheerio from 'cheerio';

const client = new Anthropic();

async function fetchArticleContent(url: string): Promise<string> {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);

  // Remove script and style elements
  $('script, style').remove();

  // Get text
  let text = $.text();

  // Clean up text (break into lines, remove whitespace)
  const lines = text.split('\n').map(line => line.trim());
  const chunks = lines.flatMap(line => line.split('  ').map(phrase => phrase.trim()));
  text = chunks.filter(chunk => chunk).join('\n');

  return text;
}

async function main() {
  // Fetch the content of the article
  const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
  const bookContent = await fetchArticleContent(bookUrl);
  // Use just enough text for caching (first few chapters)
  const LARGE_TEXT = bookContent.substring(0, 5000);

  // No system prompt - caching in messages instead
  let MESSAGES = [
    {
      role: "user",
      content: [
        {
          type: "text",
          text: LARGE_TEXT,
          cache_control: {type: "ephemeral"},
        },
        {
          type: "text",
          text: "Analyze the tone of this passage."
        }
      ]
    }
  ];

  // First request - establish cache
  console.log("First request - establishing cache");
  const response1 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 4000
    },
    messages: MESSAGES
  });

  console.log(`First response usage: `, response1.usage);

  MESSAGES = [
    ...MESSAGES,
    {
      role: "assistant",
      content: response1.content
    },
    {
      role: "user",
      content: "Analyze the characters in this passage."
    }
  ];

  // Second request - same thinking parameters (cache hit expected)
  console.log("\nSecond request - same thinking parameters (cache hit expected)");
  const response2 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 4000  // Same thinking budget
    },
    messages: MESSAGES
  });

  console.log(`Second response usage: `, response2.usage);

  MESSAGES = [
    ...MESSAGES,
    {
      role: "assistant",
      content: response2.content
    },
    {
      role: "user",
      content: "Analyze the setting in this passage."
    }
  ];

  // Third request - different thinking budget (cache miss expected)
  console.log("\nThird request - different thinking budget (cache miss expected)");
  const response3 = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 20000,
    thinking: {
      type: "enabled",
      budget_tokens: 8000  // Different thinking budget breaks cache
    },
    messages: MESSAGES
  });

  console.log(`Third response usage: `, response3.usage);
}

main().catch(console.error);
```

```java Java
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.net.URL;
import java.util.Arrays;
import java.util.regex.Pattern;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.*;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.messages.Model;

import static java.util.stream.Collectors.joining;
import static java.util.stream.Collectors.toList;

public class ThinkingCacheExample {
    public static void main(String[] args) throws IOException {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // Fetch the content of the article
        String bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
        String bookContent = fetchArticleContent(bookUrl);
        // Use just enough text for caching (first few chapters)
        String largeText = bookContent.substring(0, 5000);

        List<BetaTextBlockParam> systemPrompt = List.of(
                BetaTextBlockParam.builder()
                        .text("You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.")
                        .build(),
                BetaTextBlockParam.builder()
                        .text(largeText)
                        .cacheControl(BetaCacheControlEphemeral.builder().build())
                        .build()
        );

        List<BetaMessageParam> messages = new ArrayList<>();
        messages.add(BetaMessageParam.builder()
                .role(BetaMessageParam.Role.USER)
                .content("Analyze the tone of this passage.")
                .build());

        // First request - establish cache
        System.out.println("First request - establishing cache");
        BetaMessage response1 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .messages(messages)
                        .build()
        );

        System.out.println("First response usage: " + response1.usage());

        // Second request - same thinking parameters (cache hit expected)
        System.out.println("\nSecond request - same thinking parameters (cache hit expected)");
        BetaMessage response2 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .addMessage(response1)
                        .addUserMessage("Analyze the characters in this passage.")
                        .messages(messages)
                        .build()
        );

        System.out.println("Second response usage: " + response2.usage());

        // Third request - different thinking budget (cache hit expected because system prompt caching)
        System.out.println("\nThird request - different thinking budget (cache hit expected)");
        BetaMessage response3 = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_OPUS_4_0)
                        .maxTokens(20000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(8000).build())
                        .systemOfBetaTextBlockParams(systemPrompt)
                        .addMessage(response1)
                        .addUserMessage("Analyze the characters in this passage.")
                        .addMessage(response2)
                        .addUserMessage("Analyze the setting in this passage.")
                        .build()
        );

        System.out.println("Third response usage: " + response3.usage());
    }

    private static String fetchArticleContent(String url) throws IOException {
        // Fetch HTML content
        String htmlContent = fetchHtml(url);

        // Remove script and style elements
        String noScriptStyle = removeElements(htmlContent, "script", "style");

        // Extract text (simple approach - remove HTML tags)
        String text = removeHtmlTags(noScriptStyle);

        // Clean up text (break into lines, remove whitespace)
        List<String> lines = Arrays.asList(text.split("\n"));
        List<String> trimmedLines = lines.stream()
                .map(String::trim)
                .collect(toList());

        // Split on double spaces and flatten
        List<String> chunks = trimmedLines.stream()
                .flatMap(line -> Arrays.stream(line.split("  "))
                        .map(String::trim))
                .collect(toList());

        // Filter empty chunks and join with newlines
        return chunks.stream()
                .filter(chunk -> !chunk.isEmpty())
                .collect(joining("\n"));
    }

    /**
     * Fetches HTML content from a URL
     */
    private static String fetchHtml(String urlString) throws IOException {
        try (InputStream inputStream = new URL(urlString).openStream()) {
            StringBuilder content = new StringBuilder();
            try (BufferedReader reader = new BufferedReader(
                    new InputStreamReader(inputStream))) {
                String line;
                while ((line = reader.readLine()) != null) {
                    content.append(line).append("\n");
                }
            }
            return content.toString();
        }
    }

    /**
     * Removes specified HTML elements and their content
     */
    private static String removeElements(String html, String... elementNames) {
        String result = html;
        for (String element : elementNames) {
            // Pattern to match <element>...</element> and self-closing tags
            String pattern = "<" + element + "\\s*[^>]*>.*?</" + element + ">|<" + element + "\\s*[^>]*/?>";
            result = Pattern.compile(pattern, Pattern.DOTALL).matcher(result).replaceAll("");
        }
        return result;
    }

    /**
     * Removes all HTML tags from content
     */
    private static String removeHtmlTags(String html) {
        // Replace <br> and <p> tags with newlines for better text formatting
        String withLineBreaks = html.replaceAll("<br\\s*/?\\s*>|</?p\\s*[^>]*>", "\n");

        // Remove remaining HTML tags
        String noTags = withLineBreaks.replaceAll("<[^>]*>", "");

        // Decode HTML entities (simplified for common entities)
        return decodeHtmlEntities(noTags);
    }

    /**
     * Simple HTML entity decoder for common entities
     */
    private static String decodeHtmlEntities(String text) {
        return text
                .replaceAll("&nbsp;", " ")
                .replaceAll("&amp;", "&")
                .replaceAll("&lt;", "<")
                .replaceAll("&gt;", ">")
                .replaceAll("&quot;", "\"")
                .replaceAll("&#39;", "'")
                .replaceAll("&hellip;", "...")
                .replaceAll("&mdash;", "—");
    }

}
```
</CodeGroup>

脚本的输出如下（您可能会看到略有不同的数字）

```
First request - establishing cache
First response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }

Second request - same thinking parameters (cache hit expected)

Second response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }

Third request - different thinking budget (cache miss expected)
Third response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }
```

此示例演示了当缓存在消息数组中设置时，更改思考参数（预算令牌从4000增加到8000）**使缓存失效**。第三个请求显示没有缓存命中，`cache_creation_input_tokens=1370`和`cache_read_input_tokens=0`，证明当思考参数更改时，基于消息的缓存会失效。

</section>

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 扩展思考与工具使用中的上下文窗口

当使用扩展思考与工具使用时，思考块必须显式保留并与工具结果一起返回。

扩展思考与工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下图说明了扩展思考与工具使用的令牌管理：

![扩展思考和工具使用的上下文窗口图](/docs/images/context-window-thinking-tools.svg)

## 扩展思考中的最大令牌和上下文窗口大小

在较旧的Claude模型中（Claude Sonnet 3.7之前），如果提示令牌和`max_tokens`的总和超过了模型的上下文窗口，系统会自动调整`max_tokens`以适应上下文限制。这意味着您可以设置一个大的`max_tokens`值，系统会根据需要静默地减少它。

使用Claude 3.7和4模型，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌+`max_tokens`超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 扩展思考中的上下文窗口

在启用思考的情况下计算上下文窗口使用时，需要注意一些事项：

- 来自前面轮次的思考块会被剥离，不计入您的上下文窗口
- 当前轮次思考计入该轮次的`max_tokens`限制

下图演示了启用扩展思考时的专门令牌管理：

![扩展思考的上下文窗口图](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数API](/docs/zh-CN/build-with-claude/token-counting)来获取您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 扩展思考与工具使用中的上下文窗口

当使用扩展思考与工具使用时，思考块必须显式保留并与工具结果一起返回。

扩展思考与工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下图说明了扩展思考与工具使用的令牌管理：

![扩展思考和工具使用的上下文窗口图](/docs/images/context-window-thinking-tools.svg)

### 使用扩展思考管理令牌

鉴于Claude 3.7和4模型的上下文窗口和`max_tokens`行为与扩展思考，您可能需要：

- 更积极地监控和管理您的令牌使用
- 随着提示长度的变化调整`max_tokens`值
- 可能更频繁地使用[令牌计数端点](/docs/zh-CN/build-with-claude/token-counting)
- 意识到之前的思考块不会在您的上下文窗口中累积

这一变化是为了提供更可预测和透明的行为，特别是随着最大令牌限制显著增加。

## 最大令牌数和扩展思考的上下文窗口大小

在较早的 Claude 模型（Claude Sonnet 3.7 之前），如果提示令牌和 `max_tokens` 的总和超过了模型的上下文窗口，系统会自动调整 `max_tokens` 以适应上下文限制。这意味着您可以设置一个较大的 `max_tokens` 值，系统会根据需要静默地减少它。

使用 Claude 3.7 和 4 模型时，`max_tokens`（启用思考时包括您的思考预算）被强制执行为严格限制。如果提示令牌 + `max_tokens` 超过上下文窗口大小，系统现在将返回验证错误。

<Note>
您可以阅读我们的[上下文窗口指南](/docs/zh-CN/build-with-claude/context-windows)以获得更深入的了解。
</Note>

### 启用扩展思考的上下文窗口

在计算启用思考的上下文窗口使用情况时，需要注意以下几点：

- 来自先前轮次的思考块被剥离，不计入您的上下文窗口
- 当前轮次的思考计入该轮次的 `max_tokens` 限制

下面的图表演示了启用扩展思考时的专门令牌管理：

![启用扩展思考的上下文窗口图表](/docs/images/context-window-thinking.svg)

有效的上下文窗口计算如下：

```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

我们建议使用[令牌计数 API](/docs/zh-CN/build-with-claude/token-counting)来获得您特定用例的准确令牌计数，特别是在处理包含思考的多轮对话时。

### 启用扩展思考和工具使用的上下文窗口

使用扩展思考和工具使用时，思考块必须被显式保留并与工具结果一起返回。

启用扩展思考和工具使用的有效上下文窗口计算变为：

```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```

下面的图表说明了扩展思考和工具使用的令牌管理：

![启用扩展思考和工具使用的上下文窗口图表](/docs/images/context-window-thinking-tools.svg)

### 使用扩展思考管理令牌

考虑到 Claude 3.7 和 4 模型的上下文窗口和 `max_tokens` 行为与扩展思考的关系，您可能需要：

- 更主动地监控和管理您的令牌使用
- 随着提示长度的变化调整 `max_tokens` 值
- 可能需要更频繁地使用[令牌计数端点](/docs/zh-CN/build-with-claude/token-counting)
- 意识到先前的思考块不会在您的上下文窗口中累积

做出这一改变是为了提供更可预测和透明的行为，特别是因为最大令牌限制已大幅增加。

## 思考加密

完整的思考内容被加密并在 `signature` 字段中返回。该字段用于验证思考块是由 Claude 生成的，当传回 API 时使用。

<Note>
只有在使用[带扩展思考的工具](#extended-thinking-with-tool-use)时才严格需要发送回思考块。否则，您可以省略先前轮次的思考块，或者如果您传回它们，让 API 为您剥离它们。

如果发送回思考块，我们建议按照您接收的方式传回所有内容以保持一致性并避免潜在问题。
</Note>

以下是关于思考加密的一些重要注意事项：
- 当[流式传输响应](#streaming-thinking)时，签名通过 `content_block_delta` 事件中的 `signature_delta` 添加，位于 `content_block_stop` 事件之前。
- `signature` 值在 Claude 4 模型中的长度明显长于之前的模型。
- `signature` 字段是一个不透明字段，不应被解释或解析 - 它仅用于验证目的。
- `signature` 值在各平台上兼容（Claude API、[Amazon Bedrock](/docs/zh-CN/build-with-claude/claude-on-amazon-bedrock) 和 [Vertex AI](/docs/zh-CN/build-with-claude/claude-on-vertex-ai)）。在一个平台上生成的值将与另一个平台兼容。

### 思考编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密 `thinking` 块的部分或全部内容，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 意识到编辑的思考块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的一些内部推理已自动加密以确保安全。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉编辑的块，同时保留正常的思考块
- 透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理以优雅地管理编辑的思考，而不会破坏您的 UI

以下是显示正常和编辑思考块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到编辑的思考块是预期的行为。该模型仍然可以使用这个编辑的推理来为其响应提供信息，同时保持安全护栏。

如果您需要在应用程序中测试编辑的思考处理，您可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手轮次的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用编辑的思考块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发编辑的思考（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别编辑的思考块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块仍然可用于后续请求

    # 提取所有块（编辑和非编辑）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 在传递到后续请求时，包括所有块而不进行修改
    # 这保留了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发编辑的思考（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别编辑的思考块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块仍然可用于后续请求

  // 提取所有块（编辑和非编辑）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 在传递到后续请求时，包括所有块而不进行修改
  // 这保留了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发编辑的思考（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别编辑的思考块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块仍然可用于后续请求
            // 提取所有块（编辑和非编辑）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 在传递到后续请求时，包括所有块而不进行修改
            // 这保留了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

### 思考编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密 `thinking` 块的部分或全部内容，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 意识到编辑的思考块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的一些内部推理已自动加密以确保安全。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉编辑的块，同时保留正常的思考块
- 透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理以优雅地管理编辑的思考，而不会破坏您的 UI

以下是显示正常和编辑思考块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到编辑的思考块是预期的行为。该模型仍然可以使用这个编辑的推理来为其响应提供信息，同时保持安全护栏。

如果您需要在应用程序中测试编辑的思考处理，您可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手轮次的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用编辑的思考块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发编辑的思考（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别编辑的思考块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块仍然可用于后续请求

    # 提取所有块（编辑和非编辑）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 在传递到后续请求时，包括所有块而不进行修改
    # 这保留了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发编辑的思考（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别编辑的思考块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块仍然可用于后续请求

  // 提取所有块（编辑和非编辑）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 在传递到后续请求时，包括所有块而不进行修改
  // 这保留了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发编辑的思考（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别编辑的思考块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块仍然可用于后续请求
            // 提取所有块（编辑和非编辑）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 在传递到后续请求时，包括所有块而不进行修改
            // 这保留了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

## 不同模型版本之间的思考差异

Messages API 在 Claude Sonnet 3.7 和 Claude 4 模型之间处理思考的方式不同，主要在编辑和总结行为方面。

请参阅下表以获得简明比较：

| 功能 | Claude Sonnet 3.7 | Claude 4 模型（Opus 4.5 之前） | Claude Opus 4.5 及更高版本 |
|---------|------------------|-------------------------------|--------------------------|
| **思考输出** | 返回完整思考输出 | 返回总结的思考 | 返回总结的思考 |
| **交错思考** | 不支持 | 支持 `interleaved-thinking-2025-05-14` beta 标头 | 支持 `interleaved-thinking-2025-05-14` beta 标头 |
| **思考块保留** | 不跨轮保留 | 不跨轮保留 | **默认保留**（启用缓存优化、令牌节省） |

### Claude Opus 4.5 中的思考块保留

Claude Opus 4.5 引入了一个新的默认行为：**来自先前助手轮次的思考块默认在模型上下文中保留**。这与较早的模型不同，较早的模型会删除先前轮次的思考块。

**思考块保留的好处：**

- **缓存优化**：使用工具使用时，保留的思考块启用缓存命中，因为它们与工具结果一起传回并在助手轮次中增量缓存，在多步工作流中产生令牌节省
- **无智能影响**：保留思考块对模型性能没有负面影响

**重要注意事项：**

- **上下文使用**：长对话将消耗更多上下文空间，因为思考块保留在上下文中
- **自动行为**：这是 Claude Opus 4.5 的默认行为 - 不需要代码更改或 beta 标头
- **向后兼容性**：要利用此功能，继续将完整的、未修改的思考块传回 API，就像您对工具使用所做的那样

<Note>
对于较早的模型（Claude Sonnet 4.5、Opus 4.1 等），来自先前轮次的思考块继续从上下文中删除。[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分中描述的现有行为适用于这些模型。
</Note>

### 思考内容编辑

有时 Claude 的内部推理会被我们的安全系统标记。当这种情况发生时，我们会加密部分或全部 `thinking` 块，并将其作为 `redacted_thinking` 块返回给您。`redacted_thinking` 块在传回 API 时会被解密，允许 Claude 继续其响应而不会丢失上下文。

在构建使用扩展思考的面向客户的应用程序时：

- 请注意 redacted thinking 块包含不可读的加密内容
- 考虑提供简单的解释，例如："Claude 的某些内部推理已因安全原因自动加密。这不会影响响应的质量。"
- 如果向用户显示思考块，您可以过滤掉 redacted 块，同时保留正常的思考块
- 要透明地说明使用扩展思考功能可能偶尔会导致某些推理被加密
- 实施适当的错误处理，以优雅地管理 redacted thinking，而不会破坏您的 UI

以下是显示正常和 redacted thinking 块的示例：

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

<Note>
在您的输出中看到 redacted thinking 块是预期的行为。该模型仍然可以使用这个 redacted 推理来为其响应提供信息，同时维护安全护栏。

如果您需要在应用程序中测试 redacted thinking 处理，可以使用此特殊测试字符串作为您的提示：`ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

在多轮对话中将 `thinking` 和 `redacted_thinking` 块传回 API 时，您必须将最后一个助手转向的完整未修改块传回 API。这对于维持模型的推理流程至关重要。我们建议始终将所有思考块传回 API。有关更多详情，请参阅上面的[保留思考块](#preserving-thinking-blocks)部分。

<section title="示例：使用 redacted thinking 块">

此示例演示了当 Claude 的内部推理包含被安全系统标记的内容时，如何处理可能出现在响应中的 `redacted_thinking` 块：

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 使用特殊提示触发 redacted thinking（仅用于演示目的）
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# 识别 redacted thinking 块
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # 这些块在后续请求中仍然可用

    # 提取所有块（redacted 和非 redacted）
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # 传递到后续请求时，包括所有块而不进行修改
    # 这保持了 Claude 推理的完整性

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 使用特殊提示触发 redacted thinking（仅用于演示目的）
const response = await client.messages.create({
  model: "claude-sonnet-4-5-20250929",
  max_tokens: 16000,
  thinking: {
    type: "enabled",
    budget_tokens: 10000
  },
  messages: [{
    role: "user",
    content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  }]
});

// 识别 redacted thinking 块
const hasRedactedThinking = response.content.some(
  block => block.type === "redacted_thinking"
);

if (hasRedactedThinking) {
  console.log("Response contains redacted thinking blocks");
  // 这些块在后续请求中仍然可用

  // 提取所有块（redacted 和非 redacted）
  const allThinkingBlocks = response.content.filter(
    block => block.type === "thinking" || block.type === "redacted_thinking"
  );

  // 传递到后续请求时，包括所有块而不进行修改
  // 这保持了 Claude 推理的完整性

  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
  console.log(`These blocks are still billable as output tokens`);
}
```

```java Java
import java.util.List;

import static java.util.stream.Collectors.toList;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.beta.messages.BetaContentBlock;
import com.anthropic.models.beta.messages.BetaMessage;
import com.anthropic.models.beta.messages.MessageCreateParams;
import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
import com.anthropic.models.messages.Model;

public class RedactedThinkingExample {
    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 使用特殊提示触发 redacted thinking（仅用于演示目的）
        BetaMessage response = client.beta().messages().create(
                MessageCreateParams.builder()
                        .model(Model.CLAUDE_SONNET_4_5)
                        .maxTokens(16000)
                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                        .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                        .build()
        );

        // 识别 redacted thinking 块
        boolean hasRedactedThinking = response.content().stream()
                .anyMatch(BetaContentBlock::isRedactedThinking);

        if (hasRedactedThinking) {
            System.out.println("Response contains redacted thinking blocks");
            // 这些块在后续请求中仍然可用
            // 提取所有块（redacted 和非 redacted）
            List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                    .filter(block -> block.isThinking() ||
                            block.isRedactedThinking())
                    .collect(toList());

            // 传递到后续请求时，包括所有块而不进行修改
            // 这保持了 Claude 推理的完整性
            System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
            System.out.println("These blocks are still billable as output tokens");
        }
    }
}
```

</CodeGroup>

<TryInConsoleButton
  userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
  thinkingBudgetTokens={16000}
>
  在控制台中尝试
</TryInConsoleButton>

</section>

## 不同模型版本中的思考差异

Messages API 在 Claude Sonnet 3.7 和 Claude 4 模型中处理思考的方式不同，主要在编辑和总结行为方面。

请参阅下表了解简明比较：

| 功能 | Claude Sonnet 3.7 | Claude 4 模型（Opus 4.5 之前） | Claude Opus 4.5 及更高版本 |
|---------|------------------|-------------------------------|--------------------------|
| **思考输出** | 返回完整思考输出 | 返回总结的思考 | 返回总结的思考 |
| **交错思考** | 不支持 | 支持 `interleaved-thinking-2025-05-14` beta 标头 | 支持 `interleaved-thinking-2025-05-14` beta 标头 |
| **思考块保留** | 不跨转向保留 | 不跨转向保留 | **默认保留**（启用缓存优化、节省令牌） |

### Claude Opus 4.5 中的思考块保留

Claude Opus 4.5 引入了一个新的默认行为：**来自先前助手转向的思考块默认在模型上下文中保留**。这与早期模型不同，早期模型会删除先前转向的思考块。

**思考块保留的好处：**

- **缓存优化**：使用工具使用时，保留的思考块启用缓存命中，因为它们与工具结果一起传回并在助手转向中增量缓存，在多步工作流中节省令牌
- **无智能影响**：保留思考块对模型性能没有负面影响

**重要考虑事项：**

- **上下文使用**：长对话将消耗更多上下文空间，因为思考块保留在上下文中
- **自动行为**：这是 Claude Opus 4.5 的默认行为——不需要代码更改或 beta 标头
- **向后兼容性**：要利用此功能，继续将完整的、未修改的思考块传回 API，就像您对工具使用所做的那样

<Note>
对于早期模型（Claude Sonnet 4.5、Opus 4.1 等），来自先前转向的思考块继续从上下文中删除。[扩展思考与提示缓存](#extended-thinking-with-prompt-caching)部分中描述的现有行为适用于这些模型。
</Note>

## 定价

有关完整的定价信息，包括基础费率、缓存写入、缓存命中和输出令牌，请参阅[定价页面](/docs/zh-CN/about-claude/pricing)。

思考过程产生的费用包括：
- 思考期间使用的令牌（输出令牌）
- 后续请求中包含的最后一个助手转向的思考块（输入令牌）
- 标准文本输出令牌

<Note>
启用扩展思考时，会自动包含专门的系统提示以支持此功能。
</Note>

使用总结思考时：
- **输入令牌**：您原始请求中的令牌（不包括先前转向的思考令牌）
- **输出令牌（计费）**：Claude 内部生成的原始思考令牌
- **输出令牌（可见）**：您在响应中看到的总结思考令牌
- **无费用**：用于生成摘要的令牌

<Warning>
计费的输出令牌计数将**不**与响应中的可见令牌计数匹配。您需要为完整的思考过程付费，而不是您看到的摘要。
</Warning>

## 扩展思考的最佳实践和注意事项

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

### 使用指南

- **任务选择**：对于特别复杂的任务使用扩展思考，这些任务受益于逐步推理，如数学、编码和分析。
- **上下文处理**：您不需要自己删除以前的思考块。Claude API 会自动忽略先前转向的思考块，在计算上下文使用时不包括它们。
- **提示工程**：如果您想最大化 Claude 的思考能力，请查看我们的[扩展思考提示技巧](/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips)。

## 扩展思考的最佳实践和注意事项

### 使用思考预算

- **预算优化**：最小预算为 1,024 个令牌。我们建议从最小值开始，逐步增加思考预算，以找到适合您用例的最优范围。更高的令牌计数可以实现更全面的推理，但根据任务的不同会有递减的回报。增加预算可以提高响应质量，但代价是增加延迟。对于关键任务，测试不同的设置以找到最优平衡。请注意，思考预算是一个目标而不是严格的限制——实际令牌使用可能因任务而异。
- **起点**：对于复杂任务，从较大的思考预算（16k+ 个令牌）开始，并根据您的需求进行调整。
- **大预算**：对于超过 32k 的思考预算，我们建议使用[批处理](/docs/zh-CN/build-with-claude/batch-processing)以避免网络问题。推动模型思考超过 32k 个令牌的请求会导致长时间运行的请求，可能会遇到系统超时和开放连接限制。
- **令牌使用跟踪**：监控思考令牌使用情况以优化成本和性能。

### 性能考虑

- **响应时间**：准备好可能更长的响应时间，因为推理过程需要额外的处理。考虑到生成思考块可能会增加总体响应时间。
- **流式传输要求**：当 `max_tokens` 大于 21,333 时需要流式传输。流式传输时，准备好处理思考和文本内容块到达时的情况。

### 功能兼容性

- 思考与 `temperature` 或 `top_k` 修改以及[强制工具使用](/docs/zh-CN/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use)不兼容。
- 启用思考时，您可以将 `top_p` 设置为 1 到 0.95 之间的值。
- 启用思考时，您无法预填充响应。
- 思考预算的更改会使包含消息的缓存提示前缀失效。但是，当思考参数更改时，缓存的系统提示和工具定义将继续工作。

### 使用指南

- **任务选择**：对于特别复杂的任务使用扩展思考，这些任务受益于逐步推理，如数学、编码和分析。
- **上下文处理**：您不需要自己删除以前的思考块。Claude API 会自动忽略先前转向的思考块，在计算上下文使用时不包括它们。
- **提示工程**：如果您想最大化 Claude 的思考能力，请查看我们的[扩展思考提示技巧](/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips)。

## 后续步骤

<CardGroup>
  <Card title="尝试扩展思考食谱" icon="book" href="https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking">
    在我们的食谱中探索思考的实际示例。
  </Card>
  <Card title="扩展思考提示技巧" icon="code" href="/docs/zh-CN/build-with-claude/prompt-engineering/extended-thinking-tips">
    学习扩展思考的提示工程最佳实践。
  </Card>
</CardGroup>

# 引用

Claude能够在回答有关文档的问题时提供详细的引用，帮助您跟踪和验证响应中的信息来源。

---

Claude能够在回答有关文档的问题时提供详细的引用，帮助您跟踪和验证响应中的信息来源。

所有[活跃模型](/docs/zh-CN/about-claude/models/overview)都支持引用，除了Haiku 3。

<Warning>
*Claude Sonnet 3.7的引用*

与其他Claude模型相比，Claude Sonnet 3.7在没有用户更明确指示的情况下，可能不太容易进行引用。在使用Claude Sonnet 3.7进行引用时，我们建议在`user`轮次中包含额外的指示，例如`"使用引用来支持您的答案。"`

我们还观察到，当要求模型结构化其响应时，除非明确告知在该格式内使用引用，否则它不太可能使用引用。例如，如果要求模型在其响应中使用`<result>`标签，您应该添加类似`"始终在您的答案中使用引用，即使在<result>标签内也是如此。"`的内容
</Warning>
<Tip>
  请使用此[表单](https://forms.gle/9n9hSrKnKe3rpowH9)分享您对引用功能的反馈和建议。
</Tip>

以下是如何在Messages API中使用引用的示例：

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "document",
            "source": {
              "type": "text",
              "media_type": "text/plain",
              "data": "The grass is green. The sky is blue."
            },
            "title": "My Document",
            "context": "This is a trustworthy document.",
            "citations": {"enabled": true}
          },
          {
            "type": "text",
            "text": "What color is the grass and sky?"
          }
        ]
      }
    ]
  }'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "document",
                    "source": {
                        "type": "text",
                        "media_type": "text/plain",
                        "data": "The grass is green. The sky is blue."
                    },
                    "title": "My Document",
                    "context": "This is a trustworthy document.",
                    "citations": {"enabled": True}
                },
                {
                    "type": "text",
                    "text": "What color is the grass and sky?"
                }
            ]
        }
    ]
)
print(response)
```

```java Java
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.*;

public class DocumentExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        PlainTextSource source = PlainTextSource.builder()
                .data("The grass is green. The sky is blue.")
                .build();

        DocumentBlockParam documentParam = DocumentBlockParam.builder()
                .source(source)
                .title("My Document")
                .context("This is a trustworthy document.")
                .citations(CitationsConfigParam.builder().enabled(true).build())
                .build();
        
        TextBlockParam textBlockParam = TextBlockParam.builder()
                .text("What color is the grass and sky?")
                .build();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_SONNET_4_20250514)
                .maxTokens(1024)
                .addUserMessageOfBlockParams(List.of(ContentBlockParam.ofDocument(documentParam), ContentBlockParam.ofText(textBlockParam)))
                .build();

        Message message = client.messages().create(params);
        System.out.println(message);
    }
}
```

</CodeGroup>

<Tip>
**与基于提示的方法的比较**

与基于提示的引用解决方案相比，引用功能具有以下优势：
- **成本节省：** 如果您基于提示的方法要求Claude输出直接引用，您可能会看到成本节省，因为`cited_text`不计入您的输出令牌。
- **更好的引用可靠性：** 因为我们将引用解析为上述相应的响应格式并提取`cited_text`，引用保证包含指向提供文档的有效指针。
- **改进的引用质量：** 在我们的评估中，我们发现与纯粹基于提示的方法相比，引用功能更有可能引用文档中最相关的引用。
</Tip>

---

## 引用的工作原理

通过以下步骤将引用与Claude集成：

<Steps>
  <Step title="提供文档并启用引用">
    - 包含任何支持格式的文档：[PDF](#pdf-documents)、[纯文本](#plain-text-documents)或[自定义内容](#custom-content-documents)文档
    - 在每个文档上设置`citations.enabled=true`。目前，必须在请求中的所有文档或没有文档上启用引用。
    - 请注意，目前仅支持文本引用，图像引用尚不可能。
  </Step>
  <Step title="文档被处理">
    - 文档内容被"分块"以定义可能引用的最小粒度。例如，句子分块将允许Claude引用单个句子或将多个连续句子链接在一起以引用段落（或更长）！
      - **对于PDF：** 文本按[PDF支持](/docs/zh-CN/build-with-claude/pdf-support)中描述的方式提取，内容被分块为句子。目前不支持从PDF引用图像。
      - **对于纯文本文档：** 内容被分块为可以引用的句子。
      - **对于自定义内容文档：** 您提供的内容块按原样使用，不进行进一步分块。
  </Step>
  <Step title="Claude提供引用响应">
    - 响应现在可能包含多个文本块，其中每个文本块可以包含Claude正在做出的声明和支持该声明的引用列表。
    - 引用引用源文档中的特定位置。这些引用的格式取决于被引用文档的类型。
      - **对于PDF：** 引用将包括页码范围（从1开始索引）。
      - **对于纯文本文档：** 引用将包括字符索引范围（从0开始索引）。
      - **对于自定义内容文档：** 引用将包括内容块索引范围（从0开始索引），对应于提供的原始内容列表。
    - 提供文档索引以指示参考源，并根据原始请求中所有文档的列表从0开始索引。
  </Step>
</Steps>

<Tip>
  **自动分块与自定义内容**

  默认情况下，纯文本和PDF文档会自动分块为句子。如果您需要更多地控制引用粒度（例如，对于项目符号或转录），请改用自定义内容文档。有关更多详细信息，请参阅[文档类型](#document-types)。

  例如，如果您希望Claude能够从您的RAG块中引用特定句子，您应该将每个RAG块放入纯文本文档中。否则，如果您不希望进行任何进一步的分块，或者如果您想自定义任何额外的分块，您可以将RAG块放入自定义内容文档中。
</Tip>

### 可引用与不可引用的内容

- 文档的`source`内容中找到的文本可以被引用。
- `title`和`context`是可选字段，将传递给模型但不用于引用内容。
- `title`长度有限，因此您可能会发现`context`字段在存储任何文档元数据作为文本或字符串化json时很有用。

### 引用索引
- 文档索引从请求中所有文档内容块的列表（跨所有消息）从0开始索引。
- 字符索引从0开始索引，具有排他性结束索引。
- 页码从1开始索引，具有排他性结束页码。
- 内容块索引从自定义内容文档中提供的`content`列表从0开始索引，具有排他性结束索引。

### 令牌成本
- 启用引用会由于系统提示添加和文档分块而导致输入令牌略有增加。
- 但是，引用功能在输出令牌方面非常高效。在底层，模型以标准化格式输出引用，然后解析为引用文本和文档位置索引。`cited_text`字段是为了方便而提供的，不计入输出令牌。
- 当在后续对话轮次中传回时，`cited_text`也不计入输入令牌。

### 功能兼容性
引用与其他API功能结合使用，包括[提示缓存](/docs/zh-CN/build-with-claude/prompt-caching)、[令牌计数](/docs/zh-CN/build-with-claude/token-counting)和[批处理](/docs/zh-CN/build-with-claude/batch-processing)。

#### 将提示缓存与引用一起使用

引用和提示缓存可以有效地一起使用。

在响应中生成的引用块不能直接缓存，但它们引用的源文档可以被缓存。为了优化性能，将`cache_control`应用于您的顶级文档内容块。

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic()

# 长文档内容（例如，技术文档）
long_document = "This is a very long document with thousands of words..." + " ... " * 1000  # 最小可缓存长度

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "document",
                    "source": {
                        "type": "text",
                        "media_type": "text/plain",
                        "data": long_document
                    },
                    "citations": {"enabled": True},
                    "cache_control": {"type": "ephemeral"}  # 缓存文档内容
                },
                {
                    "type": "text",
                    "text": "What does this document say about API features?"
                }
            ]
        }
    ]
)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

// 长文档内容（例如，技术文档）
const longDocument = "This is a very long document with thousands of words..." + " ... ".repeat(1000);  // 最小可缓存长度

const response = await client.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  messages: [
    {
      role: "user",
      content: [
        {
          type: "document",
          source: {
            type: "text",
            media_type: "text/plain",
            data: longDocument
          },
          citations: { enabled: true },
          cache_control: { type: "ephemeral" }  // 缓存文档内容
        },
        {
          type: "text",
          text: "What does this document say about API features?"
        }
      ]
    }
  ]
});
```

```bash Shell
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "document",
                    "source": {
                        "type": "text",
                        "media_type": "text/plain",
                        "data": "This is a very long document with thousands of words..."
                    },
                    "citations": {"enabled": true},
                    "cache_control": {"type": "ephemeral"}
                },
                {
                    "type": "text",
                    "text": "What does this document say about API features?"
                }
            ]
        }
    ]
}'
```
</CodeGroup>

在此示例中：
- 文档内容使用文档块上的`cache_control`进行缓存
- 在文档上启用引用
- Claude可以生成带有引用的响应，同时受益于缓存的文档内容
- 使用相同文档的后续请求将受益于缓存的内容

## 文档类型

### 选择文档类型

我们支持三种文档类型用于引用。文档可以直接在消息中提供（base64、文本或URL）或通过[Files API](/docs/zh-CN/build-with-claude/files)上传并通过`file_id`引用：

| 类型 | 最适合 | 分块 | 引用格式 |
| :--- | :--- | :--- | :--- |
| 纯文本 | 简单文本文档、散文 | 句子 | 字符索引（从0开始索引） |
| PDF | 带有文本内容的PDF文件 | 句子 | 页码（从1开始索引） |
| 自定义内容 | 列表、转录、特殊格式、更细粒度的引用 | 无额外分块 | 块索引（从0开始索引） |

<Note>
不支持.csv、.xlsx、.docx、.md和.txt文件作为文档块。将这些转换为纯文本并直接包含在消息内容中。请参阅[使用其他文件格式](/docs/zh-CN/build-with-claude/files#working-with-other-file-formats)。
</Note>

### 纯文本文档

纯文本文档会自动分块为句子。您可以内联提供它们或通过其`file_id`引用：

<Tabs>
<Tab title="内联文本">
```python
{
    "type": "document",
    "source": {
        "type": "text",
        "media_type": "text/plain",
        "data": "Plain text content..."
    },
    "title": "Document Title", # 可选
    "context": "Context about the document that will not be cited from", # 可选
    "citations": {"enabled": True}
}
```
</Tab>
<Tab title="Files API">
```python
{
    "type": "document",
    "source": {
        "type": "file",
        "file_id": "file_011CNvxoj286tYUAZFiZMf1U"
    },
    "title": "Document Title", # 可选
    "context": "Context about the document that will not be cited from", # 可选
    "citations": {"enabled": True}
}
```
</Tab>
</Tabs>

<section title="纯文本引用示例">

```python
{
    "type": "char_location",
    "cited_text": "The exact text being cited", # 不计入输出令牌
    "document_index": 0,
    "document_title": "Document Title",
    "start_char_index": 0,    # 从0开始索引
    "end_char_index": 50      # 排他性
}
```

</section>

### PDF文档

PDF文档可以作为base64编码数据或通过`file_id`提供。PDF文本被提取并分块为句子。由于尚不支持图像引用，扫描文档且不包含可提取文本的PDF将无法引用。

<Tabs>
<Tab title="Base64">
```python
{
    "type": "document",
    "source": {
        "type": "base64",
        "media_type": "application/pdf",
        "data": base64_encoded_pdf_data
    },
    "title": "Document Title", # 可选
    "context": "Context about the document that will not be cited from", # 可选
    "citations": {"enabled": True}
}
```
</Tab>
<Tab title="Files API">
```python
{
    "type": "document",
    "source": {
        "type": "file",
        "file_id": "file_011CNvxoj286tYUAZFiZMf1U"
    },
    "title": "Document Title", # 可选
    "context": "Context about the document that will not be cited from", # 可选
    "citations": {"enabled": True}
}
```
</Tab>
</Tabs>

<section title="PDF引用示例">

```python
{
    "type": "page_location",
    "cited_text": "The exact text being cited", # 不计入输出令牌
    "document_index": 0,     
    "document_title": "Document Title", 
    "start_page_number": 1,  # 从1开始索引
    "end_page_number": 2     # 排他性
}
```

</section>

### 自定义内容文档

自定义内容文档让您控制引用粒度。不进行额外分块，块根据提供的内容块提供给模型。

```python
{
    "type": "document",
    "source": {
        "type": "content",
        "content": [
            {"type": "text", "text": "First chunk"},
            {"type": "text", "text": "Second chunk"}
        ]
    },
    "title": "Document Title", # 可选
    "context": "Context about the document that will not be cited from", # 可选
    "citations": {"enabled": True}
}
```

<section title="引用示例">

```python
{
    "type": "content_block_location",
    "cited_text": "The exact text being cited", # 不计入输出令牌
    "document_index": 0,
    "document_title": "Document Title",
    "start_block_index": 0,   # 从0开始索引
    "end_block_index": 1      # 排他性
}
```

</section>

---

## 响应结构

启用引用时，响应包括带有引用的多个文本块：

```python
{
    "content": [
        {
            "type": "text",
            "text": "According to the document, "
        },
        {
            "type": "text",
            "text": "the grass is green",
            "citations": [{
                "type": "char_location",
                "cited_text": "The grass is green.",
                "document_index": 0,
                "document_title": "Example Document",
                "start_char_index": 0,
                "end_char_index": 20
            }]
        },
        {
            "type": "text",
            "text": " and "
        },
        {
            "type": "text",
            "text": "the sky is blue",
            "citations": [{
                "type": "char_location",
                "cited_text": "The sky is blue.",
                "document_index": 0,
                "document_title": "Example Document",
                "start_char_index": 20,
                "end_char_index": 36
            }]
        },
        {
            "type": "text",
            "text": ". Information from page 5 states that ",
        },
        {
            "type": "text",
            "text": "water is essential",
            "citations": [{
                "type": "page_location",
                "cited_text": "Water is essential for life.",
                "document_index": 1,
                "document_title": "PDF Document",
                "start_page_number": 5,
                "end_page_number": 6
            }]
        },
        {
            "type": "text",
            "text": ". The custom document mentions ",
        },
        {
            "type": "text",
            "text": "important findings",
            "citations": [{
                "type": "content_block_location",
                "cited_text": "These are important findings.",
                "document_index": 2,
                "document_title": "Custom Content Document",
                "start_block_index": 0,
                "end_block_index": 1
            }]
        }
    ]
}
```

### 流式支持

对于流式响应，我们添加了一个`citations_delta`类型，其中包含要添加到当前`text`内容块上的`citations`列表中的单个引用。

<section title="流式事件示例">

```python
event: message_start
data: {"type": "message_start", ...}

event: content_block_start
data: {"type": "content_block_start", "index": 0, ...}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, 
       "delta": {"type": "text_delta", "text": "According to..."}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0,
       "delta": {"type": "citations_delta", 
                 "citation": {
                     "type": "char_location",
                     "cited_text": "...",
                     "document_index": 0,
                     ...
                 }}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: message_stop
data: {"type": "message_stop"}
```

</section>

# 多语言支持

Claude 在多种语言的任务中表现出色，相对于英语保持强大的跨语言性能。

---

## 概述

Claude 展示了强大的多语言能力，特别是在跨语言的零样本任务中表现出色。该模型在广泛使用的语言和低资源语言中都保持一致的相对性能，使其成为多语言应用的可靠选择。

请注意，Claude 能够处理许多超出下面基准测试范围的语言。我们鼓励您使用与您特定用例相关的任何语言进行测试。

## 性能数据

以下是 Claude 4、Claude 3.7 Sonnet 和 Claude 3.5 模型在不同语言中的零样本链式思维评估分数，显示为相对于英语性能的百分比（100%）：

| 语言 | Claude Opus 4<sup>1</sup> | Claude Sonnet 4<sup>1</sup> | Claude Sonnet 3.7 ([已弃用](/docs/zh-CN/about-claude/model-deprecations))<sup>1</sup> | Claude Haiku 3.5|
|---|---|---|---|---|
| 英语（基准，固定为 100%） | 100% | 100% | 100% | 100% |
| 西班牙语 | 98.0% | 97.5% | 97.6% | 94.6% |
| 葡萄牙语（巴西） | 97.3% | 97.2% | 97.3% | 94.6% |
| 意大利语 | 97.5% | 97.3% | 97.2% | 95.0% |
| 法语 | 97.7% | 97.1% | 96.9% | 95.3% |
| 印度尼西亚语 | 97.2% | 96.2% | 96.3% | 91.2% |
| 德语 | 97.1% | 94.7% | 96.2% | 92.5% |
| 阿拉伯语 | 96.9% | 96.1% | 95.4% | 84.7% |
| 中文（简体） | 96.7% | 95.9% | 95.3% | 90.9% |
| 韩语 | 96.4% | 95.9% | 95.2% | 89.1% |
| 日语 | 96.2% | 95.6% | 95.0% | 90.8% |
| 印地语 | 96.7% | 95.8% | 94.2% | 80.1% |
| 孟加拉语 | 95.2% | 94.4% | 92.4% | 72.9% |
| 斯瓦希里语 | 89.5% | 87.1% | 89.2% | 64.7% |
| 约鲁巴语 | 78.9% | 76.4% | 76.7% | 46.1% |

<sup>1</sup> 使用[扩展思维](/docs/zh-CN/build-with-claude/extended-thinking)。

<Note>
这些指标基于 [MMLU（大规模多任务语言理解）](https://en.wikipedia.org/wiki/MMLU)英语测试集，由专业人工翻译人员翻译成 14 种其他语言，如 [OpenAI 的 simple-evals 存储库](https://github.com/openai/simple-evals/blob/main/multilingual_mmlu_benchmark_results.md)所记录。使用人工翻译人员进行此评估可确保高质量的翻译，这对于数字资源较少的语言尤其重要。
</Note>

***

## 最佳实践

在处理多语言内容时：

1. **提供清晰的语言上下文**：虽然 Claude 可以自动检测目标语言，但明确说明所需的输入/输出语言可以提高可靠性。为了增强流畅性，您可以提示 Claude 使用"像母语使用者一样的习语表达"。
2. **使用本地文字**：以本地文字而非音译形式提交文本以获得最佳结果
3. **考虑文化背景**：有效的沟通通常需要超越纯粹翻译的文化和地区意识

我们还建议遵循我们的一般[提示工程指南](/docs/zh-CN/build-with-claude/prompt-engineering/overview)以更好地提高 Claude 的性能。

***

## 语言支持注意事项

- Claude 可以处理使用标准 Unicode 字符的大多数世界语言的输入和生成输出
- 性能因语言而异，在广泛使用的语言中具有特别强大的能力
- 即使在数字资源较少的语言中，Claude 也能保持有意义的能力

<CardGroup cols={2}>
  <Card title="提示工程指南" icon="edit" href="/docs/zh-CN/build-with-claude/prompt-engineering/overview">
    掌握提示制作的艺术，充分利用 Claude。
  </Card>
  <Card title="提示库" icon="books" href="/docs/zh-CN/resources/prompt-library">
    查找各种任务和行业的预制提示。非常适合获得灵感或快速开始。
  </Card>
</CardGroup>


# Token 计数

Token 计数使您能够在将消息发送给 Claude 之前确定消息中的 token 数量，帮助您对提示和使用做出明智的决策。

---

Token 计数使您能够在将消息发送给 Claude 之前确定消息中的 token 数量，帮助您对提示和使用做出明智的决策。通过 token 计数，您可以
- 主动管理速率限制和成本
- 做出智能的模型路由决策
- 优化提示以达到特定长度
---

## 如何计算消息 token

[token 计数](/docs/zh-CN/api/messages-count-tokens)端点接受与创建消息相同的结构化输入列表，包括支持系统提示、[工具](/docs/zh-CN/agents-and-tools/tool-use/overview)、[图像](/docs/zh-CN/build-with-claude/vision)和 [PDF](/docs/zh-CN/build-with-claude/pdf-support)。响应包含输入 token 的总数。

<Note>
token 计数应被视为**估计值**。在某些情况下，创建消息时使用的实际输入 token 数量可能会有少量差异。

Token 计数可能包括 Anthropic 为系统优化自动添加的 token。**您不会为系统添加的 token 付费**。计费仅反映您的内容。
</Note>

### 支持的模型
所有[活跃模型](/docs/zh-CN/about-claude/models/overview)都支持 token 计数。

### 计算基本消息中的 token

<CodeGroup>

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.count_tokens(
    model="claude-sonnet-4-5",
    system="You are a scientist",
    messages=[{
        "role": "user",
        "content": "Hello, Claude"
    }],
)

print(response.json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.countTokens({
  model: 'claude-sonnet-4-5',
  system: 'You are a scientist',
  messages: [{
    role: 'user',
    content: 'Hello, Claude'
  }]
});

console.log(response);
```

```bash Shell
curl https://api.anthropic.com/v1/messages/count_tokens \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "content-type: application/json" \
    --header "anthropic-version: 2023-06-01" \
    --data '{
      "model": "claude-sonnet-4-5",
      "system": "You are a scientist",
      "messages": [{
        "role": "user",
        "content": "Hello, Claude"
      }]
    }'
```

```java Java
import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.MessageCountTokensParams;
import com.anthropic.models.messages.MessageTokensCount;
import com.anthropic.models.messages.Model;

public class CountTokensExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        MessageCountTokensParams params = MessageCountTokensParams.builder()
                .model(Model.CLAUDE_SONNET_4_20250514)
                .system("You are a scientist")
                .addUserMessage("Hello, Claude")
                .build();

        MessageTokensCount count = client.messages().countTokens(params);
        System.out.println(count);
    }
}
```
</CodeGroup>

```json JSON
{ "input_tokens": 14 }
```

### 计算带工具的消息中的 token

<Note>
[服务器工具](/docs/zh-CN/agents-and-tools/tool-use/overview#server-tools) token 计数仅适用于第一次采样调用。
</Note>

<CodeGroup>

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.count_tokens(
    model="claude-sonnet-4-5",
    tools=[
        {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "input_schema": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    }
                },
                "required": ["location"],
            },
        }
    ],
    messages=[{"role": "user", "content": "What's the weather like in San Francisco?"}]
)

print(response.json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.countTokens({
  model: 'claude-sonnet-4-5',
  tools: [
    {
      name: "get_weather",
      description: "Get the current weather in a given location",
      input_schema: {
        type: "object",
        properties: {
          location: {
            type: "string",
            description: "The city and state, e.g. San Francisco, CA",
          }
        },
        required: ["location"],
      }
    }
  ],
  messages: [{ role: "user", content: "What's the weather like in San Francisco?" }]
});

console.log(response);
```

```bash Shell
curl https://api.anthropic.com/v1/messages/count_tokens \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "content-type: application/json" \
    --header "anthropic-version: 2023-06-01" \
    --data '{
      "model": "claude-sonnet-4-5",
      "tools": [
        {
          "name": "get_weather",
          "description": "Get the current weather in a given location",
          "input_schema": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA"
              }
            },
            "required": ["location"]
          }
        }
      ],
      "messages": [
        {
          "role": "user",
          "content": "What'\''s the weather like in San Francisco?"
        }
      ]
    }'
```

```java Java
import java.util.List;
import java.util.Map;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.core.JsonValue;
import com.anthropic.models.messages.MessageCountTokensParams;
import com.anthropic.models.messages.MessageTokensCount;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.Tool;
import com.anthropic.models.messages.Tool.InputSchema;

public class CountTokensWithToolsExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        InputSchema schema = InputSchema.builder()
                .properties(JsonValue.from(Map.of(
                        "location", Map.of(
 "type", "string",
 "description", "The city and state, e.g. San Francisco, CA"
                        )
                )))
                .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                .build();

        MessageCountTokensParams params = MessageCountTokensParams.builder()
                .model(Model.CLAUDE_SONNET_4_20250514)
                .addTool(Tool.builder()
                        .name("get_weather")
                        .description("Get the current weather in a given location")
                        .inputSchema(schema)
                        .build())
                .addUserMessage("What's the weather like in San Francisco?")
                .build();

        MessageTokensCount count = client.messages().countTokens(params);
        System.out.println(count);
    }
}
```
</CodeGroup>

```json JSON
{ "input_tokens": 403 }
```

### 计算带图像的消息中的 token

<CodeGroup>
```bash Shell
#!/bin/sh

IMAGE_URL="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
IMAGE_MEDIA_TYPE="image/jpeg"
IMAGE_BASE64=$(curl "$IMAGE_URL" | base64)

curl https://api.anthropic.com/v1/messages/count_tokens \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "messages": [
        {"role": "user", "content": [
            {"type": "image", "source": {
                "type": "base64",
                "media_type": "'$IMAGE_MEDIA_TYPE'",
                "data": "'$IMAGE_BASE64'"
            }},
            {"type": "text", "text": "Describe this image"}
        ]}
    ]
}'
```

```python Python
import anthropic
import base64
import httpx

image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
image_media_type = "image/jpeg"
image_data = base64.standard_b64encode(httpx.get(image_url).content).decode("utf-8")

client = anthropic.Anthropic()

response = client.messages.count_tokens(
    model="claude-sonnet-4-5",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": image_media_type,
                        "data": image_data,
                    },
                },
                {
                    "type": "text",
                    "text": "Describe this image"
                }
            ],
        }
    ],
)
print(response.json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

const image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
const image_media_type = "image/jpeg"
const image_array_buffer = await ((await fetch(image_url)).arrayBuffer());
const image_data = Buffer.from(image_array_buffer).toString('base64');

const response = await anthropic.messages.countTokens({
  model: 'claude-sonnet-4-5',
  messages: [
    {
      "role": "user",
      "content": [
        {
          "type": "image",
          "source": {
            "type": "base64",
            "media_type": image_media_type,
            "data": image_data,
          },
        }
      ],
    },
    {
      "type": "text",
      "text": "Describe this image"
    }
  ]
});
console.log(response);
```

```java Java
import java.util.Base64;
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.Base64ImageSource;
import com.anthropic.models.messages.ContentBlockParam;
import com.anthropic.models.messages.ImageBlockParam;
import com.anthropic.models.messages.MessageCountTokensParams;
import com.anthropic.models.messages.MessageTokensCount;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;

import java.net.URI;
import java.net.http.HttpClient;
import java.net.http.HttpRequest;
import java.net.http.HttpResponse;

public class CountTokensImageExample {

    public static void main(String[] args) throws Exception {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        String imageUrl = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg";
        String imageMediaType = "image/jpeg";

        HttpClient httpClient = HttpClient.newHttpClient();
        HttpRequest request = HttpRequest.newBuilder()
                .uri(URI.create(imageUrl))
                .build();
        byte[] imageBytes = httpClient.send(request, HttpResponse.BodyHandlers.ofByteArray()).body();
        String imageBase64 = Base64.getEncoder().encodeToString(imageBytes);

        ContentBlockParam imageBlock = ContentBlockParam.ofImage(
                ImageBlockParam.builder()
                        .source(Base64ImageSource.builder()
 .mediaType(Base64ImageSource.MediaType.IMAGE_JPEG)
 .data(imageBase64)
 .build())
                        .build());

        ContentBlockParam textBlock = ContentBlockParam.ofText(
                TextBlockParam.builder()
                        .text("Describe this image")
                        .build());

        MessageCountTokensParams params = MessageCountTokensParams.builder()
                .model(Model.CLAUDE_SONNET_4_20250514)
                .addUserMessageOfBlockParams(List.of(imageBlock, textBlock))
                .build();

        MessageTokensCount count = client.messages().countTokens(params);
        System.out.println(count);
    }
}
```
</CodeGroup>

```json JSON
{ "input_tokens": 1551 }
```

### 计算带扩展思考的消息中的 token

<Note>
请参阅[这里](/docs/zh-CN/build-with-claude/extended-thinking#how-context-window-is-calculated-with-extended-thinking)了解更多关于扩展思考如何计算上下文窗口的详细信息
- 来自**之前**助手轮次的思考块被忽略，**不会**计入您的输入 token
- **当前**助手轮次的思考**会**计入您的输入 token
</Note>

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages/count_tokens \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "content-type: application/json" \
    --header "anthropic-version: 2023-06-01" \
    --data '{
      "model": "claude-sonnet-4-5",
      "thinking": {
        "type": "enabled",
        "budget_tokens": 16000
      },
      "messages": [
        {
          "role": "user",
          "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        },
        {
          "role": "assistant",
          "content": [
            {
              "type": "thinking",
              "thinking": "This is a nice number theory question. Lets think about it step by step...",
              "signature": "EuYBCkQYAiJAgCs1le6/Pol5Z4/JMomVOouGrWdhYNsH3ukzUECbB6iWrSQtsQuRHJID6lWV..."
            },
            {
              "type": "text",
              "text": "Yes, there are infinitely many prime numbers p such that p mod 4 = 3..."
            }
          ]
        },
        {
          "role": "user",
          "content": "Can you write a formal proof?"
        }
      ]
    }'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.messages.count_tokens(
    model="claude-sonnet-4-5",
    thinking={
        "type": "enabled",
        "budget_tokens": 16000
    },
    messages=[
        {
            "role": "user",
            "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "thinking",
                    "thinking": "This is a nice number theory question. Let's think about it step by step...",
                    "signature": "EuYBCkQYAiJAgCs1le6/Pol5Z4/JMomVOouGrWdhYNsH3ukzUECbB6iWrSQtsQuRHJID6lWV..."
                },
                {
                  "type": "text",
                  "text": "Yes, there are infinitely many prime numbers p such that p mod 4 = 3..."
                }
            ]
        },
        {
            "role": "user",
            "content": "Can you write a formal proof?"
        }
    ]
)

print(response.json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const response = await client.messages.countTokens({
  model: 'claude-sonnet-4-5',
  thinking: {
    'type': 'enabled',
    'budget_tokens': 16000
  },
  messages: [
    {
      'role': 'user',
      'content': 'Are there an infinite number of prime numbers such that n mod 4 == 3?'
    },
    {
      'role': 'assistant',
      'content': [
        {
          'type': 'thinking',
          'thinking': "This is a nice number theory question. Let's think about it step by step...",
          'signature': 'EuYBCkQYAiJAgCs1le6/Pol5Z4/JMomVOouGrWdhYNsH3ukzUECbB6iWrSQtsQuRHJID6lWV...'
        },
        {
          'type': 'text',
          'text': 'Yes, there are infinitely many prime numbers p such that p mod 4 = 3...',
        }
      ]
    },
    {
      'role': 'user',
      'content': 'Can you write a formal proof?'
    }
  ]
});

console.log(response);
```

```java Java
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.ContentBlockParam;
import com.anthropic.models.messages.MessageCountTokensParams;
import com.anthropic.models.messages.MessageTokensCount;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;
import com.anthropic.models.messages.ThinkingBlockParam;

public class CountTokensThinkingExample {

    public static void main(String[] args) {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        List<ContentBlockParam> assistantBlocks = List.of(
                ContentBlockParam.ofThinking(ThinkingBlockParam.builder()
                        .thinking("This is a nice number theory question. Let's think about it step by step...")
                        .signature("EuYBCkQYAiJAgCs1le6/Pol5Z4/JMomVOouGrWdhYNsH3ukzUECbB6iWrSQtsQuRHJID6lWV...")
                        .build()),
                ContentBlockParam.ofText(TextBlockParam.builder()
                        .text("Yes, there are infinitely many prime numbers p such that p mod 4 = 3...")
                        .build())
        );

        MessageCountTokensParams params = MessageCountTokensParams.builder()
                .model(Model.CLAUDE_SONNET_4_20250514)
                .enabledThinking(16000)
                .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                .addAssistantMessageOfBlockParams(assistantBlocks)
                .addUserMessage("Can you write a formal proof?")
                .build();

        MessageTokensCount count = client.messages().countTokens(params);
        System.out.println(count);
    }
}
```
</CodeGroup>

```json JSON
{ "input_tokens": 88 }
```

### 计算带 PDF 的消息中的 token

<Note>
Token 计数支持 PDF，具有与 Messages API 相同的[限制](/docs/zh-CN/build-with-claude/pdf-support#pdf-support-limitations)。
</Note> 

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/messages/count_tokens \
    --header "x-api-key: $ANTHROPIC_API_KEY" \
    --header "content-type: application/json" \
    --header "anthropic-version: 2023-06-01" \
    --data '{
      "model": "claude-sonnet-4-5",
      "messages": [{
        "role": "user",
        "content": [
          {
            "type": "document",
            "source": {
              "type": "base64",
              "media_type": "application/pdf",
              "data": "'$(base64 -i document.pdf)'"
            }
          },
          {
            "type": "text",
            "text": "Please summarize this document."
          }
        ]
      }]
    }'
```

```python Python
import base64
import anthropic

client = anthropic.Anthropic()

with open("document.pdf", "rb") as pdf_file:
    pdf_base64 = base64.standard_b64encode(pdf_file.read()).decode("utf-8")

response = client.messages.count_tokens(
    model="claude-sonnet-4-5",
    messages=[{
        "role": "user",
        "content": [
            {
                "type": "document",
                "source": {
                    "type": "base64",
                    "media_type": "application/pdf",
                    "data": pdf_base64
                }
            },
            {
                "type": "text",
                "text": "Please summarize this document."
            }
        ]
    }]
)

print(response.json())
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import { readFileSync } from 'fs';

const client = new Anthropic();

const pdfBase64 = readFileSync('document.pdf', { encoding: 'base64' });

const response = await client.messages.countTokens({
  model: 'claude-sonnet-4-5',
  messages: [{
    role: 'user',
    content: [
      {
        type: 'document',
        source: {
          type: 'base64',
          media_type: 'application/pdf',
          data: pdfBase64
        }
      },
      {
        type: 'text',
        text: 'Please summarize this document.'
      }
    ]
  }]
});

console.log(response);
```

```java Java
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.Base64;
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.messages.Base64PdfSource;
import com.anthropic.models.messages.ContentBlockParam;
import com.anthropic.models.messages.DocumentBlockParam;
import com.anthropic.models.messages.MessageCountTokensParams;
import com.anthropic.models.messages.MessageTokensCount;
import com.anthropic.models.messages.Model;
import com.anthropic.models.messages.TextBlockParam;

public class CountTokensPdfExample {

    public static void main(String[] args) throws Exception {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        byte[] fileBytes = Files.readAllBytes(Path.of("document.pdf"));
        String pdfBase64 = Base64.getEncoder().encodeToString(fileBytes);

        ContentBlockParam documentBlock = ContentBlockParam.ofDocument(
                DocumentBlockParam.builder()
                        .source(Base64PdfSource.builder()
 .mediaType(Base64PdfSource.MediaType.APPLICATION_PDF)
 .data(pdfBase64)
 .build())
                        .build());

        ContentBlockParam textBlock = ContentBlockParam.ofText(
                TextBlockParam.builder()
                        .text("Please summarize this document.")
                        .build());

        MessageCountTokensParams params = MessageCountTokensParams.builder()
                .model(Model.CLAUDE_SONNET_4_20250514)
                .addUserMessageOfBlockParams(List.of(documentBlock, textBlock))
                .build();

        MessageTokensCount count = client.messages().countTokens(params);
        System.out.println(count);
    }
}
```
</CodeGroup>

```json JSON
{ "input_tokens": 2188 }
```

---

## 定价和速率限制

Token 计数**免费使用**，但受基于您的[使用层级](/docs/zh-CN/api/rate-limits#rate-limits)的每分钟请求数速率限制。如果您需要更高的限制，请通过 [Claude Console](/settings/limits) 联系销售。

| 使用层级 | 每分钟请求数 (RPM) |
|---|---|
| 1          | 100                       |
| 2          | 2,000                     |
| 3          | 4,000                     |
| 4          | 8,000                     |

<Note>
  Token 计数和消息创建具有独立的速率限制——使用其中一个不会计入另一个的限制。
</Note>

---
## 常见问题

  <section title="Token 计数是否使用提示缓存？">

    不，token 计数提供估计值而不使用缓存逻辑。虽然您可以在 token 计数请求中提供 `cache_control` 块，但提示缓存仅在实际消息创建期间发生。
  
</section>

# 嵌入向量

文本嵌入向量是文本的数值表示，能够测量语义相似性。本指南介绍嵌入向量、其应用以及如何使用嵌入模型进行搜索、推荐和异常检测等任务。

---

## 实施嵌入向量之前

在选择嵌入向量提供商时，您可以根据需求和偏好考虑以下几个因素：

- 数据集大小和领域特异性：模型训练数据集的大小及其与您要嵌入的领域的相关性。更大或更具领域特异性的数据通常会产生更好的领域内嵌入向量
- 推理性能：嵌入查找速度和端到端延迟。这对于大规模生产部署来说是一个特别重要的考虑因素
- 定制化：在私有数据上继续训练的选项，或针对非常特定领域的模型专业化。这可以提高在独特词汇表上的性能

## 如何通过 Anthropic 获取嵌入向量

Anthropic 不提供自己的嵌入模型。一个拥有广泛选择和能力的嵌入向量提供商，涵盖了上述所有考虑因素的是 Voyage AI。

Voyage AI 制作最先进的嵌入模型，并为特定行业领域（如金融和医疗保健）提供定制模型，或为个别客户提供定制微调模型。

本指南的其余部分针对 Voyage AI，但我们鼓励您评估各种嵌入向量供应商，以找到最适合您特定用例的选择。

## 可用模型

Voyage 推荐使用以下文本嵌入模型：

| 模型 | 上下文长度 | 嵌入维度 | 描述 |
| --- | --- | --- | --- |
| `voyage-3-large` | 32,000 | 1024（默认）、256、512、2048 | 最佳通用和多语言检索质量。详情请参见[博客文章](https://blog.voyageai.com/2025/01/07/voyage-3-large/)。 |
| `voyage-3.5` | 32,000 | 1024（默认）、256、512、2048 | 针对通用和多语言检索质量进行优化。详情请参见[博客文章](https://blog.voyageai.com/2025/05/20/voyage-3-5/)。 |
| `voyage-3.5-lite` | 32,000 | 1024（默认）、256、512、2048 | 针对延迟和成本进行优化。详情请参见[博客文章](https://blog.voyageai.com/2025/05/20/voyage-3-5/)。 |
| `voyage-code-3` | 32,000 | 1024（默认）、256、512、2048 | 针对**代码**检索进行优化。详情请参见[博客文章](https://blog.voyageai.com/2024/12/04/voyage-code-3/)。 |
| `voyage-finance-2` | 32,000 | 1024 | 针对**金融**检索和 RAG 进行优化。详情请参见[博客文章](https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/)。 |
| `voyage-law-2` | 16,000 | 1024 | 针对**法律**和**长上下文**检索和 RAG 进行优化。同时在所有领域都有改进的性能。详情请参见[博客文章](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/)。 |

此外，推荐以下多模态嵌入模型：

| 模型 | 上下文长度 | 嵌入维度 | 描述 |
| --- | --- | --- | --- |
| `voyage-multimodal-3` | 32000 | 1024 | 丰富的多模态嵌入模型，可以向量化交错的文本和内容丰富的图像，如 PDF、幻灯片、表格、图形等的截图。详情请参见[博客文章](https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/)。 |

需要帮助决定使用哪个文本嵌入模型？查看[常见问题解答](https://docs.voyageai.com/docs/faq#what-embedding-models-are-available-and-which-one-should-i-use&ref=anthropic)。

## Voyage AI 入门

要访问 Voyage 嵌入向量：

1. 在 Voyage AI 网站上注册
2. 获取 API 密钥
3. 为方便起见，将 API 密钥设置为环境变量：

```bash
export VOYAGE_API_KEY="<your secret key>"
```

您可以通过使用官方的[`voyageai` Python 包](https://github.com/voyage-ai/voyageai-python)或 HTTP 请求来获取嵌入向量，如下所述。

### Voyage Python 库

可以使用以下命令安装 `voyageai` 包：

```bash
pip install -U voyageai
```

然后，您可以创建一个客户端对象并开始使用它来嵌入您的文本：

```python
import voyageai

vo = voyageai.Client()
# 这将自动使用环境变量 VOYAGE_API_KEY。
# 或者，您可以使用 vo = voyageai.Client(api_key="<your secret key>")

texts = ["Sample text 1", "Sample text 2"]

result = vo.embed(texts, model="voyage-3.5", input_type="document")
print(result.embeddings[0])
print(result.embeddings[1])
```

`result.embeddings` 将是一个包含两个嵌入向量的列表，每个向量包含 1024 个浮点数。运行上述代码后，两个嵌入向量将在屏幕上打印出来：

```
[-0.013131560757756233, 0.019828535616397858, ...]   # "Sample text 1" 的嵌入向量
[-0.0069352793507277966, 0.020878976210951805, ...]  # "Sample text 2" 的嵌入向量
```

在创建嵌入向量时，您可以为 `embed()` 函数指定一些其他参数。

有关 Voyage python 包的更多信息，请参见[Voyage 文档](https://docs.voyageai.com/docs/embeddings#python-api)。

### Voyage HTTP API

您也可以通过请求 Voyage HTTP API 来获取嵌入向量。例如，您可以在终端中通过 `curl` 命令发送 HTTP 请求：

```bash
curl https://api.voyageai.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $VOYAGE_API_KEY" \
  -d '{
    "input": ["Sample text 1", "Sample text 2"],
    "model": "voyage-3.5"
  }'
```

您将得到的响应是一个包含嵌入向量和令牌使用情况的 JSON 对象：

```json
{
  "object": "list",
  "data": [
    {
      "embedding": [-0.013131560757756233, 0.019828535616397858, ...],
      "index": 0
    },
    {
      "embedding": [-0.0069352793507277966, 0.020878976210951805, ...],
      "index": 1
    }
  ],
  "model": "voyage-3.5",
  "usage": {
    "total_tokens": 10
  }
}

```

有关 Voyage HTTP API 的更多信息，请参见[Voyage 文档](https://docs.voyageai.com/reference/embeddings-api)。

### AWS Marketplace

Voyage 嵌入向量在[AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-snt4gb6fd7ljg)上可用。在 AWS 上访问 Voyage 的说明可在[此处](https://docs.voyageai.com/docs/aws-marketplace-model-package?ref=anthropic)找到。

## 快速入门示例

现在我们知道如何获取嵌入向量，让我们看一个简短的示例。

假设我们有一个包含六个文档的小语料库要检索

```python
documents = [
    "The Mediterranean diet emphasizes fish, olive oil, and vegetables, believed to reduce chronic diseases.",
    "Photosynthesis in plants converts light energy into glucose and produces essential oxygen.",
    "20th-century innovations, from radios to smartphones, centered on electronic advancements.",
    "Rivers provide water, irrigation, and habitat for aquatic species, vital for ecosystems.",
    "Apple's conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET.",
    "Shakespeare's works, like 'Hamlet' and 'A Midsummer Night's Dream,' endure in literature."
]

```

我们首先使用 Voyage 将每个文档转换为嵌入向量

```python
import voyageai

vo = voyageai.Client()

# 嵌入文档
doc_embds = vo.embed(
    documents, model="voyage-3.5", input_type="document"
).embeddings
```

嵌入向量将允许我们在向量空间中进行语义搜索/检索。给定一个示例查询，

```python
query = "When is Apple's conference call scheduled?"
```

我们将其转换为嵌入向量，并进行最近邻搜索，根据嵌入空间中的距离找到最相关的文档。

```python
import numpy as np

# 嵌入查询
query_embd = vo.embed(
    [query], model="voyage-3.5", input_type="query"
).embeddings[0]

# 计算相似性
# Voyage 嵌入向量被归一化为长度 1，因此点积
# 和余弦相似性是相同的。
similarities = np.dot(doc_embds, query_embd)

retrieved_id = np.argmax(similarities)
print(documents[retrieved_id])
```

请注意，我们分别使用 `input_type="document"` 和 `input_type="query"` 来嵌入文档和查询。更多规范可以在[这里](/docs/zh-CN/build-with-claude/embeddings#voyage-python-package)找到。

输出将是第 5 个文档，这确实是与查询最相关的：

```
Apple's conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET.
```

如果您正在寻找关于如何使用嵌入向量进行 RAG 的详细教程集，包括向量数据库，请查看我们的[RAG 教程](https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/Pinecone/rag_using_pinecone.ipynb)。

## 常见问题解答

  <section title="为什么 Voyage 嵌入向量具有卓越的质量？">

    嵌入模型依赖强大的神经网络来捕获和压缩语义上下文，类似于生成模型。Voyage 的经验丰富的 AI 研究团队优化嵌入过程的每个组件，包括：
    - 模型架构
    - 数据收集
    - 损失函数
    - 优化器选择

    在他们的[博客](https://blog.voyageai.com/)上了解更多关于 Voyage 技术方法的信息。
  
</section>

  <section title="有哪些嵌入模型可用，我应该使用哪个？">

    对于通用嵌入，我们推荐：
    - `voyage-3-large`：最佳质量
    - `voyage-3.5-lite`：最低延迟和成本
    - `voyage-3.5`：在具有竞争力的价格点上具有卓越检索质量的平衡性能
    
    对于检索，使用 `input_type` 参数来指定文本是查询还是文档类型。

    领域特定模型：

    - 法律任务：`voyage-law-2`
    - 代码和编程文档：`voyage-code-3`
    - 金融相关任务：`voyage-finance-2`
  
</section>

  <section title="我应该使用哪个相似性函数？">

    您可以将 Voyage 嵌入向量与点积相似性、余弦相似性或欧几里得距离一起使用。关于嵌入相似性的解释可以在[这里](https://www.pinecone.io/learn/vector-similarity/)找到。

    Voyage AI 嵌入向量被归一化为长度 1，这意味着：

    - 余弦相似性等同于点积相似性，而后者可以更快地计算。
    - 余弦相似性和欧几里得距离将产生相同的排名。
  
</section>

  <section title="字符、单词和令牌之间的关系是什么？">

    请参见此[页面](https://docs.voyageai.com/docs/tokenization?ref=anthropic)。
  
</section>

  <section title="何时以及如何使用 input_type 参数？">

    对于所有检索任务和用例（例如 RAG），我们建议使用 `input_type` 参数来指定输入文本是查询还是文档。不要省略 `input_type` 或设置 `input_type=None`。指定输入文本是查询还是文档可以为检索创建更好的密集向量表示，这可以导致更好的检索质量。

    使用 `input_type` 参数时，特殊提示会在嵌入之前添加到输入文本前面。具体来说：

    > 📘 **与 `input_type` 相关的提示**
    > 
    > - 对于查询，提示是"Represent the query for retrieving supporting documents: "。
    > - 对于文档，提示是"Represent the document for retrieval: "。
    > - 示例
    >     - 当 `input_type="query"` 时，像"When is Apple's conference call scheduled?"这样的查询将变成"**Represent the query for retrieving supporting documents:** When is Apple's conference call scheduled?"
    >     - 当 `input_type="document"` 时，像"Apple's conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET."这样的查询将变成"**Represent the document for retrieval:** Apple's conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET."

    `voyage-large-2-instruct`，顾名思义，被训练为对添加到输入文本前面的附加指令做出响应。对于分类、聚类或其他[MTEB](https://huggingface.co/mteb)子任务，请使用[这里](https://github.com/voyage-ai/voyage-large-2-instruct)的指令。
  
</section>

  <section title="有哪些量化选项可用？">

    嵌入中的量化将高精度值（如 32 位单精度浮点数）转换为较低精度格式（如 8 位整数或 1 位二进制值），分别将存储、内存和成本减少 4 倍和 32 倍。支持的 Voyage 模型通过使用 `output_dtype` 参数指定输出数据类型来启用量化：

    - `float`：每个返回的嵌入向量是 32 位（4 字节）单精度浮点数的列表。这是默认值，提供最高精度/检索准确性。
    - `int8` 和 `uint8`：每个返回的嵌入向量是 8 位（1 字节）整数的列表，分别范围从 -128 到 127 和 0 到 255。
    - `binary` 和 `ubinary`：每个返回的嵌入向量是 8 位整数的列表，表示位打包的量化单位嵌入值：`binary` 使用 `int8`，`ubinary` 使用 `uint8`。返回的整数列表的长度是嵌入实际维度的 1/8。二进制类型使用偏移二进制方法，您可以在下面的常见问题解答中了解更多信息。

    > **二进制量化示例**
    > 
    > 考虑以下八个嵌入值：-0.03955078、0.006214142、-0.07446289、-0.039001465、0.0046463013、0.00030612946、-0.08496094 和 0.03994751。通过二进制量化，小于或等于零的值将被量化为二进制零，正值被量化为二进制一，产生以下二进制序列：0、1、0、0、1、1、0、1。然后将这八位打包成一个 8 位整数，01001101（最左边的位作为最高有效位）。
    >   - `ubinary`：二进制序列直接转换并表示为无符号整数（`uint8`）77。
    >   - `binary`：二进制序列表示为有符号整数（`int8`）-51，使用偏移二进制方法计算（77 - 128 = -51）。
  
</section>

  <section title="如何截断 Matryoshka 嵌入向量？">

    Matryoshka 学习在单个向量内创建具有从粗到细表示的嵌入向量。支持多个输出维度的 Voyage 模型（如 `voyage-code-3`）生成这样的 Matryoshka 嵌入向量。您可以通过保留维度的前导子集来截断这些向量。例如，以下 Python 代码演示了如何将 1024 维向量截断为 256 维：

    ```python
    import voyageai
    import numpy as np

    def embd_normalize(v: np.ndarray) -> np.ndarray:
        """
        通过将每行除以其欧几里得范数，将 2D numpy 数组的行归一化为单位向量。
        如果任何行的范数为零，则引发 ValueError 以防止除零。
        """
        row_norms = np.linalg.norm(v, axis=1, keepdims=True)
        if np.any(row_norms == 0):
            raise ValueError("Cannot normalize rows with a norm of zero.")
        return v / row_norms


    vo = voyageai.Client()

    # 生成 voyage-code-3 向量，默认情况下是 1024 维浮点数
    embd = vo.embed(['Sample text 1', 'Sample text 2'], model='voyage-code-3').embeddings

    # 设置较短维度
    short_dim = 256

    # 将向量调整大小并归一化为较短维度
    resized_embd = embd_normalize(np.array(embd)[:, :short_dim]).tolist()
    ```
  
</section>

## 定价

访问 Voyage 的[定价页面](https://docs.voyageai.com/docs/pricing?ref=anthropic)获取最新的定价详情。

# 视觉

Claude 3 和 4 系列模型具有新的视觉功能，允许 Claude 理解和分析图像，为多模态交互开辟了令人兴奋的可能性。

---

本指南介绍如何在 Claude 中使用图像，包括最佳实践、代码示例和需要注意的限制。

---

## 如何使用视觉功能

通过以下方式使用 Claude 的视觉功能：

- [claude.ai](https://claude.ai/)。上传图像就像上传文件一样，或者直接将图像拖放到聊天窗口中。
- [Console Workbench](/workbench/)。如果您选择接受图像的模型（仅限 Claude 3 和 4 模型），则在每个用户消息块的右上角会出现添加图像的按钮。
- **API 请求**。请参阅本指南中的示例。

---

## 上传前

### 基础知识和限制

您可以在单个请求中包含多个图像（[claude.ai](https://claude.ai/) 最多 20 个，API 请求最多 100 个）。Claude 在制定响应时将分析所有提供的图像。这对于比较或对比图像很有帮助。

如果您提交的图像大于 8000x8000 像素，将被拒绝。如果您在一个 API 请求中提交超过 20 个图像，此限制为 2000x2000 像素。

<Note>
虽然 API 支持每个请求 100 个图像，但标准端点的[请求大小限制为 32MB](/docs/zh-CN/api/overview#request-size-limits)。
</Note>

### 评估图像大小

为了获得最佳性能，我们建议在上传前调整过大的图像大小。如果您的图像长边超过 1568 像素，或您的图像超过约 1,600 个令牌，它将首先按比例缩小，保持宽高比，直到符合大小限制。

如果您的输入图像过大需要调整大小，这将增加[首个令牌的时间](/docs/zh-CN/about-claude/glossary)的延迟，而不会为您提供任何额外的模型性能。任何边小于 200 像素的非常小的图像可能会降低性能。

<Tip>
  为了改进[首个令牌的时间](/docs/zh-CN/about-claude/glossary)，我们建议
  将图像调整为不超过 1.15 兆像素（且在两个维度上都不超过 1568 像素）。
</Tip>

以下是我们的 API 接受的最大图像大小表，这些图像不会因常见宽高比而被调整大小。使用 Claude Sonnet 3.7 模型，这些图像使用约 1,600 个令牌，每 1K 张图像约花费 $4.80。

| 宽高比 | 图像大小   |
| --- | --- |
| 1:1          | 1092x1092 px |
| 3:4          | 951x1268 px  |
| 2:3          | 896x1344 px  |
| 9:16         | 819x1456 px  |
| 1:2          | 784x1568 px  |

### 计算图像成本

您在请求中包含的每个图像都计入您的令牌使用量。要计算近似成本，请将近似图像令牌数乘以您使用的[模型的每令牌价格](https://claude.com/pricing)。

如果您的图像不需要调整大小，您可以通过此算法估计使用的令牌数：`tokens = (width px * height px)/750`

以下是基于 Claude Sonnet 3.7 每百万输入令牌 $3 的价格，在我们 API 的大小限制内不同图像大小的近似令牌化和成本示例：

| 图像大小                    | 令牌数 | 每张图像成本 | 每 1K 张图像成本 |
| --- | --- | --- | --- |
| 200x200 px(0.04 兆像素)   | \~54         | \~$0.00016   | \~$0.16          |
| 1000x1000 px(1 兆像素)     | \~1334       | \~$0.004     | \~$4.00          |
| 1092x1092 px(1.19 兆像素) | \~1590       | \~$0.0048    | \~$4.80          |

### 确保图像质量

向 Claude 提供图像时，请记住以下几点以获得最佳结果：

- **图像格式**：使用支持的图像格式：JPEG、PNG、GIF 或 WebP。
- **图像清晰度**：确保图像清晰，不会太模糊或像素化。
- **文本**：如果图像包含重要文本，请确保其清晰易读且不会太小。避免仅为了放大文本而裁剪关键视觉背景。

---

## 提示示例

许多[适用于与 Claude 进行基于文本交互的提示技术](/docs/zh-CN/build-with-claude/prompt-engineering/overview)也可以应用于基于图像的提示。

这些示例演示了涉及图像的最佳实践提示结构。

<Tip>
  就像文档查询放置一样，Claude 在图像位于文本之前时效果最好。
  放置在文本之后或与文本交错的图像仍然会表现良好，但如果您的用例允许，
  我们建议采用先图像后文本的结构。
</Tip>

### 关于提示示例

以下示例演示了如何使用各种编程语言和方法使用 Claude 的视觉功能。您可以通过三种方式向 Claude 提供图像：

1. 作为 `image` 内容块中的 base64 编码图像
2. 作为托管在线图像的 URL 引用
3. 使用文件 API（上传一次，多次使用）

base64 示例提示使用这些变量：

<CodeGroup>
```bash Shell
    # 对于基于 URL 的图像，您可以直接在 JSON 请求中使用 URL
    
    # 对于 base64 编码的图像，您需要先对图像进行编码
    # 如何在 bash 中将图像编码为 base64 的示例：
    BASE64_IMAGE_DATA=$(curl -s "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg" | base64)
    
    # 编码的数据现在可以在您的 API 调用中使用
```

```python Python
import base64
import httpx

# 对于 base64 编码的图像
image1_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
image1_media_type = "image/jpeg"
image1_data = base64.standard_b64encode(httpx.get(image1_url).content).decode("utf-8")

image2_url = "https://upload.wikimedia.org/wikipedia/commons/b/b5/Iridescent.green.sweat.bee1.jpg"
image2_media_type = "image/jpeg"
image2_data = base64.standard_b64encode(httpx.get(image2_url).content).decode("utf-8")

# 对于基于 URL 的图像，您可以直接在请求中使用 URL
```

```typescript TypeScript
import axios from 'axios';

// 对于 base64 编码的图像
async function getBase64Image(url: string): Promise<string> {
  const response = await axios.get(url, { responseType: 'arraybuffer' });
  return Buffer.from(response.data, 'binary').toString('base64');
}

// 用法
async function prepareImages() {
  const imageData = await getBase64Image('https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg');
  // 现在您可以在 API 调用中使用 imageData
}

// 对于基于 URL 的图像，您可以直接在请求中使用 URL
```

```java Java
import java.io.IOException;
import java.util.Base64;
import java.io.InputStream;
import java.net.URL;

public class ImageHandlingExample {

    public static void main(String[] args) throws IOException, InterruptedException {
        // 对于 base64 编码的图像
        String image1Url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg";
        String image1MediaType = "image/jpeg";
        String image1Data = downloadAndEncodeImage(image1Url);

        String image2Url = "https://upload.wikimedia.org/wikipedia/commons/b/b5/Iridescent.green.sweat.bee1.jpg";
        String image2MediaType = "image/jpeg";
        String image2Data = downloadAndEncodeImage(image2Url);

        // 对于基于 URL 的图像，您可以直接在请求中使用 URL
    }

    private static String downloadAndEncodeImage(String imageUrl) throws IOException {
        try (InputStream inputStream = new URL(imageUrl).openStream()) {
            return Base64.getEncoder().encodeToString(inputStream.readAllBytes());
        }
    }

}
```
</CodeGroup>

以下是如何使用 base64 编码的图像和 URL 引用在消息 API 请求中包含图像的示例：

### Base64 编码图像示例

<CodeGroup>
    ```bash Shell
    curl https://api.anthropic.com/v1/messages \
      -H "x-api-key: $ANTHROPIC_API_KEY" \
      -H "anthropic-version: 2023-06-01" \
      -H "content-type: application/json" \
      -d '{
        "model": "claude-sonnet-4-5",
        "max_tokens": 1024,
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "image",
                "source": {
                  "type": "base64",
                  "media_type": "image/jpeg",
                  "data": "'"$BASE64_IMAGE_DATA"'"
                }
              },
              {
                "type": "text",
                "text": "Describe this image."
              }
            ]
          }
        ]
      }'
    ```
    ```python Python
    import anthropic

    client = anthropic.Anthropic()
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image1_media_type,
                            "data": image1_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Describe this image."
                    }
                ],
            }
        ],
    )
    print(message)
    ```
    ```typescript TypeScript
    import Anthropic from '@anthropic-ai/sdk';

    const anthropic = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
    });

    async function main() {
      const message = await anthropic.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 1024,
        messages: [
          {
            role: "user",
            content: [
              {
                type: "image",
                source: {
                  type: "base64",
                  media_type: "image/jpeg",
                  data: imageData, // Base64-encoded image data as string
                }
              },
              {
                type: "text",
                text: "Describe this image."
              }
            ]
          }
        ]
      });
      
      console.log(message);
    }

    main();
    ```

    ```java Java
    import java.io.IOException;
    import java.util.List;

    import com.anthropic.client.AnthropicClient;
    import com.anthropic.client.okhttp.AnthropicOkHttpClient;
    import com.anthropic.models.messages.*;

    public class VisionExample {
        public static void main(String[] args) throws IOException, InterruptedException {
            AnthropicClient client = AnthropicOkHttpClient.fromEnv();
            String imageData = ""; // // Base64-encoded image data as string

            List<ContentBlockParam> contentBlockParams = List.of(
                    ContentBlockParam.ofImage(
                            ImageBlockParam.builder()
 .source(Base64ImageSource.builder()
 .data(imageData)
 .build())
 .build()
                    ),
                    ContentBlockParam.ofText(TextBlockParam.builder()
                            .text("Describe this image.")
                            .build())
            );
            Message message = client.messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_3_7_SONNET_LATEST)
                            .maxTokens(1024)
                            .addUserMessageOfBlockParams(contentBlockParams)
                            .build()
            );

            System.out.println(message);
        }
    }
    ```
</CodeGroup>

### 基于 URL 的图像示例

<CodeGroup>
    ```bash Shell
    curl https://api.anthropic.com/v1/messages \
      -H "x-api-key: $ANTHROPIC_API_KEY" \
      -H "anthropic-version: 2023-06-01" \
      -H "content-type: application/json" \
      -d '{
        "model": "claude-sonnet-4-5",
        "max_tokens": 1024,
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "image",
                "source": {
                  "type": "url",
                  "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
                }
              },
              {
                "type": "text",
                "text": "Describe this image."
              }
            ]
          }
        ]
      }'
    ```
    ```python Python
    import anthropic

    client = anthropic.Anthropic()
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "Describe this image."
                    }
                ],
            }
        ],
    )
    print(message)
    ```
    ```typescript TypeScript
    import Anthropic from '@anthropic-ai/sdk';

    const anthropic = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
    });

    async function main() {
      const message = await anthropic.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 1024,
        messages: [
          {
            role: "user",
            content: [
              {
                type: "image",
                source: {
                  type: "url",
                  url: "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
                }
              },
              {
                type: "text",
                text: "Describe this image."
              }
            ]
          }
        ]
      });
      
      console.log(message);
    }

    main();
    ```
    ```java Java
    import java.io.IOException;
    import java.util.List;

    import com.anthropic.client.AnthropicClient;
    import com.anthropic.client.okhttp.AnthropicOkHttpClient;
    import com.anthropic.models.messages.*;

    public class VisionExample {

        public static void main(String[] args) throws IOException, InterruptedException {
            AnthropicClient client = AnthropicOkHttpClient.fromEnv();

            List<ContentBlockParam> contentBlockParams = List.of(
                    ContentBlockParam.ofImage(
                            ImageBlockParam.builder()
 .source(UrlImageSource.builder()
 .url("https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg")
 .build())
 .build()
                    ),
                    ContentBlockParam.ofText(TextBlockParam.builder()
                            .text("Describe this image.")
                            .build())
            );
            Message message = client.messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_3_7_SONNET_LATEST)
                            .maxTokens(1024)
                            .addUserMessageOfBlockParams(contentBlockParams)
                            .build()
            );
            System.out.println(message);
        }
    }
    ```
</CodeGroup>

### 文件 API 图像示例

对于您将重复使用的图像或想要避免编码开销的情况，请使用[文件 API](/docs/zh-CN/build-with-claude/files)：

<CodeGroup>
```bash Shell
# 首先，将您的图像上传到文件 API
curl -X POST https://api.anthropic.com/v1/files \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14" \
  -F "file=@image.jpg"

# 然后在您的消息中使用返回的 file_id
curl https://api.anthropic.com/v1/messages \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14" \
  -H "content-type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "image",
            "source": {
              "type": "file",
              "file_id": "file_abc123"
            }
          },
          {
            "type": "text",
            "text": "Describe this image."
          }
        ]
      }
    ]
  }'
```

```python Python
import anthropic

client = anthropic.Anthropic()

# 上传图像文件
with open("image.jpg", "rb") as f:
    file_upload = client.beta.files.upload(file=("image.jpg", f, "image/jpeg"))

# 在消息中使用上传的文件
message = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    betas=["files-api-2025-04-14"],
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "file",
                        "file_id": file_upload.id
                    }
                },
                {
                    "type": "text",
                    "text": "Describe this image."
                }
            ]
        }
    ],
)

print(message.content)
```

```typescript TypeScript
import { Anthropic, toFile } from '@anthropic-ai/sdk';
import fs from 'fs';

const anthropic = new Anthropic();

async function main() {
  // 上传图像文件
  const fileUpload = await anthropic.beta.files.upload({
    file: toFile(fs.createReadStream('image.jpg'), undefined, { type: "image/jpeg" })
  }, {
    betas: ['files-api-2025-04-14']
  });

  // 在消息中使用上传的文件
  const response = await anthropic.beta.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    betas: ['files-api-2025-04-14'],
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: {
              type: 'file',
              file_id: fileUpload.id
            }
          },
          {
            type: 'text',
            text: 'Describe this image.'
          }
        ]
      }
    ]
  });

  console.log(response);
}

main();
```

```java Java
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.File;
import com.anthropic.models.files.FileUploadParams;
import com.anthropic.models.messages.*;

public class ImageFilesExample {
    public static void main(String[] args) throws IOException {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 上传图像文件
        File file = client.beta().files().upload(FileUploadParams.builder()
                .file(Files.newInputStream(Path.of("image.jpg")))
                .build());

        // 在消息中使用上传的文件
        ImageBlockParam imageParam = ImageBlockParam.builder()
                .fileSource(file.id())
                .build();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_3_7_SONNET_LATEST)
                .maxTokens(1024)
                .addUserMessageOfBlockParams(
                        List.of(
 ContentBlockParam.ofImage(imageParam),
 ContentBlockParam.ofText(
 TextBlockParam.builder()
 .text("Describe this image.")
 .build()
 )
                        )
                )
                .build();

        Message message = client.messages().create(params);
        System.out.println(message.content());
    }
}
```
</CodeGroup>

请参阅[消息 API 示例](/docs/zh-CN/api/messages)了解更多示例代码和参数详情。

<section title="示例：一张图像">

最好在提示中将图像放在关于它们的问题或使用它们的任务说明之前。

要求 Claude 描述一张图像。

| 角色 | 内容                        |
| ---- | --- |
| 用户 | \[图像\] 描述这张图像。 |

以下是使用 Claude Sonnet 3.7 模型的相应 API 调用。

<Tabs>
  <Tab title="使用 Base64">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image1_media_type,
                            "data": image1_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Describe this image."
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
  <Tab title="使用 URL">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "Describe this image."
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
</Tabs>

</section>
<section title="示例：多张图像">

在有多张图像的情况下，使用 `Image 1:` 和 `Image 2:` 等标签介绍每张图像。图像之间或图像与提示之间不需要换行。

要求 Claude 描述多张图像之间的差异。
| 角色 | 内容 |
| ---- | --- |
| 用户 | 图像 1：\[图像 1\] 图像 2：\[图像 2\] 这些图像有什么不同？ |

以下是使用 Claude Sonnet 3.7 模型的相应 API 调用。

<Tabs>
  <Tab title="使用 Base64">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Image 1:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image1_media_type,
                            "data": image1_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Image 2:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image2_media_type,
                            "data": image2_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "How are these images different?"
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
  <Tab title="使用 URL">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Image 1:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "Image 2:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/b/b5/Iridescent.green.sweat.bee1.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "How are these images different?"
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
</Tabs>

</section>
<section title="示例：多张图像和系统提示">

要求 Claude 描述多张图像之间的差异，同时给它一个系统提示来说明如何响应。

| 内容 | |
| --- | --- |
| 系统  | 仅用西班牙语回应。 |
| 用户    | 图像 1：\[图像 1\] 图像 2：\[图像 2\] 这些图像有什么不同？ |

以下是使用 Claude Sonnet 3.7 模型的相应 API 调用。

<Tabs>
  <Tab title="使用 Base64">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        system="Respond only in Spanish.",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Image 1:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image1_media_type,
                            "data": image1_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Image 2:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image2_media_type,
                            "data": image2_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "How are these images different?"
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
  <Tab title="使用 URL">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        system="Respond only in Spanish.",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Image 1:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "Image 2:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/b/b5/Iridescent.green.sweat.bee1.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "How are these images different?"
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
</Tabs>

</section>
<section title="示例：跨两个对话轮次的四张图像">

Claude 的视觉功能在混合图像和文本的多模态对话中表现出色。您可以与 Claude 进行扩展的来回交流，在任何时刻添加新图像或后续问题。这为迭代图像分析、比较或将视觉与其他知识相结合的强大工作流程提供了支持。

要求 Claude 对比两张图像，然后提出一个后续问题，将第一张图像与两张新图像进行比较。
| 角色 | 内容 |
| --- | --- |
| 用户 | 图像 1：\[图像 1\] 图像 2：\[图像 2\] 这些图像有什么不同？ |
| 助手 | \[Claude 的响应\] |
| 用户 | 图像 1：\[图像 3\] 图像 2：\[图像 4\] 这些图像与前两张相似吗？ |
| 助手 | \[Claude 的响应\] |

使用 API 时，只需将新图像插入到标准[多轮对话](/docs/zh-CN/api/messages-examples#multiple-conversational-turns)结构中 `user` 角色的消息数组中。

</section>

---

## 限制

虽然 Claude 的图像理解功能是最先进的，但需要注意一些限制：

- **人物识别**：Claude [不能用于](/docs/zh-CN/legal/aup)识别（即命名）图像中的人物，并将拒绝这样做。
- **准确性**：Claude 在解释低质量、旋转或非常小的图像（小于 200 像素）时可能会产生幻觉或犯错误。
- **空间推理**：Claude 的空间推理能力有限。它可能在需要精确定位或布局的任务中遇到困难，例如读取模拟时钟面或描述国际象棋棋子的确切位置。
- **计数**：Claude 可以给出图像中对象的近似计数，但可能不总是精确准确，特别是对于大量小对象。
- **AI 生成的图像**：Claude 不知道图像是否是 AI 生成的，如果被问及可能会不正确。不要依赖它来检测虚假或合成图像。
- **不当内容**：Claude 不会处理违反我们[可接受使用政策](https://www.anthropic.com/legal/aup)的不当或露骨图像。
- **医疗保健应用**：虽然 Claude 可以分析一般医学图像，但它不是为解释复杂的诊断扫描（如 CT 或 MRI）而设计的。Claude 的输出不应被视为专业医疗建议或诊断的替代品。

始终仔细审查和验证 Claude 的图像解释，特别是对于高风险用例。不要在没有人工监督的情况下使用 Claude 执行需要完美精度或敏感图像分析的任务。

---

## 常见问题

  <section title="Claude 支持哪些图像文件类型？">

    Claude 目前支持 JPEG、PNG、GIF 和 WebP 图像格式，具体为：
    - `image/jpeg`
    - `image/png`
    - `image/gif`
    - `image/webp`
  
</section>

{" "}

<section title="Claude 可以读取图像 URL 吗？">

  是的，Claude 现在可以通过 API 中的 URL 图像源块处理来自 URL 的图像。
  只需在 API 请求中使用"url"源类型而不是"base64"。
  示例：
  ```json
  {
    "type": "image",
    "source": {
      "type": "url",
      "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
    }
  }
  ```

</section>

  <section title="我可以上传的图像文件大小有限制吗？">

    是的，有限制：
    - API：每张图像最大 5MB
    - claude.ai：每张图像最大 10MB

    超过这些限制的图像将被拒绝，使用我们的 API 时会返回错误。

  
</section>

  <section title="我可以在一个请求中包含多少张图像？">

    图像限制为：
    - 消息 API：每个请求最多 100 张图像
    - claude.ai：每轮最多 20 张图像

    超过这些限制的请求将被拒绝并返回错误。

  
</section>

{" "}

<section title="Claude 读取图像元数据吗？">

  不，Claude 不会解析或接收从传递给它的图像中获取的任何元数据。

</section>

{" "}

<section title="我可以删除上传的图像吗？">

  不可以。图像上传是临时的，不会在 API 请求期间之外存储。上传的图像在处理后会自动删除。

</section>

{" "}

<section title="在哪里可以找到有关图像上传数据隐私的详情？">

  请参阅我们的隐私政策页面，了解我们如何处理上传的图像和其他数据的信息。我们不使用上传的图像来训练我们的模型。

</section>

  <section title="如果 Claude 的图像解释似乎有误怎么办？">

    如果 Claude 的图像解释似乎不正确：
    1. 确保图像清晰、高质量且方向正确。
    2. 尝试提示工程技术来改进结果。
    3. 如果问题仍然存在，在 claude.ai 中标记输出（竖起大拇指/竖起大拇指向下）或联系我们的支持团队。

    您的反馈帮助我们改进！

  
</section>

  <section title="Claude 可以生成或编辑图像吗？">

    不，Claude 仅是图像理解模型。它可以解释和分析图像，但不能生成、生成、编辑、操纵或创建图像。
  
</section>

---

## 深入了解视觉

准备好开始使用 Claude 构建图像了吗？以下是一些有用的资源：

- [多模态食谱](https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal)：此食谱包含有关[开始使用图像](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/getting%5Fstarted%5Fwith%5Fvision.ipynb)和[视觉最佳实践技术](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/best%5Fpractices%5Ffor%5Fvision.ipynb)的提示，以确保图像的最高质量性能。了解如何有效地使用图像提示 Claude 来执行任务，例如[解释和分析图表](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/reading%5Fcharts%5Fgraphs%5Fpowerpoints.ipynb)或[从表单中提取内容](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/how%5Fto%5Ftranscribe%5Ftext.ipynb)。
- [API 参考](/docs/zh-CN/api/messages)：访问我们的消息 API 文档，包括涉及图像的示例 [API 调用](/docs/zh-CN/build-with-claude/working-with-messages#vision)。

如果您有任何其他问题，请随时联系我们的[支持团队](https://support.claude.com/)。您也可以加入我们的[开发者社区](https://www.anthropic.com/discord)与其他创作者联系并获得 Anthropic 专家的帮助。


# 视觉

Claude 3 和 4 系列模型具有新的视觉功能，允许 Claude 理解和分析图像，为多模态交互开辟了令人兴奋的可能性。

---

本指南介绍如何在 Claude 中使用图像，包括最佳实践、代码示例和需要注意的限制。

---

## 如何使用视觉功能

通过以下方式使用 Claude 的视觉功能：

- [claude.ai](https://claude.ai/)。上传图像就像上传文件一样，或者直接将图像拖放到聊天窗口中。
- [Console Workbench](/workbench/)。如果您选择接受图像的模型（仅限 Claude 3 和 4 模型），则在每个用户消息块的右上角会出现添加图像的按钮。
- **API 请求**。请参阅本指南中的示例。

---

## 上传前

### 基础知识和限制

您可以在单个请求中包含多个图像（[claude.ai](https://claude.ai/) 最多 20 个，API 请求最多 100 个）。Claude 在制定响应时将分析所有提供的图像。这对于比较或对比图像很有帮助。

如果您提交的图像大于 8000x8000 像素，将被拒绝。如果您在一个 API 请求中提交超过 20 个图像，此限制为 2000x2000 像素。

<Note>
虽然 API 支持每个请求 100 个图像，但标准端点的[请求大小限制为 32MB](/docs/zh-CN/api/overview#request-size-limits)。
</Note>

### 评估图像大小

为了获得最佳性能，我们建议在上传前调整过大的图像大小。如果您的图像长边超过 1568 像素，或您的图像超过约 1,600 个令牌，它将首先按比例缩小，保持宽高比，直到符合大小限制。

如果您的输入图像过大需要调整大小，这将增加[首个令牌的时间](/docs/zh-CN/about-claude/glossary)的延迟，而不会为您提供任何额外的模型性能。任何边小于 200 像素的非常小的图像可能会降低性能。

<Tip>
  为了改进[首个令牌的时间](/docs/zh-CN/about-claude/glossary)，我们建议
  将图像调整为不超过 1.15 兆像素（且在两个维度上都不超过 1568 像素）。
</Tip>

以下是我们的 API 接受的最大图像大小表，这些图像不会因常见宽高比而被调整大小。使用 Claude Sonnet 3.7 模型，这些图像使用约 1,600 个令牌，每 1K 张图像约花费 $4.80。

| 宽高比 | 图像大小   |
| --- | --- |
| 1:1          | 1092x1092 px |
| 3:4          | 951x1268 px  |
| 2:3          | 896x1344 px  |
| 9:16         | 819x1456 px  |
| 1:2          | 784x1568 px  |

### 计算图像成本

您在请求中包含的每个图像都计入您的令牌使用量。要计算近似成本，请将近似图像令牌数乘以您使用的[模型的每令牌价格](https://claude.com/pricing)。

如果您的图像不需要调整大小，您可以通过此算法估计使用的令牌数：`tokens = (width px * height px)/750`

以下是基于 Claude Sonnet 3.7 每百万输入令牌 $3 的价格，在我们 API 的大小限制内不同图像大小的近似令牌化和成本示例：

| 图像大小                    | 令牌数 | 每张图像成本 | 每 1K 张图像成本 |
| --- | --- | --- | --- |
| 200x200 px(0.04 兆像素)   | \~54         | \~$0.00016   | \~$0.16          |
| 1000x1000 px(1 兆像素)     | \~1334       | \~$0.004     | \~$4.00          |
| 1092x1092 px(1.19 兆像素) | \~1590       | \~$0.0048    | \~$4.80          |

### 确保图像质量

向 Claude 提供图像时，请记住以下几点以获得最佳结果：

- **图像格式**：使用支持的图像格式：JPEG、PNG、GIF 或 WebP。
- **图像清晰度**：确保图像清晰，不会太模糊或像素化。
- **文本**：如果图像包含重要文本，请确保其清晰易读且不会太小。避免仅为了放大文本而裁剪关键视觉背景。

---

## 提示示例

许多[适用于与 Claude 进行基于文本交互的提示技术](/docs/zh-CN/build-with-claude/prompt-engineering/overview)也可以应用于基于图像的提示。

这些示例演示了涉及图像的最佳实践提示结构。

<Tip>
  就像文档查询放置一样，Claude 在图像位于文本之前时效果最好。
  放置在文本之后或与文本交错的图像仍然会表现良好，但如果您的用例允许，
  我们建议采用先图像后文本的结构。
</Tip>

### 关于提示示例

以下示例演示了如何使用各种编程语言和方法使用 Claude 的视觉功能。您可以通过三种方式向 Claude 提供图像：

1. 作为 `image` 内容块中的 base64 编码图像
2. 作为托管在线图像的 URL 引用
3. 使用文件 API（上传一次，多次使用）

base64 示例提示使用这些变量：

<CodeGroup>
```bash Shell
    # 对于基于 URL 的图像，您可以直接在 JSON 请求中使用 URL
    
    # 对于 base64 编码的图像，您需要先对图像进行编码
    # 如何在 bash 中将图像编码为 base64 的示例：
    BASE64_IMAGE_DATA=$(curl -s "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg" | base64)
    
    # 编码的数据现在可以在您的 API 调用中使用
```

```python Python
import base64
import httpx

# 对于 base64 编码的图像
image1_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
image1_media_type = "image/jpeg"
image1_data = base64.standard_b64encode(httpx.get(image1_url).content).decode("utf-8")

image2_url = "https://upload.wikimedia.org/wikipedia/commons/b/b5/Iridescent.green.sweat.bee1.jpg"
image2_media_type = "image/jpeg"
image2_data = base64.standard_b64encode(httpx.get(image2_url).content).decode("utf-8")

# 对于基于 URL 的图像，您可以直接在请求中使用 URL
```

```typescript TypeScript
import axios from 'axios';

// 对于 base64 编码的图像
async function getBase64Image(url: string): Promise<string> {
  const response = await axios.get(url, { responseType: 'arraybuffer' });
  return Buffer.from(response.data, 'binary').toString('base64');
}

// 用法
async function prepareImages() {
  const imageData = await getBase64Image('https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg');
  // 现在您可以在 API 调用中使用 imageData
}

// 对于基于 URL 的图像，您可以直接在请求中使用 URL
```

```java Java
import java.io.IOException;
import java.util.Base64;
import java.io.InputStream;
import java.net.URL;

public class ImageHandlingExample {

    public static void main(String[] args) throws IOException, InterruptedException {
        // 对于 base64 编码的图像
        String image1Url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg";
        String image1MediaType = "image/jpeg";
        String image1Data = downloadAndEncodeImage(image1Url);

        String image2Url = "https://upload.wikimedia.org/wikipedia/commons/b/b5/Iridescent.green.sweat.bee1.jpg";
        String image2MediaType = "image/jpeg";
        String image2Data = downloadAndEncodeImage(image2Url);

        // 对于基于 URL 的图像，您可以直接在请求中使用 URL
    }

    private static String downloadAndEncodeImage(String imageUrl) throws IOException {
        try (InputStream inputStream = new URL(imageUrl).openStream()) {
            return Base64.getEncoder().encodeToString(inputStream.readAllBytes());
        }
    }

}
```
</CodeGroup>

以下是如何使用 base64 编码的图像和 URL 引用在消息 API 请求中包含图像的示例：

### Base64 编码图像示例

<CodeGroup>
    ```bash Shell
    curl https://api.anthropic.com/v1/messages \
      -H "x-api-key: $ANTHROPIC_API_KEY" \
      -H "anthropic-version: 2023-06-01" \
      -H "content-type: application/json" \
      -d '{
        "model": "claude-sonnet-4-5",
        "max_tokens": 1024,
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "image",
                "source": {
                  "type": "base64",
                  "media_type": "image/jpeg",
                  "data": "'"$BASE64_IMAGE_DATA"'"
                }
              },
              {
                "type": "text",
                "text": "Describe this image."
              }
            ]
          }
        ]
      }'
    ```
    ```python Python
    import anthropic

    client = anthropic.Anthropic()
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image1_media_type,
                            "data": image1_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Describe this image."
                    }
                ],
            }
        ],
    )
    print(message)
    ```
    ```typescript TypeScript
    import Anthropic from '@anthropic-ai/sdk';

    const anthropic = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
    });

    async function main() {
      const message = await anthropic.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 1024,
        messages: [
          {
            role: "user",
            content: [
              {
                type: "image",
                source: {
                  type: "base64",
                  media_type: "image/jpeg",
                  data: imageData, // Base64-encoded image data as string
                }
              },
              {
                type: "text",
                text: "Describe this image."
              }
            ]
          }
        ]
      });
      
      console.log(message);
    }

    main();
    ```

    ```java Java
    import java.io.IOException;
    import java.util.List;

    import com.anthropic.client.AnthropicClient;
    import com.anthropic.client.okhttp.AnthropicOkHttpClient;
    import com.anthropic.models.messages.*;

    public class VisionExample {
        public static void main(String[] args) throws IOException, InterruptedException {
            AnthropicClient client = AnthropicOkHttpClient.fromEnv();
            String imageData = ""; // // Base64-encoded image data as string

            List<ContentBlockParam> contentBlockParams = List.of(
                    ContentBlockParam.ofImage(
                            ImageBlockParam.builder()
 .source(Base64ImageSource.builder()
 .data(imageData)
 .build())
 .build()
                    ),
                    ContentBlockParam.ofText(TextBlockParam.builder()
                            .text("Describe this image.")
                            .build())
            );
            Message message = client.messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_3_7_SONNET_LATEST)
                            .maxTokens(1024)
                            .addUserMessageOfBlockParams(contentBlockParams)
                            .build()
            );

            System.out.println(message);
        }
    }
    ```
</CodeGroup>

### 基于 URL 的图像示例

<CodeGroup>
    ```bash Shell
    curl https://api.anthropic.com/v1/messages \
      -H "x-api-key: $ANTHROPIC_API_KEY" \
      -H "anthropic-version: 2023-06-01" \
      -H "content-type: application/json" \
      -d '{
        "model": "claude-sonnet-4-5",
        "max_tokens": 1024,
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "image",
                "source": {
                  "type": "url",
                  "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
                }
              },
              {
                "type": "text",
                "text": "Describe this image."
              }
            ]
          }
        ]
      }'
    ```
    ```python Python
    import anthropic

    client = anthropic.Anthropic()
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "Describe this image."
                    }
                ],
            }
        ],
    )
    print(message)
    ```
    ```typescript TypeScript
    import Anthropic from '@anthropic-ai/sdk';

    const anthropic = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
    });

    async function main() {
      const message = await anthropic.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 1024,
        messages: [
          {
            role: "user",
            content: [
              {
                type: "image",
                source: {
                  type: "url",
                  url: "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
                }
              },
              {
                type: "text",
                text: "Describe this image."
              }
            ]
          }
        ]
      });
      
      console.log(message);
    }

    main();
    ```
    ```java Java
    import java.io.IOException;
    import java.util.List;

    import com.anthropic.client.AnthropicClient;
    import com.anthropic.client.okhttp.AnthropicOkHttpClient;
    import com.anthropic.models.messages.*;

    public class VisionExample {

        public static void main(String[] args) throws IOException, InterruptedException {
            AnthropicClient client = AnthropicOkHttpClient.fromEnv();

            List<ContentBlockParam> contentBlockParams = List.of(
                    ContentBlockParam.ofImage(
                            ImageBlockParam.builder()
 .source(UrlImageSource.builder()
 .url("https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg")
 .build())
 .build()
                    ),
                    ContentBlockParam.ofText(TextBlockParam.builder()
                            .text("Describe this image.")
                            .build())
            );
            Message message = client.messages().create(
                    MessageCreateParams.builder()
                            .model(Model.CLAUDE_3_7_SONNET_LATEST)
                            .maxTokens(1024)
                            .addUserMessageOfBlockParams(contentBlockParams)
                            .build()
            );
            System.out.println(message);
        }
    }
    ```
</CodeGroup>

### 文件 API 图像示例

对于您将重复使用的图像或想要避免编码开销的情况，请使用[文件 API](/docs/zh-CN/build-with-claude/files)：

<CodeGroup>
```bash Shell
# 首先，将您的图像上传到文件 API
curl -X POST https://api.anthropic.com/v1/files \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14" \
  -F "file=@image.jpg"

# 然后在您的消息中使用返回的 file_id
curl https://api.anthropic.com/v1/messages \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14" \
  -H "content-type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "image",
            "source": {
              "type": "file",
              "file_id": "file_abc123"
            }
          },
          {
            "type": "text",
            "text": "Describe this image."
          }
        ]
      }
    ]
  }'
```

```python Python
import anthropic

client = anthropic.Anthropic()

# 上传图像文件
with open("image.jpg", "rb") as f:
    file_upload = client.beta.files.upload(file=("image.jpg", f, "image/jpeg"))

# 在消息中使用上传的文件
message = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    betas=["files-api-2025-04-14"],
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "file",
                        "file_id": file_upload.id
                    }
                },
                {
                    "type": "text",
                    "text": "Describe this image."
                }
            ]
        }
    ],
)

print(message.content)
```

```typescript TypeScript
import { Anthropic, toFile } from '@anthropic-ai/sdk';
import fs from 'fs';

const anthropic = new Anthropic();

async function main() {
  // 上传图像文件
  const fileUpload = await anthropic.beta.files.upload({
    file: toFile(fs.createReadStream('image.jpg'), undefined, { type: "image/jpeg" })
  }, {
    betas: ['files-api-2025-04-14']
  });

  // 在消息中使用上传的文件
  const response = await anthropic.beta.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    betas: ['files-api-2025-04-14'],
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: {
              type: 'file',
              file_id: fileUpload.id
            }
          },
          {
            type: 'text',
            text: 'Describe this image.'
          }
        ]
      }
    ]
  });

  console.log(response);
}

main();
```

```java Java
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;

import com.anthropic.client.AnthropicClient;
import com.anthropic.client.okhttp.AnthropicOkHttpClient;
import com.anthropic.models.File;
import com.anthropic.models.files.FileUploadParams;
import com.anthropic.models.messages.*;

public class ImageFilesExample {
    public static void main(String[] args) throws IOException {
        AnthropicClient client = AnthropicOkHttpClient.fromEnv();

        // 上传图像文件
        File file = client.beta().files().upload(FileUploadParams.builder()
                .file(Files.newInputStream(Path.of("image.jpg")))
                .build());

        // 在消息中使用上传的文件
        ImageBlockParam imageParam = ImageBlockParam.builder()
                .fileSource(file.id())
                .build();

        MessageCreateParams params = MessageCreateParams.builder()
                .model(Model.CLAUDE_3_7_SONNET_LATEST)
                .maxTokens(1024)
                .addUserMessageOfBlockParams(
                        List.of(
 ContentBlockParam.ofImage(imageParam),
 ContentBlockParam.ofText(
 TextBlockParam.builder()
 .text("Describe this image.")
 .build()
 )
                        )
                )
                .build();

        Message message = client.messages().create(params);
        System.out.println(message.content());
    }
}
```
</CodeGroup>

请参阅[消息 API 示例](/docs/zh-CN/api/messages)了解更多示例代码和参数详情。

<section title="示例：一张图像">

最好在提示中将图像放在关于它们的问题或使用它们的任务说明之前。

要求 Claude 描述一张图像。

| 角色 | 内容                        |
| ---- | --- |
| 用户 | \[图像\] 描述这张图像。 |

以下是使用 Claude Sonnet 3.7 模型的相应 API 调用。

<Tabs>
  <Tab title="使用 Base64">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image1_media_type,
                            "data": image1_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Describe this image."
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
  <Tab title="使用 URL">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "Describe this image."
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
</Tabs>

</section>
<section title="示例：多张图像">

在有多张图像的情况下，使用 `Image 1:` 和 `Image 2:` 等标签介绍每张图像。图像之间或图像与提示之间不需要换行。

要求 Claude 描述多张图像之间的差异。
| 角色 | 内容 |
| ---- | --- |
| 用户 | 图像 1：\[图像 1\] 图像 2：\[图像 2\] 这些图像有什么不同？ |

以下是使用 Claude Sonnet 3.7 模型的相应 API 调用。

<Tabs>
  <Tab title="使用 Base64">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Image 1:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image1_media_type,
                            "data": image1_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Image 2:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image2_media_type,
                            "data": image2_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "How are these images different?"
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
  <Tab title="使用 URL">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Image 1:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "Image 2:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/b/b5/Iridescent.green.sweat.bee1.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "How are these images different?"
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
</Tabs>

</section>
<section title="示例：多张图像和系统提示">

要求 Claude 描述多张图像之间的差异，同时给它一个系统提示来说明如何响应。

| 内容 | |
| --- | --- |
| 系统  | 仅用西班牙语回应。 |
| 用户    | 图像 1：\[图像 1\] 图像 2：\[图像 2\] 这些图像有什么不同？ |

以下是使用 Claude Sonnet 3.7 模型的相应 API 调用。

<Tabs>
  <Tab title="使用 Base64">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        system="Respond only in Spanish.",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Image 1:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image1_media_type,
                            "data": image1_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "Image 2:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image2_media_type,
                            "data": image2_data,
                        },
                    },
                    {
                        "type": "text",
                        "text": "How are these images different?"
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
  <Tab title="使用 URL">
    ```python Python
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        system="Respond only in Spanish.",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Image 1:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "Image 2:"
                    },
                    {
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": "https://upload.wikimedia.org/wikipedia/commons/b/b5/Iridescent.green.sweat.bee1.jpg",
                        },
                    },
                    {
                        "type": "text",
                        "text": "How are these images different?"
                    }
                ],
            }
        ],
    )
    ```
  </Tab>
</Tabs>

</section>
<section title="示例：跨两个对话轮次的四张图像">

Claude 的视觉功能在混合图像和文本的多模态对话中表现出色。您可以与 Claude 进行扩展的来回交流，在任何时刻添加新图像或后续问题。这为迭代图像分析、比较或将视觉与其他知识相结合的强大工作流程提供了支持。

要求 Claude 对比两张图像，然后提出一个后续问题，将第一张图像与两张新图像进行比较。
| 角色 | 内容 |
| --- | --- |
| 用户 | 图像 1：\[图像 1\] 图像 2：\[图像 2\] 这些图像有什么不同？ |
| 助手 | \[Claude 的响应\] |
| 用户 | 图像 1：\[图像 3\] 图像 2：\[图像 4\] 这些图像与前两张相似吗？ |
| 助手 | \[Claude 的响应\] |

使用 API 时，只需将新图像插入到标准[多轮对话](/docs/zh-CN/api/messages-examples#multiple-conversational-turns)结构中 `user` 角色的消息数组中。

</section>

---

## 限制

虽然 Claude 的图像理解功能是最先进的，但需要注意一些限制：

- **人物识别**：Claude [不能用于](/docs/zh-CN/legal/aup)识别（即命名）图像中的人物，并将拒绝这样做。
- **准确性**：Claude 在解释低质量、旋转或非常小的图像（小于 200 像素）时可能会产生幻觉或犯错误。
- **空间推理**：Claude 的空间推理能力有限。它可能在需要精确定位或布局的任务中遇到困难，例如读取模拟时钟面或描述国际象棋棋子的确切位置。
- **计数**：Claude 可以给出图像中对象的近似计数，但可能不总是精确准确，特别是对于大量小对象。
- **AI 生成的图像**：Claude 不知道图像是否是 AI 生成的，如果被问及可能会不正确。不要依赖它来检测虚假或合成图像。
- **不当内容**：Claude 不会处理违反我们[可接受使用政策](https://www.anthropic.com/legal/aup)的不当或露骨图像。
- **医疗保健应用**：虽然 Claude 可以分析一般医学图像，但它不是为解释复杂的诊断扫描（如 CT 或 MRI）而设计的。Claude 的输出不应被视为专业医疗建议或诊断的替代品。

始终仔细审查和验证 Claude 的图像解释，特别是对于高风险用例。不要在没有人工监督的情况下使用 Claude 执行需要完美精度或敏感图像分析的任务。

---

## 常见问题

  <section title="Claude 支持哪些图像文件类型？">

    Claude 目前支持 JPEG、PNG、GIF 和 WebP 图像格式，具体为：
    - `image/jpeg`
    - `image/png`
    - `image/gif`
    - `image/webp`
  
</section>

{" "}

<section title="Claude 可以读取图像 URL 吗？">

  是的，Claude 现在可以通过 API 中的 URL 图像源块处理来自 URL 的图像。
  只需在 API 请求中使用"url"源类型而不是"base64"。
  示例：
  ```json
  {
    "type": "image",
    "source": {
      "type": "url",
      "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
    }
  }
  ```

</section>

  <section title="我可以上传的图像文件大小有限制吗？">

    是的，有限制：
    - API：每张图像最大 5MB
    - claude.ai：每张图像最大 10MB

    超过这些限制的图像将被拒绝，使用我们的 API 时会返回错误。

  
</section>

  <section title="我可以在一个请求中包含多少张图像？">

    图像限制为：
    - 消息 API：每个请求最多 100 张图像
    - claude.ai：每轮最多 20 张图像

    超过这些限制的请求将被拒绝并返回错误。

  
</section>

{" "}

<section title="Claude 读取图像元数据吗？">

  不，Claude 不会解析或接收从传递给它的图像中获取的任何元数据。

</section>

{" "}

<section title="我可以删除上传的图像吗？">

  不可以。图像上传是临时的，不会在 API 请求期间之外存储。上传的图像在处理后会自动删除。

</section>

{" "}

<section title="在哪里可以找到有关图像上传数据隐私的详情？">

  请参阅我们的隐私政策页面，了解我们如何处理上传的图像和其他数据的信息。我们不使用上传的图像来训练我们的模型。

</section>

  <section title="如果 Claude 的图像解释似乎有误怎么办？">

    如果 Claude 的图像解释似乎不正确：
    1. 确保图像清晰、高质量且方向正确。
    2. 尝试提示工程技术来改进结果。
    3. 如果问题仍然存在，在 claude.ai 中标记输出（竖起大拇指/竖起大拇指向下）或联系我们的支持团队。

    您的反馈帮助我们改进！

  
</section>

  <section title="Claude 可以生成或编辑图像吗？">

    不，Claude 仅是图像理解模型。它可以解释和分析图像，但不能生成、生成、编辑、操纵或创建图像。
  
</section>

---

## 深入了解视觉

准备好开始使用 Claude 构建图像了吗？以下是一些有用的资源：

- [多模态食谱](https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal)：此食谱包含有关[开始使用图像](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/getting%5Fstarted%5Fwith%5Fvision.ipynb)和[视觉最佳实践技术](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/best%5Fpractices%5Ffor%5Fvision.ipynb)的提示，以确保图像的最高质量性能。了解如何有效地使用图像提示 Claude 来执行任务，例如[解释和分析图表](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/reading%5Fcharts%5Fgraphs%5Fpowerpoints.ipynb)或[从表单中提取内容](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/how%5Fto%5Ftranscribe%5Ftext.ipynb)。
- [API 参考](/docs/zh-CN/api/messages)：访问我们的消息 API 文档，包括涉及图像的示例 [API 调用](/docs/zh-CN/build-with-claude/working-with-messages#vision)。

如果您有任何其他问题，请随时联系我们的[支持团队](https://support.claude.com/)。您也可以加入我们的[开发者社区](https://www.anthropic.com/discord)与其他创作者联系并获得 Anthropic 专家的帮助。


# Files API

---

Files API 让您可以上传和管理文件以与 Claude API 一起使用，而无需在每次请求时重新上传内容。这在使用[代码执行工具](/docs/zh-CN/agents-and-tools/tool-use/code-execution-tool)提供输入（例如数据集和文档）然后下载输出（例如图表）时特别有用。您还可以使用 Files API 来避免在多个 API 调用中不断重新上传经常使用的文档和图像。您可以[直接探索 API 参考](/docs/zh-CN/api/files-create)，以及本指南。

<Note>
Files API 目前处于测试版。请通过我们的[反馈表单](https://forms.gle/tisHyierGwgN4DUE9)分享您对 Files API 的体验。
</Note>

## 支持的模型

在 Messages 请求中引用 `file_id` 在所有支持给定文件类型的模型中都受支持。例如，[图像](/docs/zh-CN/build-with-claude/vision)在所有 Claude 3+ 模型中受支持，[PDF](/docs/zh-CN/build-with-claude/pdf-support) 在所有 Claude 3.5+ 模型中受支持，以及[各种其他文件类型](/docs/zh-CN/agents-and-tools/tool-use/code-execution-tool#supported-file-types)用于 Claude 3.5 Haiku 及所有 Claude 3.7+ 模型中的代码执行工具。

Files API 目前在 Amazon Bedrock 或 Google Vertex AI 上不受支持。

## Files API 如何工作

Files API 提供了一种简单的创建一次、使用多次的文件处理方法：

- **上传文件**到我们的安全存储并接收唯一的 `file_id`
- **下载文件**由技能或代码执行工具创建
- **在 [Messages](/docs/zh-CN/api/messages) 请求中引用文件**使用 `file_id` 而不是重新上传内容
- **管理您的文件**通过列表、检索和删除操作

## 如何使用 Files API

<Note>
要使用 Files API，您需要包含测试版功能头：`anthropic-beta: files-api-2025-04-14`。
</Note>

### 上传文件

上传文件以在将来的 API 调用中引用：

<CodeGroup>
```bash Shell
curl -X POST https://api.anthropic.com/v1/files \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14" \
  -F "file=@/path/to/document.pdf"
```

```python Python
import anthropic

client = anthropic.Anthropic()
client.beta.files.upload(
  file=("document.pdf", open("/path/to/document.pdf", "rb"), "application/pdf"),
)
```

```typescript TypeScript
import Anthropic, { toFile } from '@anthropic-ai/sdk';
import fs from "fs";

const anthropic = new Anthropic();

await anthropic.beta.files.upload({
  file: await toFile(fs.createReadStream('/path/to/document.pdf'), undefined, { type: 'application/pdf' })
}, {
  betas: ['files-api-2025-04-14']
});
```
</CodeGroup>

上传文件的响应将包括：

```json
{
  "id": "file_011CNha8iCJcU1wXNR6q4V8w",
  "type": "file",
  "filename": "document.pdf",
  "mime_type": "application/pdf",
  "size_bytes": 1024000,
  "created_at": "2025-01-01T00:00:00Z",
  "downloadable": false
}
```

### 在消息中使用文件

上传后，使用其 `file_id` 引用文件：

<CodeGroup>
```bash Shell
curl -X POST https://api.anthropic.com/v1/messages \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14" \
  -H "content-type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "Please summarize this document for me."          
          },
          {
            "type": "document",
            "source": {
              "type": "file",
              "file_id": "file_011CNha8iCJcU1wXNR6q4V8w"
            }
          }
        ]
      }
    ]
  }'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Please summarize this document for me."
                },
                {
                    "type": "document",
                    "source": {
                        "type": "file",
                        "file_id": "file_011CNha8iCJcU1wXNR6q4V8w"
                    }
                }
            ]
        }
    ],
    betas=["files-api-2025-04-14"],
)
print(response)
```

```typescript TypeScript
import { Anthropic } from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

const response = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  messages: [
    {
      role: "user",
      content: [
        {
          type: "text",
          text: "Please summarize this document for me."
        },
        {
          type: "document",
          source: {
            type: "file",
            file_id: "file_011CNha8iCJcU1wXNR6q4V8w"
          }
        }
      ]
    }
  ],
  betas: ["files-api-2025-04-14"],
});

console.log(response);
```
</CodeGroup>

### 文件类型和内容块

Files API 支持对应于不同内容块类型的不同文件类型：

| 文件类型 | MIME 类型 | 内容块类型 | 用例 |
| :--- | :--- | :--- | :--- |
| PDF | `application/pdf` | `document` | 文本分析、文档处理 |
| 纯文本 | `text/plain` | `document` | 文本分析、处理 |
| 图像 | `image/jpeg`, `image/png`, `image/gif`, `image/webp` | `image` | 图像分析、视觉任务 |
| [数据集、其他](/docs/zh-CN/agents-and-tools/tool-use/code-execution-tool#supported-file-types) | 变化 | `container_upload` | 分析数据、创建可视化  |

### 使用其他文件格式

对于不支持作为 `document` 块的文件类型（.csv、.txt、.md、.docx、.xlsx），将文件转换为纯文本，并将内容直接包含在您的消息中：

<CodeGroup>
```bash Shell
# 示例：读取文本文件并将其作为纯文本发送
# 注意：对于包含特殊字符的文件，请考虑 base64 编码
TEXT_CONTENT=$(cat document.txt | jq -Rs .)

curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -d @- <<EOF
{
  "model": "claude-sonnet-4-5",
  "max_tokens": 1024,
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Here's the document content:\n\n${TEXT_CONTENT}\n\nPlease summarize this document."
        }
      ]
    }
  ]
}
EOF
```

```python Python
import pandas as pd
import anthropic

client = anthropic.Anthropic()

# 示例：读取 CSV 文件
df = pd.read_csv('data.csv')
csv_content = df.to_string()

# 在消息中作为纯文本发送
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": f"Here's the CSV data:\n\n{csv_content}\n\nPlease analyze this data."
                }
            ]
        }
    ]
)

print(response.content[0].text)
```

```typescript TypeScript
import { Anthropic } from '@anthropic-ai/sdk';
import fs from 'fs';

const anthropic = new Anthropic();

async function analyzeDocument() {
  // 示例：读取文本文件
  const textContent = fs.readFileSync('document.txt', 'utf-8');

  // 在消息中作为纯文本发送
  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: `Here's the document content:\n\n${textContent}\n\nPlease summarize this document.`
          }
        ]
      }
    ]
  });

  console.log(response.content[0].text);
}

analyzeDocument();
```
</CodeGroup>

<Note>
对于包含图像的 .docx 文件，首先将其转换为 PDF 格式，然后使用[PDF 支持](/docs/zh-CN/build-with-claude/pdf-support)来利用内置的图像解析。这允许使用 PDF 文档中的引用。
</Note>

#### 文档块

对于 PDF 和文本文件，使用 `document` 内容块：

```json
{
  "type": "document",
  "source": {
    "type": "file",
    "file_id": "file_011CNha8iCJcU1wXNR6q4V8w"
  },
  "title": "Document Title", // 可选
  "context": "Context about the document", // 可选  
  "citations": {"enabled": true} // 可选，启用引用
}
```

#### 图像块

对于图像，使用 `image` 内容块：

```json
{
  "type": "image",
  "source": {
    "type": "file",
    "file_id": "file_011CPMxVD3fHLUhvTqtsQA5w"
  }
}
```

### 管理文件

#### 列表文件

检索您上传的文件列表：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/files \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14"
```

```python Python
import anthropic

client = anthropic.Anthropic()
files = client.beta.files.list()
```

```typescript TypeScript
import { Anthropic } from '@anthropic-ai/sdk';

const anthropic = new Anthropic();
const files = await anthropic.beta.files.list({
  betas: ['files-api-2025-04-14'],
});
```
</CodeGroup>

#### 获取文件元数据

检索有关特定文件的信息：

<CodeGroup>
```bash Shell
curl https://api.anthropic.com/v1/files/file_011CNha8iCJcU1wXNR6q4V8w \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14"
```

```python Python
import anthropic

client = anthropic.Anthropic()
file = client.beta.files.retrieve_metadata("file_011CNha8iCJcU1wXNR6q4V8w")
```

```typescript TypeScript
import { Anthropic } from '@anthropic-ai/sdk';

const anthropic = new Anthropic();
const file = await anthropic.beta.files.retrieveMetadata(
  "file_011CNha8iCJcU1wXNR6q4V8w",
  { betas: ['files-api-2025-04-14'] },
);
```
</CodeGroup>

#### 删除文件

从您的工作区删除文件：

<CodeGroup>
```bash Shell
curl -X DELETE https://api.anthropic.com/v1/files/file_011CNha8iCJcU1wXNR6q4V8w \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14"
```

```python Python
import anthropic

client = anthropic.Anthropic()
result = client.beta.files.delete("file_011CNha8iCJcU1wXNR6q4V8w")
```

```typescript TypeScript
import { Anthropic } from '@anthropic-ai/sdk';

const anthropic = new Anthropic();
const result = await anthropic.beta.files.delete(
  "file_011CNha8iCJcU1wXNR6q4V8w",
  { betas: ['files-api-2025-04-14'] },
);
```
</CodeGroup>

### 下载文件

下载由技能或代码执行工具创建的文件：

<CodeGroup>
```bash Shell
curl -X GET "https://api.anthropic.com/v1/files/file_011CNha8iCJcU1wXNR6q4V8w/content" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: files-api-2025-04-14" \
  --output downloaded_file.txt
```

```python Python
import anthropic

client = anthropic.Anthropic()
file_content = client.beta.files.download("file_011CNha8iCJcU1wXNR6q4V8w")

# 保存到文件
with open("downloaded_file.txt", "w") as f:
    f.write(file_content.decode('utf-8'))
```

```typescript TypeScript
import { Anthropic } from '@anthropic-ai/sdk';
import fs from 'fs';

const anthropic = new Anthropic();

const fileContent = await anthropic.beta.files.download(
  "file_011CNha8iCJcU1wXNR6q4V8w",
  { betas: ['files-api-2025-04-14'] },
);

// 保存到文件
fs.writeFileSync("downloaded_file.txt", fileContent);
```
</CodeGroup>

<Note>
您只能下载由[技能](/docs/zh-CN/build-with-claude/skills-guide)或[代码执行工具](/docs/zh-CN/agents-and-tools/tool-use/code-execution-tool)创建的文件。您上传的文件无法下载。
</Note>

---

## 文件存储和限制

### 存储限制

- **最大文件大小：** 每个文件 500 MB
- **总存储空间：** 每个组织 100 GB

### 文件生命周期

- 文件的作用域限于 API 密钥的工作区。其他 API 密钥可以使用由与同一工作区关联的任何其他 API 密钥创建的文件
- 文件持续存在直到您删除它们
- 已删除的文件无法恢复
- 删除后不久，文件将无法通过 API 访问，但它们可能会在活跃的 `Messages` API 调用和相关工具使用中持续存在
- 用户删除的文件将根据我们的[数据保留政策](https://privacy.claude.com/en/articles/7996866-how-long-do-you-store-my-organization-s-data)被删除。

---

## 错误处理

使用 Files API 时的常见错误包括：

- **文件未找到 (404)：** 指定的 `file_id` 不存在或您无权访问它
- **无效的文件类型 (400)：** 文件类型与内容块类型不匹配（例如，在文档块中使用图像文件）
- **超过上下文窗口大小 (400)：** 文件大于上下文窗口大小（例如在 `/v1/messages` 请求中使用 500 MB 纯文本文件）
- **无效的文件名 (400)：** 文件名不符合长度要求（1-255 个字符）或包含禁止字符（`<`、`>`、`:`、`"`、`|`、`?`、`*`、`\`、`/` 或 unicode 字符 0-31）
- **文件过大 (413)：** 文件超过 500 MB 限制
- **超过存储限制 (403)：** 您的组织已达到 100 GB 存储限制

```json
{
  "type": "error",
  "error": {
    "type": "invalid_request_error",
    "message": "File not found: file_011CNha8iCJcU1wXNR6q4V8w"
  }
}
```

## 使用和计费

Files API 操作是**免费的**：
- 上传文件
- 下载文件
- 列表文件
- 获取文件元数据  
- 删除文件

在 `Messages` 请求中使用的文件内容按输入令牌计费。您只能下载由[技能](/docs/zh-CN/build-with-claude/skills-guide)或[代码执行工具](/docs/zh-CN/agents-and-tools/tool-use/code-execution-tool)创建的文件。

### 速率限制

在测试版期间：
- 与文件相关的 API 调用限制为每分钟大约 100 个请求
- 如果您的用例需要更高的限制，请[联系我们](mailto:sales@anthropic.com)


# 搜索结果

通过提供带有源归属的搜索结果，为 RAG 应用程序启用自然引用

---

搜索结果内容块可以实现带有适当源归属的自然引用，为您的自定义应用程序带来网络搜索质量的引用。此功能对于 RAG（检索增强生成）应用程序特别强大，您需要 Claude 准确引用来源。

搜索结果功能在以下模型上可用：

- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
- Claude Opus 4 (`claude-opus-4-20250514`)
- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
- Claude Sonnet 3.7 ([已弃用](/docs/zh-CN/about-claude/model-deprecations)) (`claude-3-7-sonnet-20250219`)
- Claude 3.5 Haiku (`claude-3-5-haiku-20241022`)

## 主要优势

- **自然引用** - 为任何内容实现与网络搜索相同的引用质量
- **灵活集成** - 在工具返回中用于动态 RAG，或作为顶级内容用于预取数据
- **适当的源归属** - 每个结果都包含源和标题信息以实现清晰的归属
- **无需文档解决方法** - 消除了对基于文档的解决方法的需要
- **一致的引用格式** - 与 Claude 网络搜索功能的引用质量和格式相匹配

## 工作原理

搜索结果可以通过两种方式提供：

1. **来自工具调用** - 您的自定义工具返回搜索结果，启用动态 RAG 应用程序
2. **作为顶级内容** - 您直接在用户消息中提供搜索结果，用于预取或缓存的内容

在这两种情况下，Claude 都可以自动引用搜索结果中的信息，并进行适当的源归属。

### 搜索结果架构

搜索结果使用以下结构：

```json
{
  "type": "search_result",
  "source": "https://example.com/article",  // 必需：源 URL 或标识符
  "title": "Article Title",                  // 必需：结果的标题
  "content": [                               // 必需：文本块数组
    {
      "type": "text",
      "text": "The actual content of the search result..."
    }
  ],
  "citations": {                             // 可选：引用配置
    "enabled": true                          // 为此结果启用/禁用引用
  }
}
```

### 必需字段

| 字段 | 类型 | 描述 |
|-------|------|-------------|
| `type` | string | 必须为 `"search_result"` |
| `source` | string | 内容的源 URL 或标识符 |
| `title` | string | 搜索结果的描述性标题 |
| `content` | array | 包含实际内容的文本块数组 |

### 可选字段

| 字段 | 类型 | 描述 |
|-------|------|-------------|
| `citations` | object | 带有 `enabled` 布尔字段的引用配置 |
| `cache_control` | object | 缓存控制设置（例如 `{"type": "ephemeral"}`) |

`content` 数组中的每一项必须是一个文本块，包含：
- `type`：必须为 `"text"`
- `text`：实际文本内容（非空字符串）

## 方法 1：来自工具调用的搜索结果

最强大的用例是从您的自定义工具返回搜索结果。这启用了动态 RAG 应用程序，其中工具获取并返回相关内容，具有自动引用。

### 示例：知识库工具

<CodeGroup>
```python Python
from anthropic import Anthropic
from anthropic.types import (
    MessageParam,
    TextBlockParam,
    SearchResultBlockParam,
    ToolResultBlockParam
)

client = Anthropic()

# 定义知识库搜索工具
knowledge_base_tool = {
    "name": "search_knowledge_base",
    "description": "Search the company knowledge base for information",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "The search query"
            }
        },
        "required": ["query"]
    }
}

# 处理工具调用的函数
def search_knowledge_base(query):
    # 您的搜索逻辑在这里
    # 返回正确格式的搜索结果
    return [
        SearchResultBlockParam(
            type="search_result",
            source="https://docs.company.com/product-guide",
            title="Product Configuration Guide",
            content=[
                TextBlockParam(
                    type="text",
                    text="To configure the product, navigate to Settings > Configuration. The default timeout is 30 seconds, but can be adjusted between 10-120 seconds based on your needs."
                )
            ],
            citations={"enabled": True}
        ),
        SearchResultBlockParam(
            type="search_result",
            source="https://docs.company.com/troubleshooting",
            title="Troubleshooting Guide",
            content=[
                TextBlockParam(
                    type="text",
                    text="If you encounter timeout errors, first check the configuration settings. Common causes include network latency and incorrect timeout values."
                )
            ],
            citations={"enabled": True}
        )
    ]

# 使用工具创建消息
response = client.messages.create(
    model="claude-sonnet-4-5",  # 适用于所有支持的模型
    max_tokens=1024,
    tools=[knowledge_base_tool],
    messages=[
        MessageParam(
            role="user",
            content="How do I configure the timeout settings?"
        )
    ]
)

# 当 Claude 调用工具时，提供搜索结果
if response.content[0].type == "tool_use":
    tool_result = search_knowledge_base(response.content[0].input["query"])
    
    # 将工具结果发送回去
    final_response = client.messages.create(
        model="claude-sonnet-4-5",  # 适用于所有支持的模型
        max_tokens=1024,
        messages=[
            MessageParam(role="user", content="How do I configure the timeout settings?"),
            MessageParam(role="assistant", content=response.content),
            MessageParam(
                role="user",
                content=[
                    ToolResultBlockParam(
                        type="tool_result",
                        tool_use_id=response.content[0].id,
                        content=tool_result  # 搜索结果在这里
                    )
                ]
            )
        ]
    )
```

```typescript TypeScript
import { Anthropic } from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

// 定义知识库搜索工具
const knowledgeBaseTool = {
  name: "search_knowledge_base",
  description: "Search the company knowledge base for information",
  input_schema: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "The search query"
      }
    },
    required: ["query"]
  }
};

// 处理工具调用的函数
function searchKnowledgeBase(query: string) {
  // 您的搜索逻辑在这里
  // 返回正确格式的搜索结果
  return [
    {
      type: "search_result" as const,
      source: "https://docs.company.com/product-guide",
      title: "Product Configuration Guide",
      content: [
        {
          type: "text" as const,
          text: "To configure the product, navigate to Settings > Configuration. The default timeout is 30 seconds, but can be adjusted between 10-120 seconds based on your needs."
        }
      ],
      citations: { enabled: true }
    },
    {
      type: "search_result" as const,
      source: "https://docs.company.com/troubleshooting",
      title: "Troubleshooting Guide",
      content: [
        {
          type: "text" as const,
          text: "If you encounter timeout errors, first check the configuration settings. Common causes include network latency and incorrect timeout values."
        }
      ],
      citations: { enabled: true }
    }
  ];
}

// 使用工具创建消息
const response = await anthropic.messages.create({
  model: "claude-sonnet-4-5", // 适用于所有支持的模型
  max_tokens: 1024,
  tools: [knowledgeBaseTool],
  messages: [
    {
      role: "user",
      content: "How do I configure the timeout settings?"
    }
  ]
});

// 处理工具使用并提供结果
if (response.content[0].type === "tool_use") {
  const toolResult = searchKnowledgeBase(response.content[0].input.query);
  
  const finalResponse = await anthropic.messages.create({
    model: "claude-sonnet-4-5", // 适用于所有支持的模型
    max_tokens: 1024,
      messages: [
      { role: "user", content: "How do I configure the timeout settings?" },
      { role: "assistant", content: response.content },
      {
        role: "user",
        content: [
          {
            type: "tool_result" as const,
            tool_use_id: response.content[0].id,
            content: toolResult  // 搜索结果在这里
          }
        ]
      }
    ]
  });
}
```
</CodeGroup>

## 方法 2：作为顶级内容的搜索结果

您也可以直接在用户消息中提供搜索结果。这对以下情况很有用：
- 从您的搜索基础设施预取的内容
- 来自先前查询的缓存搜索结果
- 来自外部搜索服务的内容
- 测试和开发

### 示例：直接搜索结果

<CodeGroup>
```python Python
from anthropic import Anthropic
from anthropic.types import (
    MessageParam,
    TextBlockParam,
    SearchResultBlockParam
)

client = Anthropic()

# 直接在用户消息中提供搜索结果
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[
        MessageParam(
            role="user",
            content=[
                SearchResultBlockParam(
                    type="search_result",
                    source="https://docs.company.com/api-reference",
                    title="API Reference - Authentication",
                    content=[
                        TextBlockParam(
                            type="text",
                            text="All API requests must include an API key in the Authorization header. Keys can be generated from the dashboard. Rate limits: 1000 requests per hour for standard tier, 10000 for premium."
                        )
                    ],
                    citations={"enabled": True}
                ),
                SearchResultBlockParam(
                    type="search_result",
                    source="https://docs.company.com/quickstart",
                    title="Getting Started Guide",
                    content=[
                        TextBlockParam(
                            type="text",
                            text="To get started: 1) Sign up for an account, 2) Generate an API key from the dashboard, 3) Install our SDK using pip install company-sdk, 4) Initialize the client with your API key."
                        )
                    ],
                    citations={"enabled": True}
                ),
                TextBlockParam(
                    type="text",
                    text="Based on these search results, how do I authenticate API requests and what are the rate limits?"
                )
            ]
        )
    ]
)

print(response.model_dump_json(indent=2))
```

```typescript TypeScript
import { Anthropic } from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

// 直接在用户消息中提供搜索结果
const response = await anthropic.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  messages: [
    {
      role: "user",
      content: [
        {
          type: "search_result" as const,
          source: "https://docs.company.com/api-reference",
          title: "API Reference - Authentication",
          content: [
            {
              type: "text" as const,
              text: "All API requests must include an API key in the Authorization header. Keys can be generated from the dashboard. Rate limits: 1000 requests per hour for standard tier, 10000 for premium."
            }
          ],
          citations: { enabled: true }
        },
        {
          type: "search_result" as const,
          source: "https://docs.company.com/quickstart",
          title: "Getting Started Guide",
          content: [
            {
              type: "text" as const,
              text: "To get started: 1) Sign up for an account, 2) Generate an API key from the dashboard, 3) Install our SDK using pip install company-sdk, 4) Initialize the client with your API key."
            }
          ],
          citations: { enabled: true }
        },
        {
          type: "text" as const,
          text: "Based on these search results, how do I authenticate API requests and what are the rate limits?"
        }
      ]
    }
  ]
});

console.log(response);
```

```bash Shell
#!/bin/sh
curl https://api.anthropic.com/v1/messages \
     --header "x-api-key: $ANTHROPIC_API_KEY" \
     --header "anthropic-version: 2023-06-01" \
     --header "content-type: application/json" \
     --data \
'{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "search_result",
                    "source": "https://docs.company.com/api-reference",
                    "title": "API Reference - Authentication",
                    "content": [
                        {
                            "type": "text",
                            "text": "All API requests must include an API key in the Authorization header. Keys can be generated from the dashboard. Rate limits: 1000 requests per hour for standard tier, 10000 for premium."
                        }
                    ],
                    "citations": {
                        "enabled": true
                    }
                },
                {
                    "type": "search_result",
                    "source": "https://docs.company.com/quickstart",
                    "title": "Getting Started Guide",
                    "content": [
                        {
                            "type": "text",
                            "text": "To get started: 1) Sign up for an account, 2) Generate an API key from the dashboard, 3) Install our SDK using pip install company-sdk, 4) Initialize the client with your API key."
                        }
                    ],
                    "citations": {
                        "enabled": true
                    }
                },
                {
                    "type": "text",
                    "text": "Based on these search results, how do I authenticate API requests and what are the rate limits?"
                }
            ]
        }
    ]
}'
```
</CodeGroup>

## Claude 的带引用的响应

无论如何提供搜索结果，Claude 在使用搜索结果中的信息时都会自动包含引用：

```json
{
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "To authenticate API requests, you need to include an API key in the Authorization header",
      "citations": [
        {
          "type": "search_result_location",
          "source": "https://docs.company.com/api-reference",
          "title": "API Reference - Authentication",
          "cited_text": "All API requests must include an API key in the Authorization header",
          "search_result_index": 0,
          "start_block_index": 0,
          "end_block_index": 0
        }
      ]
    },
    {
      "type": "text",
      "text": ". You can generate API keys from your dashboard",
      "citations": [
        {
          "type": "search_result_location",
          "source": "https://docs.company.com/api-reference",
          "title": "API Reference - Authentication",
          "cited_text": "Keys can be generated from the dashboard",
          "search_result_index": 0,
          "start_block_index": 0,
          "end_block_index": 0
        }
      ]
    },
    {
      "type": "text",
      "text": ". The rate limits are 1,000 requests per hour for the standard tier and 10,000 requests per hour for the premium tier.",
      "citations": [
        {
          "type": "search_result_location",
          "source": "https://docs.company.com/api-reference",
          "title": "API Reference - Authentication",
          "cited_text": "Rate limits: 1000 requests per hour for standard tier, 10000 for premium",
          "search_result_index": 0,
          "start_block_index": 0,
          "end_block_index": 0
        }
      ]
    }
  ]
}
```

### 引用字段

每个引用包含：

| 字段 | 类型 | 描述 |
|-------|------|-------------|
| `type` | string | 对于搜索结果引用始终为 `"search_result_location"` |
| `source` | string | 原始搜索结果中的源 |
| `title` | string or null | 原始搜索结果中的标题 |
| `cited_text` | string | 被引用的确切文本 |
| `search_result_index` | integer | 搜索结果的索引（从 0 开始） |
| `start_block_index` | integer | 内容数组中的起始位置 |
| `end_block_index` | integer | 内容数组中的结束位置 |

注意：`search_result_index` 指的是搜索结果内容块的索引（从 0 开始），无论搜索结果如何提供（工具调用或顶级内容）。

## 多个内容块

搜索结果可以在 `content` 数组中包含多个文本块：

```json
{
  "type": "search_result",
  "source": "https://docs.company.com/api-guide",
  "title": "API Documentation",
  "content": [
    {
      "type": "text",
      "text": "Authentication: All API requests require an API key."
    },
    {
      "type": "text",
      "text": "Rate Limits: The API allows 1000 requests per hour per key."
    },
    {
      "type": "text",
      "text": "Error Handling: The API returns standard HTTP status codes."
    }
  ]
}
```

Claude 可以使用 `start_block_index` 和 `end_block_index` 字段引用特定块。

## 高级用法

### 结合两种方法

您可以在同一对话中使用基于工具和顶级搜索结果：

```python
# 第一条消息带有顶级搜索结果
messages = [
    MessageParam(
        role="user",
        content=[
            SearchResultBlockParam(
                type="search_result",
                source="https://docs.company.com/overview",
                title="Product Overview",
                content=[
                    TextBlockParam(type="text", text="Our product helps teams collaborate...")
                ],
                citations={"enabled": True}
            ),
            TextBlockParam(
                type="text",
                text="Tell me about this product and search for pricing information"
            )
        ]
    )
]

# Claude 可能会响应并调用工具来搜索定价
# 然后您提供带有更多搜索结果的工具结果
```

### 与其他内容类型结合

两种方法都支持将搜索结果与其他内容混合：

```python
# 在工具结果中
tool_result = [
    SearchResultBlockParam(
        type="search_result",
        source="https://docs.company.com/guide",
        title="User Guide",
        content=[TextBlockParam(type="text", text="Configuration details...")],
        citations={"enabled": True}
    ),
    TextBlockParam(
        type="text",
        text="Additional context: This applies to version 2.0 and later."
    )
]

# 在顶级内容中
user_content = [
    SearchResultBlockParam(
        type="search_result",
        source="https://research.com/paper",
        title="Research Paper",
        content=[TextBlockParam(type="text", text="Key findings...")],
        citations={"enabled": True}
    ),
    {
        "type": "image",
        "source": {"type": "url", "url": "https://example.com/chart.png"}
    },
    TextBlockParam(
        type="text",
        text="How does the chart relate to the research findings?"
    )
]
```

### 缓存控制

添加缓存控制以获得更好的性能：

```json
{
  "type": "search_result",
  "source": "https://docs.company.com/guide",
  "title": "User Guide",
  "content": [{"type": "text", "text": "..."}],
  "cache_control": {
    "type": "ephemeral"
  }
}
```

### 引用控制

默认情况下，搜索结果禁用引用。您可以通过显式设置 `citations` 配置来启用引用：

```json
{
  "type": "search_result",
  "source": "https://docs.company.com/guide",
  "title": "User Guide",
  "content": [{"type": "text", "text": "Important documentation..."}],
  "citations": {
    "enabled": true  // 为此结果启用引用
  }
}
```

当 `citations.enabled` 设置为 `true` 时，Claude 在使用搜索结果中的信息时将包含引用参考。这启用了：
- 为您的自定义 RAG 应用程序提供自然引用
- 与专有知识库交互时的源归属
- 为任何返回搜索结果的自定义工具提供网络搜索质量的引用

如果省略 `citations` 字段，默认禁用引用。

<Warning>
引用是全有或全无的：请求中的所有搜索结果要么都必须启用引用，要么都必须禁用引用。混合具有不同引用设置的搜索结果将导致错误。如果您需要为某些源禁用引用，必须为该请求中的所有搜索结果禁用引用。
</Warning>

## 最佳实践

### 对于基于工具的搜索（方法 1）

- **动态内容**：用于实时搜索和动态 RAG 应用程序
- **错误处理**：搜索失败时返回适当的消息
- **结果限制**：仅返回最相关的结果以避免上下文溢出

### 对于顶级搜索（方法 2）

- **预取内容**：当您已经有搜索结果时使用
- **批处理**：非常适合一次处理多个搜索结果
- **测试**：非常适合用已知内容测试引用行为

### 一般最佳实践

1. **有效地构建结果**
   - 使用清晰、永久的源 URL
   - 提供描述性标题
   - 将长内容分解为逻辑文本块

2. **保持一致性**
   - 在应用程序中使用一致的源格式
   - 确保标题准确反映内容
   - 保持格式一致

3. **优雅地处理错误**
   ```python
   def search_with_fallback(query):
       try:
           results = perform_search(query)
           if not results:
               return {"type": "text", "text": "No results found."}
           return format_as_search_results(results)
       except Exception as e:
           return {"type": "text", "text": f"Search error: {str(e)}"}
   ```

## 限制

- 搜索结果内容块在 Claude API 和 Google Cloud 的 Vertex AI 上可用
- 搜索结果中仅支持文本内容（无图像或其他媒体）
- `content` 数组必须至少包含一个文本块


# 结构化输出

从代理工作流中获取经过验证的 JSON 结果

---

结构化输出限制 Claude 的响应遵循特定的模式，确保有效的、可解析的输出用于下游处理。使用 **JSON 输出**（`output_format`）获取结构化数据响应，或使用**严格工具使用**（`strict: true`）确保对工具名称和输入进行模式验证。

<Note>
结构化输出目前在 Claude API 中作为公开测试版功能提供，支持 Claude Sonnet 4.5 和 Claude Opus 4.1。

要使用该功能，请设置 [beta 标头](/docs/zh-CN/api/beta-headers) `structured-outputs-2025-11-13`。
</Note>

<Tip>
使用此[表单](https://forms.gle/BFnYc6iCkWoRzFgk7)分享反馈。
</Tip>

## 为什么使用结构化输出

如果没有结构化输出，Claude 可能会生成格式错误的 JSON 响应或无效的工具输入，从而破坏您的应用程序。即使进行仔细的提示，您也可能会遇到：
- 来自无效 JSON 语法的解析错误
- 缺少必需字段
- 数据类型不一致
- 需要错误处理和重试的模式违规

结构化输出通过约束解码保证模式兼容的响应：
- **始终有效**：不再有 `JSON.parse()` 错误
- **类型安全**：保证字段类型和必需字段
- **可靠**：不需要因模式违规而重试
- **两种模式**：JSON 用于数据提取等任务，严格工具用于复杂工具和代理工作流等情况

## 快速开始

<Tabs>
<Tab title="JSON 输出">

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: structured-outputs-2025-11-13" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": "Extract the key information from this email: John Smith (john@example.com) is interested in our Enterprise plan and wants to schedule a demo for next Tuesday at 2pm."
      }
    ],
    "output_format": {
      "type": "json_schema",
      "schema": {
        "type": "object",
        "properties": {
          "name": {"type": "string"},
          "email": {"type": "string"},
          "plan_interest": {"type": "string"},
          "demo_requested": {"type": "boolean"}
        },
        "required": ["name", "email", "plan_interest", "demo_requested"],
        "additionalProperties": false
      }
    }
  }'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    betas=["structured-outputs-2025-11-13"],
    messages=[
        {
            "role": "user",
            "content": "Extract the key information from this email: John Smith (john@example.com) is interested in our Enterprise plan and wants to schedule a demo for next Tuesday at 2pm."
        }
    ],
    output_format={
        "type": "json_schema",
        "schema": {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "email": {"type": "string"},
                "plan_interest": {"type": "string"},
                "demo_requested": {"type": "boolean"}
            },
            "required": ["name", "email", "plan_interest", "demo_requested"],
            "additionalProperties": False
        }
    }
)
print(response.content[0].text)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
});

const response = await client.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  betas: ["structured-outputs-2025-11-13"],
  messages: [
    {
      role: "user",
      content: "Extract the key information from this email: John Smith (john@example.com) is interested in our Enterprise plan and wants to schedule a demo for next Tuesday at 2pm."
    }
  ],
  output_format: {
    type: "json_schema",
    schema: {
      type: "object",
      properties: {
        name: { type: "string" },
        email: { type: "string" },
        plan_interest: { type: "string" },
        demo_requested: { type: "boolean" }
      },
      required: ["name", "email", "plan_interest", "demo_requested"],
      additionalProperties: false
    }
  }
});
console.log(response.content[0].text);
```

</CodeGroup>

**响应格式：** 与您的模式匹配的有效 JSON，在 `response.content[0].text` 中返回

```json
{
  "name": "John Smith",
  "email": "john@example.com",
  "plan_interest": "Enterprise",
  "demo_requested": true
}
```

</Tab>
<Tab title="严格工具使用">

<CodeGroup>

```bash Shell
curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "anthropic-beta: structured-outputs-2025-11-13" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
      {"role": "user", "content": "What is the weather in San Francisco?"}
    ],
    "tools": [{
      "name": "get_weather",
      "description": "Get the current weather in a given location",
      "strict": true,
      "input_schema": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"]
          }
        },
        "required": ["location"],
        "additionalProperties": false
      }
    }]
  }'
```

```python Python
import anthropic

client = anthropic.Anthropic()

response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    betas=["structured-outputs-2025-11-13"],
    messages=[
        {"role": "user", "content": "What's the weather like in San Francisco?"}
    ],
    tools=[
        {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "strict": True,  # Enable strict mode
            "input_schema": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "The unit of temperature, either 'celsius' or 'fahrenheit'"
                    }
                },
                "required": ["location"],
                "additionalProperties": False
            }
        }
    ]
)
print(response.content)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
});

const response = await client.beta.messages.create({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  betas: ["structured-outputs-2025-11-13"],
  messages: [
    {
      role: "user",
      content: "What's the weather like in San Francisco?"
    }
  ],
  tools: [{
    name: "get_weather",
    description: "Get the current weather in a given location",
    strict: true,  // Enable strict mode
    input_schema: {
      type: "object",
      properties: {
        location: {
          type: "string",
          description: "The city and state, e.g. San Francisco, CA"
        },
        unit: {
          type: "string",
          enum: ["celsius", "fahrenheit"]
        }
      },
      required: ["location"],
      additionalProperties: false
    }
  }]
});
console.log(response.content);
```

</CodeGroup>

**响应格式：** 工具使用块，在 `response.content[x].input` 中具有经过验证的输入

```json
{
  "type": "tool_use",
  "name": "get_weather",
  "input": {
    "location": "San Francisco, CA"
  }
}
```

**保证：**
- 工具 `input` 严格遵循 `input_schema`
- 工具 `name` 始终有效（来自提供的工具或服务器工具）

</Tab>
</Tabs>

## 何时使用 JSON 输出与严格工具使用

为您的用例选择正确的模式：

| 何时使用 JSON 输出 | 何时使用严格工具使用 |
|---|---|
| 您需要 Claude 的响应采用特定格式 | 您需要为工具调用验证参数和工具名称 |
| 从图像或文本中提取数据 | 构建代理工作流 |
| 生成结构化报告 | 确保类型安全的函数调用 |
| 格式化 API 响应 | 具有许多和/或嵌套属性的复杂工具 |

### 为什么严格工具使用对代理很重要

构建可靠的代理系统需要保证模式一致性。无效的工具参数会破坏您的函数和工作流。Claude 可能会返回不兼容的类型（`"2"` 而不是 `2`）或缺少字段，导致运行时错误。

严格工具使用保证类型安全的参数：
- 函数每次都接收正确类型的参数
- 无需验证和重试工具调用
- 生产就绪的代理在规模上一致工作

例如，假设预订系统需要 `passengers: int`。在没有严格模式的情况下，Claude 可能会提供 `passengers: "two"` 或 `passengers: "2"`。使用 `strict: true`，您可以保证 `passengers: 2`。

## 结构化输出如何工作

<Tabs>
<Tab title="JSON 输出">

使用以下步骤实现 JSON 结构化输出：

<Steps>
  <Step title="定义您的 JSON 模式">
    创建一个 JSON 模式来描述您希望 Claude 遵循的结构。该模式使用标准 JSON Schema 格式，但有一些限制（请参阅 [JSON Schema 限制](#json-schema-limitations)）。
  </Step>
  <Step title="添加 output_format 参数">
    在您的 API 请求中包含 `output_format` 参数，其中 `type: "json_schema"` 和您的模式定义。
  </Step>
  <Step title="包含 beta 标头">
    将 `anthropic-beta: structured-outputs-2025-11-13` 标头添加到您的请求中。
  </Step>
  <Step title="解析响应">
    Claude 的响应将是与您的模式匹配的有效 JSON，在 `response.content[0].text` 中返回。
  </Step>
</Steps>

</Tab>
<Tab title="严格工具使用">

使用以下步骤实现严格工具使用：

<Steps>
  <Step title="定义您的工具模式">
    为您的工具的 `input_schema` 创建一个 JSON 模式。该模式使用标准 JSON Schema 格式，但有一些限制（请参阅 [JSON Schema 限制](#json-schema-limitations)）。
  </Step>
  <Step title="添加 strict: true">
    在您的工具定义中将 `"strict": true` 设置为顶级属性，与 `name`、`description` 和 `input_schema` 一起。
  </Step>
  <Step title="包含 beta 标头">
    将 `anthropic-beta: structured-outputs-2025-11-13` 标头添加到您的请求中。
  </Step>
  <Step title="处理工具调用">
    当 Claude 使用该工具时，工具使用块中的 `input` 字段将严格遵循您的 `input_schema`，`name` 将始终有效。
  </Step>
</Steps>

</Tab>
</Tabs>

## 在 SDK 中使用 JSON 输出

Python 和 TypeScript SDK 提供了帮助程序，使使用 JSON 输出变得更容易，包括模式转换、自动验证和与流行模式库的集成。

### 使用 Pydantic 和 Zod

对于 Python 和 TypeScript 开发人员，您可以使用熟悉的模式定义工具（如 Pydantic 和 Zod），而不是编写原始 JSON 模式。

<Note>
**仅限 JSON 输出**

SDK 帮助程序（Pydantic、Zod、`parse()`）仅适用于 JSON 输出（`output_format`）。

这些帮助程序转换和验证 Claude 对您的输出。严格工具使用验证 Claude 对您的工具的输入，这些工具使用工具定义中现有的 `input_schema` 字段。

对于严格工具使用，在工具定义中直接定义您的 `input_schema`，使用 `strict: true`。
</Note>

<CodeGroup>

```python Python
from pydantic import BaseModel
from anthropic import Anthropic, transform_schema

class ContactInfo(BaseModel):
    name: str
    email: str
    plan_interest: str
    demo_requested: bool

client = Anthropic()

# With .create() - requires transform_schema()
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    betas=["structured-outputs-2025-11-13"],
    messages=[
        {
            "role": "user",
            "content": "Extract the key information from this email: John Smith (john@example.com) is interested in our Enterprise plan and wants to schedule a demo for next Tuesday at 2pm."
        }
    ],
    output_format={
        "type": "json_schema",
        "schema": transform_schema(ContactInfo),
    }
)

print(response.content[0].text)

# With .parse() - can pass Pydantic model directly
response = client.beta.messages.parse(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    betas=["structured-outputs-2025-11-13"],
    messages=[
        {
            "role": "user",
            "content": "Extract the key information from this email: John Smith (john@example.com) is interested in our Enterprise plan and wants to schedule a demo for next Tuesday at 2pm."
        }
    ],
    output_format=ContactInfo,
)

print(response.parsed_output)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';
import { z } from 'zod';
import { betaZodOutputFormat } from '@anthropic-ai/sdk/helpers/beta/zod';

const ContactInfoSchema = z.object({
  name: z.string(),
  email: z.string(),
  plan_interest: z.string(),
  demo_requested: z.boolean(),
});

const client = new Anthropic();

const response = await client.beta.messages.parse({
  model: "claude-sonnet-4-5",
  max_tokens: 1024,
  betas: ["structured-outputs-2025-11-13"],
  messages: [
    {
      role: "user",
      content: "Extract the key information from this email: John Smith (john@example.com) is interested in our Enterprise plan and wants to schedule a demo for next Tuesday at 2pm."
    }
  ],
  output_format: betaZodOutputFormat(ContactInfoSchema),
});

// Automatically parsed and validated
console.log(response.parsed_output);
```

</CodeGroup>

### SDK 特定方法

**Python：`client.beta.messages.parse()`（推荐）**

`parse()` 方法自动转换您的 Pydantic 模型，验证响应，并返回 `parsed_output` 属性。

<Note>
`parse()` 方法在 `client.beta.messages` 上可用，而不是在 `client.messages` 上。
</Note>

<section title="示例用法">

```python
from pydantic import BaseModel
import anthropic

class ContactInfo(BaseModel):
    name: str
    email: str
    plan_interest: str

client = anthropic.Anthropic()

response = client.beta.messages.parse(
    model="claude-sonnet-4-5",
    betas=["structured-outputs-2025-11-13"],
    max_tokens=1024,
    messages=[{"role": "user", "content": "..."}],
    output_format=ContactInfo,
)

# Access the parsed output directly
contact = response.parsed_output
print(contact.name, contact.email)
```

</section>

**Python：`transform_schema()` 帮助程序**

用于在发送前手动转换模式，或当您想修改 Pydantic 生成的模式时。与 `client.beta.messages.parse()` 不同，后者自动转换提供的模式，这给了您转换后的模式，以便您可以进一步自定义它。

<section title="示例用法">

```python
from anthropic import transform_schema
from pydantic import TypeAdapter

# First convert Pydantic model to JSON schema, then transform
schema = TypeAdapter(ContactInfo).json_schema()
schema = transform_schema(schema)
# Modify schema if needed
schema["properties"]["custom_field"] = {"type": "string"}

response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    betas=["structured-outputs-2025-11-13"],
    max_tokens=1024,
    output_format=schema,
    messages=[{"role": "user", "content": "..."}],
)
```

</section>

### SDK 转换如何工作

Python 和 TypeScript SDK 都自动转换具有不支持功能的模式：

1. **删除不支持的约束**（例如 `minimum`、`maximum`、`minLength`、`maxLength`）
2. **更新描述**与约束信息（例如"必须至少 100"），当约束不直接支持结构化输出时
3. **为所有对象添加 `additionalProperties: false`**
4. **过滤字符串格式**为仅支持的列表
5. **验证响应**对照您的原始模式（包含所有约束）

这意味着 Claude 接收一个简化的模式，但您的代码仍然通过验证强制执行所有约束。

**示例：** 具有 `minimum: 100` 的 Pydantic 字段在发送的模式中变成普通整数，但描述更新为"必须至少 100"，SDK 根据原始约束验证响应。

## 常见用例

<section title="数据提取（JSON 输出）">

从非结构化文本中提取结构化数据：

<CodeGroup>

```python Python
from pydantic import BaseModel
from typing import List

class Invoice(BaseModel):
    invoice_number: str
    date: str
    total_amount: float
    line_items: List[dict]
    customer_name: str

response = client.beta.messages.parse(
    model="claude-sonnet-4-5",
    betas=["structured-outputs-2025-11-13"],
    output_format=Invoice,
    messages=[{"role": "user", "content": f"Extract invoice data from: {invoice_text}"}]
)
```

```typescript TypeScript
import { z } from 'zod';

const InvoiceSchema = z.object({
  invoice_number: z.string(),
  date: z.string(),
  total_amount: z.number(),
  line_items: z.array(z.record(z.any())),
  customer_name: z.string(),
});

const response = await client.beta.messages.parse({
  model: "claude-sonnet-4-5",
  betas: ["structured-outputs-2025-11-13"],
  output_format: InvoiceSchema,
  messages: [{"role": "user", "content": `Extract invoice data from: ${invoiceText}`}]
});
```

</CodeGroup>

</section>

<section title="分类（JSON 输出）">

使用结构化类别对内容进行分类：

<CodeGroup>

```python Python
from pydantic import BaseModel
from typing import List

class Classification(BaseModel):
    category: str
    confidence: float
    tags: List[str]
    sentiment: str

response = client.beta.messages.parse(
    model="claude-sonnet-4-5",
    betas=["structured-outputs-2025-11-13"],
    output_format=Classification,
    messages=[{"role": "user", "content": f"Classify this feedback: {feedback_text}"}]
)
```

```typescript TypeScript
import { z } from 'zod';

const ClassificationSchema = z.object({
  category: z.string(),
  confidence: z.number(),
  tags: z.array(z.string()),
  sentiment: z.string(),
});

const response = await client.beta.messages.parse({
  model: "claude-sonnet-4-5",
  betas: ["structured-outputs-2025-11-13"],
  output_format: ClassificationSchema,
  messages: [{"role": "user", "content": `Classify this feedback: ${feedbackText}`}]
});
```

</CodeGroup>

</section>

<section title="API 响应格式化（JSON 输出）">

生成 API 就绪的响应：

<CodeGroup>

```python Python
from pydantic import BaseModel
from typing import List, Optional

class APIResponse(BaseModel):
    status: str
    data: dict
    errors: Optional[List[dict]]
    metadata: dict

response = client.beta.messages.parse(
    model="claude-sonnet-4-5",
    betas=["structured-outputs-2025-11-13"],
    output_format=APIResponse,
    messages=[{"role": "user", "content": "Process this request: ..."}]
)
```

```typescript TypeScript
import { z } from 'zod';

const APIResponseSchema = z.object({
  status: z.string(),
  data: z.record(z.any()),
  errors: z.array(z.record(z.any())).optional(),
  metadata: z.record(z.any()),
});

const response = await client.beta.messages.parse({
  model: "claude-sonnet-4-5",
  betas: ["structured-outputs-2025-11-13"],
  output_format: APIResponseSchema,
  messages: [{"role": "user", "content": "Process this request: ..."}]
});
```

</CodeGroup>

</section>

<section title="经过验证的工具输入（严格工具使用）">

确保工具参数完全匹配您的模式：

<CodeGroup>

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    betas=["structured-outputs-2025-11-13"],
    messages=[{"role": "user", "content": "Search for flights to Tokyo"}],
    tools=[{
        "name": "search_flights",
        "strict": True,
        "input_schema": {
            "type": "object",
            "properties": {
                "destination": {"type": "string"},
                "departure_date": {"type": "string", "format": "date"},
                "passengers": {"type": "integer", "enum": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}
            },
            "required": ["destination", "departure_date"],
            "additionalProperties": False
        }
    }]
)
```

```typescript TypeScript
const response = await client.beta.messages.create({
  model: "claude-sonnet-4-5",
  betas: ["structured-outputs-2025-11-13"],
  messages: [{"role": "user", "content": "Search for flights to Tokyo"}],
  tools: [{
    name: "search_flights",
    strict: true,
    input_schema: {
      type: "object",
      properties: {
        destination: {type: "string"},
        departure_date: {type: "string", format: "date"},
        passengers: {type: "integer", enum: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}
      },
      required: ["destination", "departure_date"],
      additionalProperties: false
    }
  }]
});
```

</CodeGroup>

</section>

<section title="具有多个经过验证的工具的代理工作流（严格工具使用）">

使用保证的工具参数构建可靠的多步代理：

<CodeGroup>

```python Python
response = client.beta.messages.create(
    model="claude-sonnet-4-5",
    betas=["structured-outputs-2025-11-13"],
    messages=[{"role": "user", "content": "Help me plan a trip to Paris for 2 people"}],
    tools=[
        {
            "name": "search_flights",
            "strict": True,
            "input_schema": {
                "type": "object",
                "properties": {
                    "origin": {"type": "string"},
                    "destination": {"type": "string"},
                    "departure_date": {"type": "string", "format": "date"},
                    "travelers": {"type": "integer", "enum": [1, 2, 3, 4, 5, 6]}
                },
                "required": ["origin", "destination", "departure_date"],
                "additionalProperties": False
            }
        },
        {
            "name": "search_hotels",
            "strict": True,
            "input_schema": {
                "type": "object",
                "properties": {
                    "city": {"type": "string"},
                    "check_in": {"type": "string", "format": "date"},
                    "guests": {"type": "integer", "enum": [1, 2, 3, 4]}
                },
                "required": ["city", "check_in"],
                "additionalProperties": False
            }
        }
    ]
)
```

```typescript TypeScript
const response = await client.beta.messages.create({
  model: "claude-sonnet-4-5",
  betas: ["structured-outputs-2025-11-13"],
  messages: [{"role": "user", "content": "Help me plan a trip to Paris for 2 people"}],
  tools: [
    {
      name: "search_flights",
      strict: true,
      input_schema: {
        type: "object",
        properties: {
          origin: {type: "string"},
          destination: {type: "string"},
          departure_date: {type: "string", format: "date"},
          travelers: {type: "integer", enum: [1, 2, 3, 4, 5, 6]}
        },
        required: ["origin", "destination", "departure_date"],
        additionalProperties: false
      }
    },
    {
      name: "search_hotels",
      strict: true,
      input_schema: {
        type: "object",
        properties: {
          city: {type: "string"},
          check_in: {type: "string", format: "date"},
          guests: {type: "integer", enum: [1, 2, 3, 4]}
        },
        required: ["city", "check_in"],
        additionalProperties: false
      }
    }
  ]
});
```

</CodeGroup>

</section>

## 重要考虑事项

### 语法编译和缓存

结构化输出使用带有编译语法工件的约束采样。这引入了一些需要注意的性能特征：

- **首次请求延迟**：第一次使用特定模式时，在编译语法时会有额外的延迟
- **自动缓存**：编译的语法从上次使用起缓存 24 小时，使后续请求快得多
- **缓存失效**：如果您更改以下内容，缓存将失效：
  - JSON 模式结构
  - 请求中的工具集（使用结构化输出和工具使用时）
  - 仅更改 `name` 或 `description` 字段不会使缓存失效

### 提示修改和令牌成本

使用结构化输出时，Claude 自动接收一个额外的系统提示，解释预期的输出格式。这意味着：

- 您的输入令牌计数将略高
- 注入的提示像任何其他系统提示一样花费您令牌
- 更改 `output_format` 参数将使该对话线程的任何 [prompt cache](/docs/zh-CN/build-with-claude/prompt-caching) 失效

### JSON Schema 限制

结构化输出支持标准 JSON Schema，但有一些限制。JSON 输出和严格工具使用都共享这些限制。

<section title="支持的功能">

- 所有基本类型：object、array、string、integer、number、boolean、null
- `enum`（仅字符串、数字、布尔值或空值 - 无复杂类型）
- `const`
- `anyOf` 和 `allOf`（有限制 - 不支持带 `$ref` 的 `allOf`）
- `$ref`、`$def` 和 `definitions`（不支持外部 `$ref`）
- 所有支持类型的 `default` 属性
- `required` 和 `additionalProperties`（对象必须设置为 `false`）
- 字符串格式：`date-time`、`time`、`date`、`duration`、`email`、`hostname`、`uri`、`ipv4`、`ipv6`、`uuid`
- 数组 `minItems`（仅支持值 0 和 1）

</section>

<section title="不支持">

- 递归模式
- 枚举中的复杂类型
- 外部 `$ref`（例如 `'$ref': 'http://...'`）
- 数值约束（`minimum`、`maximum`、`multipleOf` 等）
- 字符串约束（`minLength`、`maxLength`）
- 超过 `minItems` 为 0 或 1 的数组约束
- `additionalProperties` 设置为 `false` 以外的任何值

如果您使用不支持的功能，您将收到 400 错误，其中包含详细信息。

</section>

<section title="模式支持（正则表达式）">

**支持的正则表达式功能：**
- 完全匹配（`^...$`）和部分匹配
- 量词：`*`、`+`、`?`、简单 `{n,m}` 情况
- 字符类：`[]`、`.`、`\d`、`\w`、`\s`
- 组：`(...)`

**不支持：**
- 对组的反向引用（例如 `\1`、`\2`）
- 前向/后向断言（例如 `(?=...)`、`(?!...)`）
- 字边界：`\b`、`\B`
- 具有大范围的复杂 `{n,m}` 量词

简单的正则表达式模式效果很好。复杂的模式可能导致 400 错误。

</section>

<Tip>
Python 和 TypeScript SDK 可以通过删除不支持的功能并将约束添加到字段描述来自动转换模式。有关详细信息，请参阅 [SDK 特定方法](#sdk-specific-methods)。
</Tip>

### 无效输出

虽然结构化输出在大多数情况下保证模式合规性，但在某些情况下输出可能不匹配您的模式：

**拒绝**（`stop_reason: "refusal"`）

Claude 即使在使用结构化输出时也保持其安全性和有用性属性。如果 Claude 出于安全原因拒绝请求：

- 响应将具有 `stop_reason: "refusal"`
- 您将收到 200 状态代码
- 您将被计费生成的令牌
- 输出可能不匹配您的模式（拒绝优先）

**达到令牌限制**（`stop_reason: "max_tokens"`）

如果响应因达到 `max_tokens` 限制而被截断：

- 响应将具有 `stop_reason: "max_tokens"`
- 输出可能不完整且不匹配您的模式
- 使用更高的 `max_tokens` 值重试以获得完整的结构化输出

### 模式验证错误

如果您的模式使用不支持的功能或过于复杂，您将收到 400 错误：

**"模式中的递归定义过多"**
- 原因：模式具有过多或循环递归定义
- 解决方案：简化模式结构，减少嵌套深度

**"模式过于复杂"**
- 原因：模式超过复杂性限制
- 解决方案：分解为较小的模式，简化结构，或减少标记为 `strict: true` 的工具数量

对于有效模式的持续问题，请 [联系支持](https://support.claude.com/en/articles/9015913-how-to-get-support) 并提供您的模式定义。

## 功能兼容性

**适用于：**
- **[批处理](/docs/zh-CN/build-with-claude/batch-processing)**：以 50% 折扣大规模处理结构化输出
- **[令牌计数](/docs/zh-CN/build-with-claude/token-counting)**：计数令牌而不编译
- **[流式传输](/docs/zh-CN/build-with-claude/streaming)**：像普通响应一样流式传输结构化输出
- **组合使用**：在同一请求中一起使用 JSON 输出（`output_format`）和严格工具使用（`strict: true`）

**不兼容：**
- **[引用](/docs/zh-CN/build-with-claude/citations)**：引用需要将引用块与文本交错，这与严格 JSON 模式约束冲突。如果启用了引用的 `output_format`，返回 400 错误。
- **[消息预填充](/docs/zh-CN/build-with-claude/prompt-engineering/prefill-claudes-response)**：与 JSON 输出不兼容

<Tip>
**语法范围**：语法仅适用于 Claude 的直接输出，不适用于工具使用调用、工具结果或思考标签（使用 [Extended Thinking](/docs/zh-CN/build-with-claude/extended-thinking) 时）。语法状态在各部分之间重置，允许 Claude 自由思考，同时仍在最终响应中生成结构化输出。
</Tip>


# Google Sheets 插件

[Claude for Sheets 扩展](https://workspace.google.com/marketplace/app/claude%5Ffor%5Fsheets/909417792257)将 Claude 集成到 Google Sheets 中，允许您直接在单元格中执行与 Claude 的交互。

---

## 为什么使用 Claude for Sheets？

Claude for Sheets 通过让您能够在评估套件中并行测试提示，实现了大规模的提示工程。此外，它在调查分析和在线数据处理等办公任务方面表现出色。

访问我们的[提示工程示例表格](https://docs.google.com/spreadsheets/d/1sUrBWO0u1-ZuQ8m5gt3-1N5PLR6r__UsRsB7WeySDQA/copy)来查看实际效果。

***

## 开始使用 Claude for Sheets

### 安装 Claude for Sheets

使用以下步骤轻松启用 Claude for Sheets：

<Steps>
  <Step title="获取您的 Claude API 密钥">
    如果您还没有 API 密钥，您可以在 [Claude Console](/settings/keys) 中创建 API 密钥。
  </Step>
  <Step title="安装 Claude for Sheets 扩展">
    在插件市场中找到 [Claude for Sheets 扩展](https://workspace.google.com/marketplace/app/claude%5Ffor%5Fsheets/909417792257)，然后点击蓝色的 `Install` 按钮并接受权限。
    <section title="权限">

      Claude for Sheets 扩展将要求各种权限以正常运行。请放心，我们只处理用户要求 Claude 运行的特定数据片段。这些数据从不用于训练我们的生成模型。

      扩展权限包括：

      - **查看和管理已安装此应用程序的电子表格：** 需要运行提示并返回结果
      - **连接到外部服务：** 需要调用 Claude API 端点
      - **允许此应用程序在您不在场时运行：** 需要在没有用户干预的情况下运行单元格重新计算
      - **在 Google 应用程序内的提示和侧边栏中显示和运行第三方网页内容：** 需要显示侧边栏和安装后提示
    
</section>
  </Step>
  <Step title="连接您的 API 密钥">
    在 `Extensions` > `Claude for Sheets™` > `Open sidebar` > `☰` > `Settings` > `API provider` 中输入您的 API 密钥。您可能需要等待或刷新以使 Claude for Sheets 菜单出现。
    ![](/docs/images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png)
  </Step>
</Steps>

<Warning>
  每次创建新的 Google Sheet 时，您都必须重新输入您的 API 密钥
</Warning>

### 输入您的第一个提示

您可以使用两个主要函数来通过 Claude for Sheets 调用 Claude。现在，让我们使用 `CLAUDE()`。

<Steps>
   <Step title="简单提示">
      在任何单元格中，输入 `=CLAUDE("Claude，用一句话说说蓝色有什么好处？")`
      > Claude 应该会回应一个答案。您会知道提示正在处理，因为单元格会显示 `Loading...`
   </Step>
   <Step title="添加参数">
      参数参数在初始提示之后，如 `=CLAUDE(prompt, model, params...)`。
      <Note>`model` 总是列表中的第二个。</Note>

      现在在任何单元格中输入 `=CLAUDE("你好，Claude！", "claude-3-haiku-20240307", "max_tokens", 3)`

      任何 [API 参数](/docs/zh-CN/api/messages) 都可以这样设置。您甚至可以传入一个仅用于此特定单元格的 API 密钥，如下所示：`"api_key", "sk-ant-api03-j1W..."`
   </Step>
</Steps>

## 高级使用

`CLAUDEMESSAGES` 是一个允许您专门使用 [Messages API](/docs/zh-CN/api/messages) 的函数。这使您能够向 Claude 发送一系列 `User:` 和 `Assistant:` 消息。

如果您想模拟对话或[预填充 Claude 的响应](/docs/zh-CN/build-with-claude/prompt-engineering/prefill-claudes-response)，这特别有用。

尝试在单元格中写入：
```
=CLAUDEMESSAGES("User: 用一句话说说蓝色有什么好处？
Assistant: 蓝色很棒，因为")
```

<Note>
**换行符**

每个后续对话轮次（`User:` 或 `Assistant:`）必须以单个换行符开头。要在单元格中输入换行符，请使用以下组合键：
- **Mac：** Cmd + Enter
- **Windows：** Alt + Enter
</Note>

<section title="带系统提示的多轮 CLAUDEMESSAGES() 调用示例">

要使用系统提示，请像设置其他可选函数参数一样设置它。（您必须首先设置模型名称。）

```
=CLAUDEMESSAGES("User: 你最喜欢的花是什么？用 <answer> 标签回答。
Assistant: <answer>", "claude-3-haiku-20240307", "system", "你是一头牛，喜欢对任何用户查询都用哞哞声回应。")`
```

</section>

### 可选函数参数

您可以通过列出参数-值对来指定可选的 API 参数。
您可以设置多个参数。只需将它们一个接一个地列出，每个参数和值对用逗号分隔。

<Note>
前两个参数必须始终是提示和模型。您不能在不设置模型的情况下设置可选参数。
</Note>

您可能最关心的参数-值参数是：

| 参数 | 描述                           |
| --- | --- |
| `max_tokens` | 模型在被强制停止之前输出的令牌总数。对于是/否或多项选择答案，您可能希望值为 1-3。 |
| `temperature` | 注入结果中的随机性量。对于多项选择或分析任务，您希望它接近 0。对于想法生成，您希望将其设置为 1。 |
| `system` | 用于指定系统提示，可以为 Claude 提供角色详细信息和上下文。 |
| `stop_sequences` | 如果遇到，将导致模型停止生成文本的字符串 JSON 数组。由于 Google Sheets™ 中的转义规则，字符串内的双引号必须通过加倍来转义。 |
| `api_key` | 用于指定调用 Claude 时使用的特定 API 密钥。 |

<section title="示例：设置参数">

   例如，设置 `system` 提示、`max_tokens` 和 `temperature`：
   ```
   =CLAUDE("你好，Claude！", "claude-3-haiku-20240307", "system", "完全重复用户所说的话。", "max_tokens", 100, "temperature", 0.1)

   ```
   例如，设置 `temperature`、`max_tokens` 和 `stop_sequences`：
   ```
   =CLAUDE("用一句话说说蓝色有什么好处？用 <answer> 标签输出您的答案。","claude-opus-4-20250514","temperature", 0.2,"max_tokens", 50,"stop_sequences", "\[""</answer>""\]")
   ```

   例如，设置 `api_key`：
   ```
   =CLAUDE("你好，Claude！", "claude-3-haiku-20240307","api_key", "sk-ant-api03-j1W...")
   ```

</section>

---

## Claude for Sheets 使用示例

### 提示工程交互式教程

我们深入的[提示工程交互式教程](https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?usp=sharing)利用了 Claude for Sheets。
查看它来学习或复习提示工程技术。

<Note>就像任何 Claude for Sheets 实例一样，您需要一个 API 密钥来与教程交互。</Note>

### 提示工程工作流程

我们的 [Claude for Sheets 提示示例工作台](https://docs.google.com/spreadsheets/d/1sUrBWO0u1-ZuQ8m5gt3-1N5PLR6r%5F%5FUsRsB7WeySDQA/copy) 是一个由 Claude 驱动的电子表格，包含示例提示和提示工程结构。

### Claude for Sheets 工作簿模板

复制我们的 [Claude for Sheets 工作簿模板](https://docs.google.com/spreadsheets/d/1UwFS-ZQWvRqa6GkbL4sy0ITHK2AhXKe-jpMLzS0kTgk/copy) 来开始您自己的 Claude for Sheets 工作！

---

## 故障排除

<section title="NAME? 错误：未知函数：'claude'">

1. 确保您已在当前工作表中启用了扩展
   1. 转到 _Extensions_ \> _Add-ons_ \> _Manage add-ons_
   2. 点击 Claude for Sheets 扩展右上角的三点菜单，确保选中"在此文档中使用"
      ![](/docs/images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png)
2. 刷新页面

</section>

<section title="#ERROR!、⚠ DEFERRED ⚠ 或 ⚠ THROTTLED ⚠">

您可以通过从 Claude for Sheets 扩展菜单中的重新计算选项中选择来手动重新计算 `#ERROR!`、`⚠ DEFERRED ⚠` 或 `⚠ THROTTLED ⚠` 单元格。

![](/docs/images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png)

</section>

<section title="无法输入 API 密钥">

1. 等待 20 秒，然后再次检查
2. 刷新页面并再次等待 20 秒
3. 卸载并重新安装扩展

</section>
---

## 更多信息

有关此扩展的更多信息，请参阅 [Claude for Sheets Google Workspace Marketplace](https://workspace.google.com/marketplace/app/claude%5Ffor%5Fsheets/909417792257) 概述页面。

