"""
LLMæœåŠ¡ - ç®¡ç†ä¸åŒLLM Providerçš„è°ƒç”¨
"""
import json
from typing import AsyncGenerator
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.chat_models import ChatOllama
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from app.models import LLMProvider
from app.schemas.provider import ProviderTestResponse
from app.logger import get_logger

logger = get_logger("rag-chat.llm")


class LLMService:
    """LLMæœåŠ¡"""
    
    def get_llm(
        self, 
        provider: LLMProvider, 
        model: str | None = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        streaming: bool = False
    ) -> BaseChatModel:
        """
        æ ¹æ®Provideré…ç½®è·å–LLMå®ä¾‹
        """
        model_name = model
        if not model_name and provider.models:
            # ä½¿ç”¨é…ç½®çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
            models = json.loads(provider.models)
            model_name = models[0] if models else None
        
        if not model_name:
            raise ValueError("No model specified")
        
        logger.debug(f"åˆ›å»ºLLMå®ä¾‹: type={provider.provider_type}, model={model_name}")
        
        if provider.provider_type == "openai":
            return ChatOpenAI(
                base_url=provider.base_url,
                api_key=provider.api_key or "not-needed",
                model=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                streaming=streaming
            )
        elif provider.provider_type == "claude":
            return ChatAnthropic(
                api_key=provider.api_key,
                model=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                streaming=streaming
            )
        elif provider.provider_type == "ollama":
            return ChatOllama(
                base_url=provider.base_url or "http://localhost:11434",
                model=model_name,
                temperature=temperature,
                num_predict=max_tokens
            )
        else:
            raise ValueError(f"Unsupported provider type: {provider.provider_type}")
    
    async def test_provider(self, provider: LLMProvider) -> ProviderTestResponse:
        """
        æµ‹è¯•Providerè¿æ¥
        """
        logger.info(f"ğŸ”— æµ‹è¯•LLMè¿æ¥: {provider.name} ({provider.provider_type})")
        
        try:
            llm = self.get_llm(provider, temperature=0)
            # å‘é€ç®€å•æµ‹è¯•æ¶ˆæ¯
            response = await llm.ainvoke([HumanMessage(content="Say 'OK' if you can hear me.")])
            
            # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
            models = []
            if provider.models:
                models = json.loads(provider.models)
            
            logger.info(f"âœ… LLMè¿æ¥æˆåŠŸ: {provider.name}")
            return ProviderTestResponse(
                success=True,
                message=f"è¿æ¥æˆåŠŸ: {response.content[:100]}",
                models=models
            )
        except Exception as e:
            logger.error(f"âŒ LLMè¿æ¥å¤±è´¥: {provider.name} - {e}")
            return ProviderTestResponse(
                success=False,
                message=f"è¿æ¥å¤±è´¥: {str(e)}"
            )
    
    async def chat(
        self,
        provider: LLMProvider,
        messages: list[dict],
        model: str | None = None,
        temperature: float = 0.7,
        max_tokens: int = 2048
    ) -> str:
        """
        åŒæ­¥èŠå¤©
        """
        llm = self.get_llm(provider, model, temperature, max_tokens)
        
        langchain_messages = []
        for msg in messages:
            if msg["role"] == "system":
                langchain_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                langchain_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                langchain_messages.append(AIMessage(content=msg["content"]))
        
        response = await llm.ainvoke(langchain_messages)
        return response.content
    
    async def chat_stream(
        self,
        provider: LLMProvider,
        messages: list[dict],
        model: str | None = None,
        temperature: float = 0.7,
        max_tokens: int = 2048
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼èŠå¤©
        """
        llm = self.get_llm(provider, model, temperature, max_tokens, streaming=True)
        
        langchain_messages = []
        for msg in messages:
            if msg["role"] == "system":
                langchain_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                langchain_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                langchain_messages.append(AIMessage(content=msg["content"]))
        
        async for chunk in llm.astream(langchain_messages):
            if chunk.content:
                yield chunk.content
