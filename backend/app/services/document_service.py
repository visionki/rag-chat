"""
æ–‡æ¡£æœåŠ¡ - å¤„ç†æ–‡æ¡£ä¸Šä¼ ã€è§£æã€å‘é‡åŒ–
"""
import uuid
import aiofiles
from pathlib import Path
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from app.config import settings
from app.models import Document, EmbeddingProvider
from app.schemas.document import DocumentResponse, DocumentListResponse, DocumentUploadResponse
from app.utils.file_parser import FileParser
from app.utils.text_splitter import get_text_splitter
from app.services.vectorstore_service import VectorStoreService
from app.logger import get_logger

logger = get_logger("rag-chat.document")


class DocumentService:
    """æ–‡æ¡£æœåŠ¡"""
    
    def __init__(self):
        self.vectorstore_service = VectorStoreService()
    
    async def upload_document(
        self,
        db: AsyncSession,
        filename: str,
        content: bytes
    ) -> DocumentUploadResponse:
        """
        ä¸Šä¼ æ–‡æ¡£
        """
        logger.info(f"ğŸ“„ å¼€å§‹ä¸Šä¼ æ–‡æ¡£: {filename}")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not FileParser.is_supported(filename):
            logger.warning(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename}")
            raise ValueError(f"Unsupported file type: {filename}")
        
        # ç”Ÿæˆå­˜å‚¨è·¯å¾„
        file_ext = Path(filename).suffix
        stored_filename = f"{uuid.uuid4().hex}{file_ext}"
        file_path = settings.UPLOAD_DIR / stored_filename
        
        # ä¿å­˜æ–‡ä»¶
        async with aiofiles.open(file_path, "wb") as f:
            await f.write(content)
        
        file_size_kb = len(content) / 1024
        logger.info(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜: {stored_filename} ({file_size_kb:.1f} KB)")
        
        # è®¡ç®—å“ˆå¸Œ
        content_hash = FileParser.compute_hash(content)
        
        # åˆ›å»ºæ–‡æ¡£è®°å½•
        document = Document(
            filename=filename,
            file_path=str(file_path),
            file_type=FileParser.get_file_type(filename),
            file_size=len(content),
            content_hash=content_hash,
            status="pending"
        )
        
        db.add(document)
        await db.flush()
        await db.refresh(document)
        
        logger.info(f"âœ… æ–‡æ¡£è®°å½•å·²åˆ›å»º: ID={document.id}, çŠ¶æ€=pending")
        
        return DocumentUploadResponse(
            id=document.id,
            filename=document.filename,
            status=document.status,
            message="æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼Œç­‰å¾…å¤„ç†"
        )
    
    async def process_document(
        self,
        db: AsyncSession,
        document_id: int
    ) -> Document:
        """
        å¤„ç†æ–‡æ¡£ - è§£æã€åˆ†å—ã€å‘é‡åŒ–
        
        ä½¿ç”¨æµ‹è¯•ä»£ç éªŒè¯çš„é…ç½®ï¼š
        - chunk_size: 1000
        - chunk_overlap: 200
        - is_markdown: Trueï¼ˆå¯¹markdownæ–‡ä»¶ï¼‰
        """
        logger.info(f"âš™ï¸  å¼€å§‹å¤„ç†æ–‡æ¡£: ID={document_id}")
        
        # è·å–æ–‡æ¡£
        result = await db.execute(
            select(Document).where(Document.id == document_id)
        )
        document = result.scalar_one_or_none()
        if not document:
            logger.error(f"âŒ æ–‡æ¡£ä¸å­˜åœ¨: ID={document_id}")
            raise ValueError("Document not found")
        
        # è·å–é»˜è®¤Embedding Provider
        result = await db.execute(
            select(EmbeddingProvider).where(
                EmbeddingProvider.is_default == True,
                EmbeddingProvider.is_active == True
            )
        )
        embedding_provider = result.scalar_one_or_none()
        
        if not embedding_provider:
            error_msg = "æœªé…ç½®é»˜è®¤Embedding Provider"
            logger.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
        
        try:
            # æ›´æ–°çŠ¶æ€
            document.status = "processing"
            await db.flush()
            logger.info(f"ğŸ“– æ­£åœ¨è§£ææ–‡æ¡£: {document.filename}")
            
            # 1. è§£ææ–‡æ¡£
            file_path = Path(document.file_path)
            content = await FileParser.parse(file_path)
            
            content_length = len(content)
            logger.info(f"ğŸ“ æ–‡æ¡£è§£æå®Œæˆ: æå– {content_length} å­—ç¬¦")
            
            # 2. æ–‡æœ¬åˆ†å— - ä½¿ç”¨æµ‹è¯•ä»£ç çš„é…ç½®
            is_markdown = self._is_markdown_document(document, content)
            
            # ğŸ”¥ ä½¿ç”¨æµ‹è¯•ä»£ç ä¸­éªŒè¯æœ‰æ•ˆçš„å‚æ•°
            chunk_size = 1000
            chunk_overlap = 200
            
            splitter = get_text_splitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                is_markdown=is_markdown
            )
            chunks = splitter.split_text(content)
            
            if is_markdown:
                logger.info(f"ğŸ“‘ ä½¿ç”¨Markdownæ„ŸçŸ¥åˆ†å—ï¼ˆsize={chunk_size}, overlap={chunk_overlap}ï¼‰")
            else:
                logger.info(f"ğŸ“„ ä½¿ç”¨æ™®é€šåˆ†å—ï¼ˆsize={chunk_size}, overlap={chunk_overlap}ï¼‰")
            
            if not chunks:
                raise ValueError("No content extracted from document")
            
            logger.info(f"âœ‚ï¸  æ–‡æœ¬åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
            
            # 3. å‡†å¤‡å…ƒæ•°æ®
            metadatas = [
                {
                    "document_id": document.id,
                    "filename": document.filename,
                    "chunk_index": i
                }
                for i in range(len(chunks))
            ]
            
            # 4. å‘é‡åŒ–å¹¶å­˜å‚¨
            logger.info(f"ğŸ”¢ æ­£åœ¨å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°ChromaDB... (ä½¿ç”¨ {embedding_provider.name})")
            await self.vectorstore_service.add_documents(
                embedding_provider=embedding_provider,
                document_id=document.id,
                chunks=chunks,
                metadatas=metadatas
            )
            
            # 5. æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document.status = "completed"
            document.chunk_count = len(chunks)
            document.embedding_provider_id = embedding_provider.id
            await db.flush()
            
            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {document.filename} ({len(chunks)} ä¸ªå‘é‡)")
            
            return document
            
        except Exception as e:
            document.status = "failed"
            document.error_message = str(e)
            await db.flush()
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document.filename} - {e}")
            raise
    
    def _is_markdown_document(self, document: Document, content: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºMarkdownæ–‡æ¡£
        
        åˆ¤æ–­ä¾æ®ï¼š
        1. æ–‡ä»¶æ‰©å±•åæ˜¯.md
        2. æ–‡ä»¶ç±»å‹æ˜¯markdownæˆ–text
        3. å†…å®¹ä¸­åŒ…å«Markdownæ ‡è®°
        """
        # æ£€æŸ¥æ‰©å±•å
        if document.filename.lower().endswith(".md"):
            return True
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if document.file_type not in ("markdown", "text"):
            return False
        
        # æ£€æŸ¥å†…å®¹ç‰¹å¾ï¼ˆå‰1000å­—ç¬¦ï¼‰
        preview = content[:1000]
        markdown_markers = ["###", "##", "- ", "* ", "```", "**"]
        
        marker_count = sum(1 for marker in markdown_markers if marker in preview)
        
        # å¦‚æœåŒ…å«2ä¸ªä»¥ä¸ŠMarkdownæ ‡è®°ï¼Œè®¤ä¸ºæ˜¯Markdown
        return marker_count >= 2
    
    async def get_document(self, db: AsyncSession, document_id: int) -> Document | None:
        """è·å–æ–‡æ¡£"""
        result = await db.execute(
            select(Document).where(Document.id == document_id)
        )
        return result.scalar_one_or_none()
    
    async def list_documents(
        self,
        db: AsyncSession,
        skip: int = 0,
        limit: int = 20,
        status: str | None = None
    ) -> DocumentListResponse:
        """è·å–æ–‡æ¡£åˆ—è¡¨"""
        query = select(Document)
        count_query = select(func.count(Document.id))
        
        if status:
            query = query.where(Document.status == status)
            count_query = count_query.where(Document.status == status)
        
        # è·å–æ€»æ•°
        total_result = await db.execute(count_query)
        total = total_result.scalar()
        
        # è·å–åˆ—è¡¨
        query = query.order_by(Document.created_at.desc()).offset(skip).limit(limit)
        result = await db.execute(query)
        documents = result.scalars().all()
        
        logger.debug(f"ğŸ“‹ æŸ¥è¯¢æ–‡æ¡£åˆ—è¡¨: å…± {total} æ¡")
        
        return DocumentListResponse(
            total=total,
            items=[DocumentResponse.model_validate(doc) for doc in documents]
        )
    
    async def delete_document(self, db: AsyncSession, document_id: int) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        document = await self.get_document(db, document_id)
        if not document:
            return False
        
        logger.info(f"ğŸ—‘ï¸  åˆ é™¤æ–‡æ¡£: {document.filename}")
        
        # åˆ é™¤å‘é‡æ•°æ®
        await self.vectorstore_service.delete_documents(document_id)
        logger.debug(f"  - å·²åˆ é™¤å‘é‡æ•°æ®")
        
        # åˆ é™¤æ–‡ä»¶
        file_path = Path(document.file_path)
        if file_path.exists():
            file_path.unlink()
            logger.debug(f"  - å·²åˆ é™¤æ–‡ä»¶: {file_path}")
        
        # åˆ é™¤æ•°æ®åº“è®°å½•
        await db.delete(document)
        logger.info(f"âœ… æ–‡æ¡£åˆ é™¤å®Œæˆ: ID={document_id}")
        
        return True
    
    async def reprocess_document(
        self,
        db: AsyncSession,
        document_id: int
    ) -> Document:
        """é‡æ–°å¤„ç†æ–‡æ¡£"""
        logger.info(f"ğŸ”„ é‡æ–°å¤„ç†æ–‡æ¡£: ID={document_id}")
        
        # å…ˆåˆ é™¤æ—§çš„å‘é‡æ•°æ®
        await self.vectorstore_service.delete_documents(document_id)
        
        # é‡æ–°å¤„ç†ï¼ˆä¼šè‡ªåŠ¨è·å–é»˜è®¤embedding providerï¼‰
        return await self.process_document(db, document_id)
